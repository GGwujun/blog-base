exports.category = "computer";
exports.title = "网络排查案例课";
exports.data = [
  {
    chapterTitle: "开篇词",
    children: [
      {
        title: "开篇词 | 网络排查是工程师的必备能力",
        id: 477459,
        content:
          '<p>你好，我是杨胜辉，欢迎和我一起开启这场网络排查之旅。</p><h2>为什么在这个时代，网络排查能力变得越来越重要了？</h2><p>先和你简单介绍一下我自己，我目前是eBay中国卓越技术中心基础架构部门的运维经理，主要负责eBay全球的流量管理业务，推动Kubernetes在eBay流量管理场景中的落地。</p><p>我算是一个基础架构的老兵了，在18年的工作经历中，我实实在在地遇到过太多技术方面的疑难杂症。尤其是最近几年，随着微服务和云计算的不断普及，越来越多的系统从本地的单体服务，变成跨网络的分布式的微服务。那么随之而来，就是数不清的跟网络相关的问题。比如：</p><ul>\n<li>为什么我的应用在单体应用的时候很正常，拆分成微服务以后却时常超时、报错呢？</li>\n<li>为什么我的带宽是足够的，但数据传输速度却很慢？</li>\n<li>为什么我的应用偶尔会卡住，但又不是每次都这样？</li>\n<li>为什么……</li>\n</ul><p>面对这么多问题，我们经常束手无策。有些同学自嘲是优秀的“SRE”（Server Restart Engineer），遇到问题先上“重启大法”，也许也能搞定不少问题。但是呢，根因依然是未知，即使问题暂时消失了，但不知道什么时候，它又会再次到来，然后再次重启……</p><p>所有这一切，都让我深深地感到：<strong>我们的工程师，太需要网络排查方面的能力了。</strong></p><!-- [[[read_end]]] --><p>无论你是运维还是开发，无论是产品还是测试，都没法彻底回避网络问题。但是，因为大部分同学并不是网络出身，对于跟网络相关的问题，经常无从下手，或者事倍功半。在这方面，常见的困难就有：</p><ol>\n<li>对于网络知识点好像是懂的，但一遇到实际的网络故障，就又不懂了，更不知道排查工作如何做起；</li>\n<li>网络知识是懂的，也能做一些排查，但是遇到艰深一点的问题时，就“卡壳”了，无法把排查工作进一步推动下去；</li>\n<li>对于网络排查有一些经验，比如前面说的“重启大法”，比如知道修改某个配置可能有用，但是因为不知道其原理，在三板斧不见效的时候，就别无他法；</li>\n<li>本身负责应用开发，也有兴趣自己来排查网络问题，但苦于没有这方面的背景和积累，排查工作很容易“熄火”，令人沮丧。</li>\n</ol><p>如果你符合1，那么请不要失去信心，只要你踏实打好网络知识的基础，网络排查并没有那么艰难和神秘，通过学习，你也一样能掌握网络排查的能力。</p><p>如果你符合2，那么请多一些坚持，因为你还需要继续打磨对网络关键知识点的深刻理解。同时，也需要强化自己在一些高级技巧，比如抓包分析上的能力。因为只有这样，你才能突破瓶颈，真正把问题搞透。</p><p>如果你符合3，那么请多一些耐心，先不要过于依赖你过往的经验，而是沉下心来，剖析你的经验之所以有效的底层原理。这样，你既有经验也有理论，你的排查能力将又提升一个层级。</p><p>如果你符合4，那么请不要气馁，你能成为应用方面的高手，那么一样有潜能成为网络排查方面的高手。事实上，要成为应用开发方面的大牛，网络也是极为重要的一关，搞不定网络，你的应用依然是跛脚的，无法达到最优的状态。</p><h2>我的网络排查能力是如何成长起来的？</h2><p>不过，虽然我现在能给很多人讲解网络排查的技巧，其实我以前也是一枚小白。</p><p>刚工作时，我在一家央企中国远洋集团工作，主要维护公司的几百台Windows服务器、邮件系统等，不太接触网络。但是有一件跟网络相关的事情，给我留下了很深的印象。</p><p>当时欧洲分公司的Exchange邮件系统遇到故障，我们从Exchange系统本身来排查，却怎么也找不到问题所在。团队奋战数天后，最终在网络组的协助下，才发现是网络层问题导致的。假如时光可以倒流，我带着现在的网络排查能力回到过去，是不是可以把几天变成几小时甚至几分钟呢？我忍不住会这样遐想。</p><p>也是从那次开始，我深刻地体会到：<strong>无论是不是专职的网络工程师，我们都应该掌握好网络，只有这样才能把本职工作做到更好。</strong></p><p>后来我来到了eBay，开始负责起eBay的整体的Web流量。我们知道，网络和应用的关系是十分错综复杂的，而正是因为要处理eBay这种海量级的访问场景，多年来，我在这方面就积累了很多经验和理解，逐步形成了对于“网络排查”这个宏大主题的一些自己的实操经验和方法论。</p><p>在eBay做了几年后，因为被云计算的丰富的技术性所吸引，我加入了一家公有云初创企业UCloud，负责起售后技术服务。</p><p>如我所愿，由于客户和云平台的多样性，大部分网络场景和协议我都能遇到，也帮助我积累了鲜活的案例，以及接地气的排查经验，我也很享受这个过程，不断地丰富和打磨我的网络排查体系。我有一个习惯是每处理一个问题就开一个文件夹，里面放上相关的日志、抓包文件等，当时记录有500个文件夹之多。这几年有一个新流行的说法叫“<strong>刻意练习</strong>”，而那段日子，也是印证了这个说法，我也确实从中收获良多。</p><p>就比如说，我们负责的一个客户是做健康硬件的，就是一个硬件的体重计，配合他们的手机App，帮助消费者做身材管理。有一段时间，他们的Nginx上有大量的499报错。起初他们怀疑问题在我们云平台网络这里。为了“自证清白”，同时也是为了寻求真相，我花了好几天研究这个案子，最终证明这并不是云平台的问题，并且找到了根本性的解决办法。</p><p>有意思的是，在合作过程中，我跟另一家大厂的工程师建立了网络上的友谊。这也让我体会到：<strong>技术本身就是技术人之间的共同语言，也是连接我们的桥梁</strong>。</p><p>也是从那时候开始，我会在博客上分享自己的排查经验，包括从云计算公司回eBay后也一直在坚持，并逐渐得到了一些朋友的关注。能给身边的人以帮助，同时还发挥了自己的长处，这让我非常开心。我也特别想把这份收获，通过极客时间这么好的平台，分享给更多在这个技术行业里的其他同学。</p><p>也许你也对这个领域有兴趣，但苦于找不到合适的老师；也许你在实际工作中面临着类似的技术问题，但找不到好的方法，那我的这门课程可能对你会比较有帮助。倘若你通过学习我的课程，把很多知识点搞明白了，或者在实际工作中，确实能把网络问题给解决了，那我就再高兴不过了。</p><h2>这门课程能给你带来什么？</h2><p>那么现在你可能会问了，跟着我学习这门实战案例课程，都会有怎么样的收获呢？具体来说，有这么几点。</p><h4>更扎实的对网络各层知识的理解</h4><p>我举个例子，技术面试的时候，一个常见的问题就是：请介绍一下TCP握手的过程。这也算是面试八股文了，很多人背一下也能过，不过是否真的理解就难说了。那么在这门课程里面，我会把握手、挥手等各种细节都带到，而且会结合真实案例来讲解这些知识点，使你不仅知其然，而且知其所以然。下次回答这个问题的时候，恐怕面试官都要对你刮目相看了。</p><h4>更广阔的排查视野</h4><p>一般来说，网络问题用网络方面的工具排查，系统的问题用系统方面的工具排查，应用层也是如此。不过现实情况是，很多问题在一开始，并不能明确地归属到网络，还是系统，还是应用代码。所以作为排查者，我们不应该在一个预设的立场下展开排查工作，因为那样很容易就偏离了真相。</p><p>而在这门课里，很多案例也都是“看起来是A，查着是B，最后定位出来是C”的情况，这其实也是最真实的现实了。有了更广阔的排查视野，你就不会因为不熟悉另外一个领域，就不得不放弃，或者“硬扛”了；有了更广阔的排查视野，你就不会局限在原有的一亩三分地里面，而是真正地把问题搞清楚，你的价值也不用说，会得到更大的认可。</p><h4>更熟练的排查技术</h4><p>很多同学其实也用过tcpdump和Wireshark，包括我做面试的时候，不少候选人也都能就tcpdump和Wireshark侃侃而谈，但是追问细节的时候就语焉不详了。以Wireshark为例，它确实提供了相当多的信息提示，比如丢包和重传都会用不同的颜色跟正常的数据包区分开。</p><p>不过，为什么重传、为什么丢包，这些问题的答案，Wireshark会告诉你吗？不会。是Wireshark不够强吗？也不是。</p><p>因为每个组织、每个应用的情况都相差极大，只有靠你自己平时积累的经验，以及结合具体的网络和应用环境，才能获得最符合你这个特定案例的答案。而这门课里就有很多这样的例子，特别是我做公有云服务的时候的多样化的案例经验，都是我个人的独家经验，我也相信一定能给你很多启发。</p><h4>更完善的知识体系</h4><p><strong>如果你是做开发的</strong>，现在各种软件库和框架都极大地提升了我们的开发效率，但同时也屏蔽了很多底层细节。举个例子，应用层的“connection reset by peer”的报错，又如何跟底层网络的实际情况结合起来呢？应用层本身并不能回答这个问题。</p><p>那么通过这门课程，我们将解开很多的技术点的层层包裹，端详它们本来的模样，真正地理解这些技术设计者的初心。这也会帮助我们更好地理解这些技术的来龙去脉，反过来也能帮助我们更好地完成上层业务。</p><p><strong>如果你是做运维的</strong>，那么这个作用会更加明显。对底层原理的掌握，将会极大地提升运维工作的质量，无论是平时工作时候的“气定神闲”，还是故障危机时候的“英勇相救”，都将是你通过这门课程获得的能力。</p><p>所以呢，这次我的网络排查课，形式会跟其他课程有所不同。不是单纯地讲理论或者讲工具，而是围绕<strong>案例</strong>这个核心，展开我们的排查过程，并会聚焦到工具的使用，以及深入到关键技术点的分析上。</p><p><img src="https://static001.geekbang.org/resource/image/35/7c/35039428afc5b307fbbde2580fcff77c.jpg?wh=2000x462" alt=""></p><h2>这门课程是怎么安排的？</h2><p>那么，课程具体是怎么设置的呢？我把这门课分成了五大模块，分别是：</p><ul>\n<li><strong>预习篇</strong></li>\n</ul><p>在这个部分，我会从网络分层模型出发，来带你了解、学习并掌握整个网络世界的大体层次，和每层的相关工具。然后带你进入抓包分析这个技术殿堂，了解它的历史和现在，以及初步的使用方法。通过对分层模型和每层工具的理解，以及对抓包分析技术的认识，你就能打下网络排查的底层基础，为后续的学习铺平道路。</p><ul>\n<li><strong>实战一：TCP真实案例揭秘篇</strong></li>\n</ul><p>接下来，我们就要进入真正的实战了。</p><p>在这个模块里，我会从各种跟TCP相关的实际案例出发，来带你了解、学习并掌握TCP这个精密仪器的核心技术，包括传输性能的关键点、TCP重传的原因和对策、拥塞的优化策略、TCP保活机制等。我会通过一个个真实的案例，帮助你达成对这些核心知识点的真正理解，最后能够融会贯通，再也不怵TCP相关的难题。</p><ul>\n<li><strong>实战二：应用层真实案例揭秘篇</strong></li>\n</ul><p>在理解了TCP这部重要篇章之后，网络排查的核心知识，你就掌握了快一半了。不过，还有另外一个同等重量级的篇章等待你去学习，它就是应用层网络排查。其中，我们还需要补齐一个重大的短板：应用和网络之间的桥梁。</p><p>那么在这个模块里，我会从一个个典型的应用层网络排查案例出发，来带你了解、学习并掌握如何排查应用层的网络问题，让你通过对抓包分析这个核心技术在应用层的运用，搭建起应用和网络之间的“桥梁”。学完这个部分后，你在应对应用层的网络问题时就会成竹在胸了。</p><ul>\n<li><strong>实战三：不用抓包就能做的网络排查篇</strong></li>\n</ul><p>最后，我们还需要学习抓包分析之外的其他网络排查方法，因为掌握抓包分析相当于掌握了网络排查的主干，但还需要补充枝叶，这样我们的网络排查技能树才足够完整。所以在这个模块里，依然是从实际案例出发，来带你了解、学习并掌握这些工具的背后原理、使用场景、个人总结，让你能够通过对原理和实践经验的理解，达成融会贯通的目的。</p><ul>\n<li><strong>总结篇</strong></li>\n</ul><p>在学完前四个模块的具体技术之后，现在我们应该来一次总结了。所以在最后，我想再带你整体沉淀升华一下，一起把前面学习过的网络知识、抓包分析技术、所有其他的网络工具的技巧复习一遍，把它们打碎后，再次拼接在一起，形成你自己的技术体系。这样，你不仅可以学习到我的经验，还能够转化为你自己的理解，从而实现真正突破网络排查瓶颈的最终目标。</p><p><img src="https://static001.geekbang.org/resource/image/16/b4/16a83a46e9712fc85af6b0bd695aacb4.jpg?wh=2000x1203" alt=""></p><p>最后，我还想说的是，网络问题的排查过程，其实就像读一本侦探小说一样，充满了神秘感和吸引力。当你掌握了网络排查技术之后，你在遇到问题的那一刻，就不会再像过去那样想要逃避，反而会像猎人遇到猎物一样兴奋，很想一试身手，最终把案件调查彻底，水落石出。对于这样一个美妙的状态，你是不是很期待呢？</p><p>从这个角度上说，你甚至可以把课程当故事来看，不过你一样可以收获到知识，经历我曾经历过的起伏，从一个胡同到一个旮旯，兜兜转转，找到灯火阑珊处。我也许做不到让你“衣带渐宽”，但我愿你“终不悔”。共勉。</p><p>在这里，我想跟你一起立一个flag：<strong>你可以每节课后在留言区进行提问和交流，我也会及时回复你，哪怕只是打个卡，等两个月后你学完全部课程，一定会有很大的收获。</strong></p><p>最后的最后，我想说：</p><p>二十多讲的课程，就是我们有二十多次的相遇，而随着课程的推进，我们或许还会有更多的交集。我要感谢你决定花这么多时间，来学习这门课程。同时我也建议你，感谢下你自己，因为你愿意花时间来这个课程学习，因为你想做更好的自己。只要坚持下去，你一定可以做到！</p>',
        article_title: "开篇词 | 网络排查是工程师的必备能力",
      },
    ],
  },
  {
    chapterTitle: "预习篇",
    children: [
      {
        title: "01 | 网络模型和工具：网络为什么要分层？",
        id: 477510,
        content:
          '<p>你好，我是胜辉。</p><p>今天是咱们的第一节正课，就像我在开篇词里介绍的，在预习篇这里，我们的目标是搞清楚网络分层的概念，还有初步学习抓包分析。所以接下来，我会先从一些基础的网络知识说起，为你重点讲解网络分层模型以及各层之间的区别和联系。</p><p>因为咱们是以案例实战为导向的课程，所以我除了会在网络的每一层，给你介绍相关的技术细节以外，还会带你认识相应的排查工具。学完这节课，哪怕你原本是网络方面的小白，你也可以在网络排查方面“一试身手”了，是不是有点期待了呢？好，让我们开始吧。</p><h2>网络是七层、五层还是四层？</h2><p>学习网络排查，可能首先要搞清楚的，就是网络的分层模型了。工作中，我们也时常会听到这些术语，比如三层交换机、七层规则等等。网络分层的概念，可谓深入人心。</p><p>可是你有没有想过，网络为什么要分层呢？难道是非分不可吗？回答这个问题之前，我们先做个有趣的假设：这会儿是在网络诞生的前夜，什么IP协议、TCP协议都还不存在，而你是网络的缔造者，面临设计网络这个伟大的任务。面对这么好的机会，你会选择做怎样的设计呢？</p><p>你大体上有这么两种选择：</p><ul>\n<li><strong>应用程序包办一切。</strong>程序把应用层的数据，按某种编码转化为二进制数据，然后程序去操控网卡，把二进制数据发送到网络上。这期间，通信的连接方式、传输的可靠性、速度和效率的保证等等，都需要这个程序去实现。然后下次开发另外一个应用的时候，就把上面这些活，再干一遍。</li>\n<li><strong>应用程序、操作系统、网络设备等环节各自分工。</strong>应用程序只负责实现应用层的业务逻辑，操作系统负责连接的建立、处理网络拥塞和丢包乱序、优化网络读写速度等等，然后把数据交给网卡，后者和交换机等设备做好联动，负责二进制数据在物理线路上的传送和接收。</li>\n</ul><!-- [[[read_end]]] --><p>那么显然，第一种大包大揽的方式，实现难度太大、耦合度太高，怎么看都是一个“反面典型”。所以，我们应该选择第二种，也就是分层的方式去实现。</p><p>你有没有发现，其实这个思路，跟编程的思想是类似的。在编程中，我们需要把一些逻辑抽象为函数或者对象，以实现更好的解耦和复用。在网络世界里也是如此，每一层干好自己的分内事，那么所有的层次配合起来工作的时候，就显得有条不紊了。</p><p>说到具体的分层模型，你应该会想到两种比较有名的方案。对，它们就是<strong>OSI的七层模型</strong>，和<strong>TCP/IP的四层/五层模型</strong>。这两种模型的最大区别，就是前者在传输层和应用层之间，还有会话层和表示层，而后者没有。</p><p>我们来看一下示意图：</p><p><img src="https://static001.geekbang.org/resource/image/83/73/83bde3e930ea80c4cf1a3ae30868f973.jpg?wh=1920x1080" alt="图片"></p><p>那在这里，你可能还会想：这两种模型哪种用得最多，或者说，哪种更合理呢？</p><p>其实我觉得倒不用过于纠结在“谁比谁更好”这个点上，如果我们理解了每一层的作用，那么就不会被表象上的层级所束缚了。事实上，两种分法都有可取之处。</p><p>一般来说，七层模型在我们工作当中谈论得更多些。比如，我的同事会找过来说“你帮我建一个七层规则吧”。这里的七层，就是指应用层，他说的“七层规则”呢，可能是HTTP路由规则，比如把符合某种条件的HTTP请求，分流到某个特定的后端集群。</p><p>还有一些场景，也是比较适合用七层模型来解释的。比如，TLS虽然在TCP之上，按TCP/IP模型就要被归入应用层。但事实上，在HTTPS的场景下，HTTP协议就是运行在TLS协议之上的，那么是不是把HTTP和TLS分到不同的层次更合适呢？正好在七层模型里，第五层和第六层，可以分别代表TLS的会话保持功能和数据加解密这种表示层的功能。</p><p><img src="https://static001.geekbang.org/resource/image/e9/d5/e9f4b3258ec7c621b780db18be32d7d5.jpg?wh=1920x1080" alt="图片"></p><p>不过，会话层和表示层的协议确实比较少。从控制模型复杂度的角度来看，如果把这两层都合并到应用层，那么模型倒是比较简单，也适合入门学习的。所以从这一点上看，TCP/IP模型也有可取之处。</p><p>这里你可能稍有疑问，为什么TCP/IP还有四层和五层模型这两种说法呢？其实五层模型就是OSI的前四层，加上一个应用层。这样的话，这个五层模型跟OSI七层模型，差异就比四层模型又缩小了一点。</p><p>所以，你现在应该明白了，<strong>两种分层模型的最大差异，其实还是在会话层和表示层上面。</strong>第一到第四层，已经基本统一了。而它们的最高层，虽然一个叫第七层，一个叫第四层或者第五层，表面上虽然并不一致，但实际上都可以用“应用层”来代替。这样既避免了可能的误解，也更加准确地表示了这一层的具体用途。</p><h2>什么是TCP流？</h2><p>在一些技术文档，特别是Wireshark相关的文档中，“TCP流”是一个很常见的词汇。它是什么意思呢？为什么叫“流”，难道跟水有关吗？</p><p>其实，这里的TCP流，就是英文的TCP Stream。Stream这个词有“流”的意思，也有“连续的事件”这样一个含义，所以它是有前后、有顺序的，这也正对应了TCP的特性。</p><p>跟Stream相对的一个词是Datagram，它是指没有前后关系的数据单元，比如UDP和IP都属于Datagram。在Linux网络编程里面，TCP对应的socket类型是SOCK_STREAM，而UDP对应的，就是SOCK_DGRAM了。显然，DGRAM就是Datagram的简写。</p><p>在具体的网络报文层面，一个TCP流，对应的就是一个五元组：<strong>传输协议类型、源IP、源端口、目的IP、目的端口</strong>。比如，今天你访问了极客时间网站，那么你这次的TCP流就可能就是这样一个五元组：</p><pre><code class="language-java">(TCP, your_ip, your_port, geekbang_ip, 443)\n</code></pre><p>一个IP报文，包含了所有这五个元素，所以Wireshark在解析抓包文件时，自然就能通过五元组知道每个报文所属的TCP流了。这也是为什么我们可以在Wireshark里，用Follow TCP Stream的方法，找到报文所在的TCP流。</p><p>不过有时候，也会有四元组的说法。其实它跟五元组大体上是一致的，只是四元组没有区分传输层协议类型（TCP或者UDP）。但是如果我们都清楚地知道应用类型，比如知道应用是HTTP协议的，那它的传输层协议默认就是TCP，这一元是否算在里面，已经不重要了。</p><h2>报文、帧、分组、段、数据包，这些术语是同一个东西吗？</h2><p><strong>报文（packet）</strong>，是一种相对宽泛和通用的说法，基本上每一层都可以用。比如，在应用层，你可以说“HTTP报文”；在传输层，你可以说“TCP报文”；同样的，在网络层，当然就是“IP报文”了。事实上，网络层也是“报文”一词被使用最多的场景了。<strong>数据包</strong>也是类似的，可以在很多场景下通用。</p><p>我们再稍微考究一下语法。packet这个词的后缀是et。而在英文中，以et结尾的很多词表示某一个小小的东西。比如功能完备的一小段代码，叫code snippet，一小段内嵌在HTML中的Java前端代码，叫applet。自然的，packet就是一个小的pack（包裹）。</p><p>然而，另外几个术语在用的时候，就需要讲究一点了，因为它们并不是通用词，而是特定层的专有词汇。</p><p><strong>帧（frame）</strong>是二层也就是数据链路层的概念，代表了二层报文，它包含帧头、载荷、帧尾。注意，帧是有尾部的，而其他像IP、TCP、HTTP等层级的报文，都没有尾部。我们不可以说“TCP帧”或者“IP帧”，虽然也许对方也明白你的意思，但我们都想做得专业一点，不是嘛。这里还有个小知识点：HTTP/2实现了多路复用，其中也有帧的概念，不过那个帧，跟这里网络二层的帧，除了名称相同以外，就没有别的联系了。</p><p><strong>分组</strong>是IP层报文，也就是狭义的packet。</p><p><strong>段特指TCP segment</strong>，也就是TCP报文。既然segment是“部分”的意思，那这个“整体”又是什么呢？它就是在应用层交付给传输层的消息（message）。当message被交付给传输层时，如果这个message的原始尺寸，超出了传输层数据单元的限制（比如超出了TCP的MSS），它就会被划分为多个segment。这个过程就是<strong>分段</strong>（segmentation），也是TCP层的一个很重要的职责。</p><p>说到segmentation，你可能也会想到fragmentation（分片）。这俩是同一个东西吗？这方面的知识点也不少，我在这里就不具体展开了。不过别着急，我会在第8讲里，帮你把这两个东西梳理清楚。</p><p>另外，这里还要提一下，Datagram的中文叫<strong>“数据报”</strong>，但不是“数据包”。读音类似，但意思并不完全相同。前面说过，“数据包”是一个通用词，所以用“UDP数据包”指代“UDP数据报”并没有问题。但反过来，非UDP协议的数据包，比如TCP段，就不能叫“TCP数据报”了，因为TCP不是Datagram。</p><p>最后，你可以再来看下这张层级和术语对应关系的示意图：</p><p><img src="https://static001.geekbang.org/resource/image/21/06/210167875fb87016a6c4a52fbafc0006.jpg?wh=1920x1080" alt="图片"></p><h2>网络各层都有哪些排查工具呢？</h2><p>通过上面的内容，你应该对于网络为什么要做分层、为什么那样做分层，已经有了比较清晰地认识了，我也带你探讨了每个层级的名词概念。所谓“名不正则言不顺”，咱们把这些术语搞清楚了，是不是感觉自己的技术“格调”也有那么点提升了呢？</p><p>接下来，我们进入干货部分，也就是每个层级的排查工具，用大白话说就是：“这可是我们吃饭的家伙儿”。</p><h3>应用层</h3><p>应用层的排查工具就太多了，相信做应用的同学，对自己的应用排查，应该是比我要更加熟悉。那我这里呢，就选一个主要的应用来展开吧，我们来谈谈 <strong>HTTP应用的排查工具</strong>。</p><p>现在主流的浏览器是Google的Chrome，它本身就<strong>内置了一个开发者工具</strong>。在Chrome界面里按下F12，或者你是苹果系统的话，还可以按下组合键option + command + I，启动开发者工具。</p><p>其实在其他的浏览器上，都有类似这样的工具，比如<strong>Firefox和Edge</strong>。而且因为Edge基于Chromium浏览器内核，它的开发者工具跟Chrome的开发者工具很相似。</p><p>在更老的IE浏览器时代，并没有原生的开发者工具。当时有一个叫<strong>HttpWatch</strong>的工具，可以在IE上实现类似的功能，但需要另外安装。</p><p>借助开发者工具，我们可以非常方便地做很多事，比如以下这些。</p><ul>\n<li><strong>找到有问题的服务端IP</strong></li>\n</ul><p>比如有用户报告死活访问不了你的网站，但是你很清楚这个网站的域名对应了很多IP地址，你怎么知道用户连的是哪个IP呢？</p><p>你可以这样做：让客户启用开发者工具，在Network页找到主页对象，在它的Headers部分，就能看到Remote address，这里的IP就是当前连接的IP，比如下面这样：</p><p><img src="https://static001.geekbang.org/resource/image/96/49/969a3674269b60593e83f623c310c749.jpg?wh=2572x1072" alt=""></p><p>不过有句成语叫“刻舟求剑”，因为DNS解析的关系，你很可能下次重连就不是这个IP了，所以每次都应该重新确认一下这个信息。</p><p>这个技巧，在<strong>排查公网的访问问题</strong>的时候特别有用。要知道，现在流量大一点的网站都已经上了CDN，那就必然在全国乃至全球各地，有少则数十个、多则数百个CDN终端节点，在给访问者提供就近的服务。如果有人说他访问不了某个站点了，那么请一定让他用开发者工具，找到他连的远程IP，然后你再根据这个信息展开排查工作。</p><ul>\n<li><strong>辅助排查网页慢的问题</strong></li>\n</ul><p>访问页面感觉很慢，那么可以借助开发者工具的<strong>时间统计功能</strong>，找到耗时较高的HTTP资源对象，再针对性排查。比如我觉得访问<a href="https://github.com">https://github.com</a>很慢，那么可以先打开开发者工具，然后访问站点，等全部加载完成后，到Network页查看这些HTTP对象的加载时间。</p><p><img src="https://static001.geekbang.org/resource/image/52/ba/529e907d52d11c95d26dd8a8681428ba.jpg?wh=2668x1074" alt=""></p><p>不过，这个办法只能排查到是哪个资源对象耗时比较长，但更进一步的排查，比如“为什么这个对象的加载时间比别的对象长”这个问题，开发者工具就难以回答了。关于这个问题，我会在后续的课程里深入展开，我们会用到抓包分析这把“手术刀”，来根本性地排查这类问题。</p><ul>\n<li><strong>解决失效Cookie带来的问题</strong></li>\n</ul><p>有时候我们的Cookie过期了，导致无法正常登录站点，那么可以打开开发者工具，到Application页，找到Storage -&gt; Cookie，把对应的条目清除。这样下次你再访问这个站点，就已经“洗心革面”了。对站点来说，你就是一次新的访问，可以生成一次新的Cookie了。</p><p>当然，你通过删除浏览器缓存的方式，也是可以做到这一点的。但开发者工具的优点是，可以<strong>细粒度</strong>到这个网站级别，而删除缓存的方式，删除的就是所有站点的Cookie了，这未必是你想要的。</p><h3>表示层和会话层</h3><p>在前面的网络分层部分，我提到过，其实表示层和会话层的协议并不多，TLS可以归入这两个层级。为了对TLS的问题进行排查，我推荐你两种工具。</p><p><strong>第一种，还是基于浏览器做初步的检查，主要是围绕证书本身做检查。</strong>在浏览器的地址栏那里，有一个按钮，点开后就可以查看TLS证书等信息：</p><p><img src="https://static001.geekbang.org/resource/image/ff/3c/ff2324fa28934951c39b6e65b8d5833c.jpg?wh=593x408" alt="图片"></p><p>在上面的菜单中，继续点开Connection is secure按钮，进而点击Certificate is valid按钮，就能查看证书了。</p><p>另外，使用开发者工具的Security菜单，还可以查看更为详细的TLS信息，包括协议版本、密钥交换算法、证书有效期等等。</p><p><img src="https://static001.geekbang.org/resource/image/59/88/59c05ec8a3c287036d4b286f749eb188.jpg?wh=779x676" alt="图片"></p><p><strong>第二种，关于TLS握手、密钥交换、密文传输等方面的排查，还是需要用tcpdump和Wireshark来做。</strong>在Wireshark中，可以更加全面地查看TLS细节。</p><p>比如，我们可以直接看到TLS握手阶段里，双方协商<strong>过程中</strong>各自展示的Cipher suite，而在开发者工具里，我们只能看到协商<strong>完成后</strong>的选择。</p><p><img src="https://static001.geekbang.org/resource/image/a4/b3/a4153e0a5c4be0e520ab00bb44b0fab3.jpg?wh=671x611" alt="图片"></p><h3>传输层</h3><p>传输层毫无疑问是重中之重，工具也很多。我们就按排查场景来介绍工具。</p><ul>\n<li><strong>路径可达性测试</strong></li>\n</ul><p>如果我们要测试TCP握手，我们有<strong>telnet、nc</strong>这两个常规工具。比如telnet：</p><pre><code class="language-plain">$ telnet www.baidu.com 443\nTrying 180.101.49.12...\nConnected to www.a.shifen.com.\nEscape character is \'^]\'.\n\n</code></pre><p>用nc呢，可以这样：</p><pre><code class="language-plain">$ nc -w 2 -zv www.baidu.com 443\nConnection to www.baidu.com 443 port [tcp/https] succeeded!\n</code></pre><ul>\n<li><strong>查看当前连接状况</strong></li>\n</ul><p><strong>netstat</strong> 命令是一个经典命令了，很多同学都会使用它来获取当前的TCP、UDP等的连接信息，比如：</p><pre><code class="language-plain">$ netstat -ant\nActive Internet connections (servers and established)\nProto Recv-Q Send-Q Local Address&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Foreign Address&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;State\ntcp&nbsp; &nbsp; &nbsp; &nbsp; 0&nbsp; &nbsp; &nbsp; 0 127.0.0.53:53&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0.0.0.0:*&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;LISTEN\ntcp&nbsp; &nbsp; &nbsp; &nbsp; 0&nbsp; &nbsp; &nbsp; 0 0.0.0.0:22&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.0.0.0:*&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;LISTEN\ntcp&nbsp; &nbsp; &nbsp; &nbsp; 0&nbsp; &nbsp; &nbsp; 0 0.0.0.0:80&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.0.0.0:*&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;LISTEN\ntcp&nbsp; &nbsp; &nbsp; &nbsp; 0&nbsp; &nbsp; 280 10.0.2.15:22&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 10.0.2.2:56669&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ESTABLISHED\ntcp6&nbsp; &nbsp; &nbsp; &nbsp;0&nbsp; &nbsp; &nbsp; 0 :::22&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;:::*&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; LISTEN\n</code></pre><ul>\n<li><strong>查看当前连接的传输速率</strong></li>\n</ul><p>有时候，你的网络跑得挺繁忙的，但你却不知道哪个连接占用了大量的带宽？你可以用 <strong>iftop</strong>。这个工具不是系统默认自带的，需要你安装一下，然后执行iftop就好了。对了，你需要有sudo权限，也就是执行sudo iftop，然后就能看到不同连接的传输速率，把祸害你带宽的连接给找到。比如下面这样：</p><p><img src="https://static001.geekbang.org/resource/image/36/a2/367d7286ecc1bf97c6f8bcd5709df1a2.jpg?wh=927x227" alt="图片"></p><ul>\n<li><strong>查看丢包和乱序等的统计</strong></li>\n</ul><p>其实，用netstat除了可以获取实时连接状况，还可以获取历史统计信息。比如，你怀疑一台机器的网络很不稳定，除了用ping做简单的测试，你还可以用 <strong>netstat -s</strong> 来获取更加详细的统计信息。比如，其中的TCP丢包和乱序计数值，就能帮助你判断传输层的状况。下面是我截取了一次netstat -s命令的输出：</p><pre><code class="language-plain">$ netstat -s\n......\nTcp:\n&nbsp; &nbsp; 16 active connection openings\n&nbsp; &nbsp; 1 passive connection openings\n&nbsp; &nbsp; 8 failed connection attempts\n&nbsp; &nbsp; 1 connection resets received\n&nbsp; &nbsp; 1 connections established\n&nbsp; &nbsp; 6254 segments received\n&nbsp; &nbsp; 4035 segments sent out\n&nbsp; &nbsp; 1 segments retransmitted\n&nbsp; &nbsp; 0 bad segments received\n&nbsp; &nbsp; 3 resets sent\n......\nTcpExt:\n&nbsp; &nbsp; 1 ICMP packets dropped because socket was locked\n&nbsp; &nbsp; 3 TCP sockets finished time wait in fast timer\n&nbsp; &nbsp; 8 delayed acks sent\n&nbsp; &nbsp; 4674 packet headers predicted\n&nbsp; &nbsp; 10 acknowledgments not containing data payload received\n&nbsp; &nbsp; 1008 predicted acknowledgments\n&nbsp; &nbsp; TCPTimeouts: 1\n&nbsp; &nbsp; TCPBacklogCoalesce: 140\n&nbsp; &nbsp; 1 connections reset due to early user close\n&nbsp; &nbsp; TCPRcvCoalesce: 2187\n&nbsp; &nbsp; TCPAutoCorking: 110\n&nbsp; &nbsp; TCPSynRetrans: 1\n&nbsp; &nbsp; TCPOrigDataSent: 1041\n&nbsp; &nbsp; TCPDelivered: 1049\n</code></pre><p>你可能会问：这些不是静态值吗，我想知道当前情况啊？这个也很好解决，你可以这样做：</p><pre><code class="language-plain">watch --diff netstat -s\n</code></pre><p>这个命令会把发生变化的数值进行高亮，方便我们查看：</p><p><img src="https://static001.geekbang.org/resource/image/1c/be/1c67a7092ac84aff78360fyy9af7cabe.jpg?wh=606x418" alt=""></p><p>当然，上面这个算运维“青铜”版。你也可以写一个简单的脚本，在两次netstat -s命令之间执行sleep，然后计算两个读数之间的差值，并除以sleep的时间，得到大致的变化速度。这样就又升级了一点。</p><p>如果你想做得再到位一点，你可以把netstat -s的输出值写入到TSDB，然后用Grafana之类的Dashboard展示，这样不仅有视图，也有历史值，可以算运维“王者”了。</p><ul>\n<li><strong>还有ss？</strong></li>\n</ul><p><strong>ss</strong> 命令是Iproute2包里的命令，也是netstat的“取代者”。它提供了对socket的丰富的统计信息。比如下面这条命令我也经常用，可以查看到当前连接的统计信息：</p><pre><code class="language-plain">$ ss -s\nTotal: 164\nTCP:&nbsp; &nbsp;5 (estab 1, closed 0, orphaned 0, timewait 0)\n\nTransport Total&nbsp; &nbsp; &nbsp;IP&nbsp; &nbsp; &nbsp; &nbsp; IPv6\nRAW\t&nbsp; 1&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;1\nUDP\t&nbsp; 2&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;2&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0\nTCP\t&nbsp; 5&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;4&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;1\nINET\t&nbsp; 8&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;6&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;2\nFRAG\t&nbsp; 0&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0\n</code></pre><p>当然，也不能完全说“ss等于netstat”，因为事实上netstat命令的功能，被拆分到了ss和ip这两个命令里，并分别得到了丰富和加强。具体的细节，我们在课程中还会陆续提到。</p><h3>网络层</h3><p>在这一层，除了可以直接用ping这个非常简便的工具以外，你还应该掌握另外两个命令，它们能提供更为强大的排查能力，它们就是<strong>traceroute和mtr</strong>。</p><ul>\n<li><strong>查看网络路径状况</strong></li>\n</ul><p>下面这个，是我用自己的Mac笔记本做一个简单的traceroute的典型输出：</p><pre><code class="language-plain">$ traceroute&nbsp; www.baidu.com\ntraceroute to www.a.shifen.com (180.101.49.12), 64 hops max\n&nbsp; 1&nbsp; &nbsp;10.0.2.2&nbsp; 0.133ms&nbsp; 0.131ms&nbsp; 0.087ms\n&nbsp; 2&nbsp; &nbsp;192.168.1.1&nbsp; 3.048ms&nbsp; 1.466ms&nbsp; 1.574ms\n&nbsp; 3&nbsp; &nbsp;100.65.0.1&nbsp; 8.975ms&nbsp; 3.067ms&nbsp; 6.472ms\n&nbsp; 4&nbsp; &nbsp;61.152.53.149&nbsp; 5.644ms&nbsp; 3.691ms&nbsp; 4.624ms\n&nbsp; 5&nbsp; &nbsp;61.152.24.226&nbsp; 5.357ms&nbsp; 4.393ms&nbsp; 4.244ms\n&nbsp; 6&nbsp; &nbsp;202.97.29.122&nbsp; 10.171ms&nbsp; 10.403ms&nbsp; 8.755ms\n&nbsp; 7&nbsp; &nbsp;58.213.94.118&nbsp; 10.707ms&nbsp; 11.880ms&nbsp; 11.441ms\n&nbsp; 8&nbsp; &nbsp;58.213.94.90&nbsp; 9.644ms&nbsp; *&nbsp; *\n&nbsp; 9&nbsp; &nbsp;58.213.96.110&nbsp; 12.758ms&nbsp; 12.095ms&nbsp; 11.842ms\n&nbsp;10&nbsp; &nbsp;*&nbsp; *&nbsp; *\n&nbsp;11&nbsp; &nbsp;*&nbsp; *&nbsp; *\n&nbsp;12&nbsp; &nbsp;*&nbsp; *&nbsp; *\n&nbsp;13&nbsp; &nbsp;*&nbsp; *&nbsp; *\n&nbsp;14&nbsp; &nbsp;*&nbsp; *&nbsp; *\n&nbsp;15&nbsp; &nbsp;*&nbsp; *&nbsp; *\n&nbsp;16&nbsp; &nbsp;*&nbsp; *&nbsp; *\n&nbsp;17&nbsp; &nbsp;*&nbsp; *&nbsp; *\n&nbsp;18&nbsp; &nbsp;*&nbsp; *&nbsp; *\n&nbsp;19&nbsp; &nbsp;*&nbsp; *&nbsp; *\n&nbsp;20&nbsp; &nbsp;*&nbsp; *&nbsp; *\n</code></pre><p>哦，等等，为什么从第10跳开始就没有IP，只有星号了？你是不是也遇到过这种情况呢？其实，你稍微改一下命令，也就是加上<strong>-I</strong>参数（I代表ICMP），就可以正常跑到底了：</p><pre><code class="language-plain">$ traceroute&nbsp; www.baidu.com -I\ntraceroute to www.a.shifen.com (180.101.49.12), 64 hops max\n&nbsp; 1&nbsp; &nbsp;10.0.2.2&nbsp; 0.099ms&nbsp; 2.363ms&nbsp; 0.078ms\n&nbsp; 2&nbsp; &nbsp;192.168.1.1&nbsp; 3.320ms&nbsp; 1.220ms&nbsp; 1.204ms\n&nbsp; 3&nbsp; &nbsp;100.65.0.1&nbsp; 8.737ms&nbsp; 4.872ms&nbsp; 6.403ms\n&nbsp; 4&nbsp; &nbsp;61.152.54.125&nbsp; 5.035ms&nbsp; 3.397ms&nbsp; 4.288ms\n&nbsp; 5&nbsp; &nbsp;*&nbsp; 61.152.25.110&nbsp; 4.176ms&nbsp; *\n&nbsp; 6&nbsp; &nbsp;202.97.101.30&nbsp; 7.447ms&nbsp; 6.399ms&nbsp; 5.936ms\n&nbsp; 7&nbsp; &nbsp;58.213.95.110&nbsp; 10.488ms&nbsp; *&nbsp; 9.014ms\n&nbsp; 8&nbsp; &nbsp;*&nbsp; 58.213.95.134&nbsp; 11.064ms&nbsp; *\n&nbsp; 9&nbsp; &nbsp;58.213.96.74&nbsp; 10.997ms&nbsp; 10.042ms&nbsp; 10.592ms\n&nbsp;10&nbsp; &nbsp;*&nbsp; *&nbsp; *\n&nbsp;11&nbsp; &nbsp;*&nbsp; *&nbsp; *\n&nbsp;12&nbsp; &nbsp;*&nbsp; *&nbsp; *\n&nbsp;13&nbsp; &nbsp;180.101.49.12&nbsp; 11.269ms&nbsp; 9.518ms&nbsp; 8.779ms\n</code></pre><p>背后的原理，就是traceroute默认是用UDP作为探测协议的，但是很多网络设备并不会对UDP作出回应。所以我们改成ICMP协议做探测后，网络设备就有回应了。其实，Windows上的tracert，就是默认用ICMP，这一点跟Linux正好是反过来的。两个操作系统，真是“相爱相杀”啊。</p><p>但是，traceroute也有一个明显的不足：<strong>它不能对这个路径做连续多次的探测</strong>。</p><p>于是，mtr出现了，它可以说是traceroute的超集，除了traceroute的功能，还能实现丰富的探测报告。尤其是它对每一跳的丢包率的百分比，是用来定位路径中节点问题的重要指标。所以，当你在遇到<strong>“连接状况时好时坏的问题”</strong>的时候，单纯用一次性的traceroute恐怕难以看清楚，那就可以用mtr，来获取更加全面和动态的链路状态信息了。</p><pre><code class="language-plain">$ mtr www.baidu.com -r -c 10\nStart: 2022-01-07T04:05:02+0000\nHOST: victorebpf&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Loss%&nbsp; &nbsp;Snt&nbsp; &nbsp;Last&nbsp; &nbsp;Avg&nbsp; Best&nbsp; Wrst StDev\n&nbsp; 1.|-- _gateway&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0.0%&nbsp; &nbsp; 10&nbsp; &nbsp; 0.3&nbsp; &nbsp;0.4&nbsp; &nbsp;0.2&nbsp; &nbsp;1.2&nbsp; &nbsp;0.3\n&nbsp; 2.|-- 192.168.1.1&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.0%&nbsp; &nbsp; 10&nbsp; &nbsp; 1.6&nbsp; &nbsp;1.8&nbsp; &nbsp;1.4&nbsp; &nbsp;3.2&nbsp; &nbsp;0.5\n&nbsp; 3.|-- 100.65.0.1&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0.0%&nbsp; &nbsp; 10&nbsp; &nbsp; 3.8&nbsp; &nbsp;7.0&nbsp; &nbsp;3.8&nbsp; 10.3&nbsp; &nbsp;2.0\n&nbsp; 4.|-- 61.152.54.125&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.0%&nbsp; &nbsp; 10&nbsp; &nbsp; 4.0&nbsp; &nbsp;4.3&nbsp; &nbsp;3.6&nbsp; &nbsp;5.1&nbsp; &nbsp;0.5\n&nbsp; 5.|-- 61.152.25.110&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;30.0%&nbsp; &nbsp; 10&nbsp; &nbsp; 5.0&nbsp; &nbsp;6.8&nbsp; &nbsp;4.4&nbsp; 18.9&nbsp; &nbsp;5.4\n&nbsp; 6.|-- 202.97.101.30&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;20.0%&nbsp; &nbsp; 10&nbsp; &nbsp; 7.8&nbsp; &nbsp;6.6&nbsp; &nbsp;5.4&nbsp; &nbsp;7.8&nbsp; &nbsp;0.8\n&nbsp; 7.|-- 58.213.95.110&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;80.0%&nbsp; &nbsp; 10&nbsp; &nbsp;10.0&nbsp; &nbsp;9.8&nbsp; &nbsp;9.6&nbsp; 10.0&nbsp; &nbsp;0.3\n&nbsp; 8.|-- ???&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;100.0&nbsp; &nbsp; 10&nbsp; &nbsp; 0.0&nbsp; &nbsp;0.0&nbsp; &nbsp;0.0&nbsp; &nbsp;0.0&nbsp; &nbsp;0.0\n&nbsp; 9.|-- 58.213.96.74&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0.0%&nbsp; &nbsp; 10&nbsp; &nbsp;10.5&nbsp; 12.7&nbsp; &nbsp;9.9&nbsp; 24.7&nbsp; &nbsp;4.9\n&nbsp;10.|-- ???&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;100.0&nbsp; &nbsp; 10&nbsp; &nbsp; 0.0&nbsp; &nbsp;0.0&nbsp; &nbsp;0.0&nbsp; &nbsp;0.0&nbsp; &nbsp;0.0\n&nbsp;11.|-- ???&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;100.0&nbsp; &nbsp; 10&nbsp; &nbsp; 0.0&nbsp; &nbsp;0.0&nbsp; &nbsp;0.0&nbsp; &nbsp;0.0&nbsp; &nbsp;0.0\n&nbsp;12.|-- ???&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;100.0&nbsp; &nbsp; 10&nbsp; &nbsp; 0.0&nbsp; &nbsp;0.0&nbsp; &nbsp;0.0&nbsp; &nbsp;0.0&nbsp; &nbsp;0.0\n&nbsp;13.|-- 180.101.49.12&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.0%&nbsp; &nbsp; 10&nbsp; &nbsp; 9.4&nbsp; &nbsp;9.1&nbsp; &nbsp;8.3&nbsp; &nbsp;9.7&nbsp; &nbsp;0.5\n</code></pre><ul>\n<li><strong>查看路由</strong></li>\n</ul><p>命令 <strong>route</strong> 可以查看路由表，不过这个命令比较老一点：</p><pre><code class="language-plain"># route -n\nKernel IP routing table\nDestination&nbsp; &nbsp; &nbsp;Gateway&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Genmask&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Flags Metric Ref&nbsp; &nbsp; Use Iface\n0.0.0.0&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;10.0.2.2&nbsp; &nbsp; &nbsp; &nbsp; 0.0.0.0&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;UG&nbsp; &nbsp; 100&nbsp; &nbsp; 0&nbsp; &nbsp; &nbsp; &nbsp; 0 enp0s3\n10.0.2.0&nbsp; &nbsp; &nbsp; &nbsp; 0.0.0.0&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;255.255.255.0&nbsp; &nbsp;U&nbsp; &nbsp; &nbsp;0&nbsp; &nbsp; &nbsp; 0&nbsp; &nbsp; &nbsp; &nbsp; 0 enp0s3\n10.0.2.2&nbsp; &nbsp; &nbsp; &nbsp; 0.0.0.0&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;255.255.255.255 UH&nbsp; &nbsp; 100&nbsp; &nbsp; 0&nbsp; &nbsp; &nbsp; &nbsp; 0 enp0s3\n172.17.0.0&nbsp; &nbsp; &nbsp; 0.0.0.0&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;255.255.0.0&nbsp; &nbsp; &nbsp;U&nbsp; &nbsp; &nbsp;0&nbsp; &nbsp; &nbsp; 0&nbsp; &nbsp; &nbsp; &nbsp; 0 docker0\n</code></pre><p>传输层工具里介绍的 <strong>netstat</strong>，其实也能帮我们查看路由，只要加上 <strong>-r</strong> 参数：</p><pre><code class="language-plain">$ netstat -r\nKernel IP routing table\nDestination&nbsp; &nbsp; &nbsp;Gateway&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Genmask&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Flags&nbsp; &nbsp;MSS Window&nbsp; irtt Iface\ndefault&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;_gateway&nbsp; &nbsp; &nbsp; &nbsp; 0.0.0.0&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;UG&nbsp; &nbsp; &nbsp; &nbsp; 0 0&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0 enp0s3\n10.0.2.0&nbsp; &nbsp; &nbsp; &nbsp; 0.0.0.0&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;255.255.255.0&nbsp; &nbsp;U&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0 0&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0 enp0s3\n_gateway&nbsp; &nbsp; &nbsp; &nbsp; 0.0.0.0&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;255.255.255.255 UH&nbsp; &nbsp; &nbsp; &nbsp; 0 0&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0 enp0s3\n172.17.0.0&nbsp; &nbsp; &nbsp; 0.0.0.0&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;255.255.0.0&nbsp; &nbsp; &nbsp;U&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0 0&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0 docker0\n</code></pre><p>我前面说过，netstat是被ss和ip这两个命令替代了。所以我们同样可以用 <strong>ip命令</strong>查看路由。比如这样：</p><pre><code class="language-plain">$ ip route\ndefault via 10.0.2.2 dev enp0s3 proto dhcp src 10.0.2.15 metric 100\n10.0.2.0/24 dev enp0s3 proto kernel scope link src 10.0.2.15\n10.0.2.2 dev enp0s3 proto dhcp scope link src 10.0.2.15 metric 100\n172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown\n</code></pre><h3>数据链路层和物理层</h3><p>这一层离应用层已经很远了，一般来说是专职的网络团队在负责。如果这一层有问题，就会直接体现在网络层表现上面，比如IP会有丢包和延迟等现象，然后会引发传输层异常（如丢包、乱序、重传等）。所以，<strong>一个稳定的数据链路层乃至物理层，是网络可靠性的基石。</strong></p><p>你可能会奇怪：既然底下这两层的稳定性如此重要，那上层的TCP不是号称还有传输可靠性的保障吗？难道这种保障形同虚设？</p><p>其实，这两点并不矛盾。TCP的传输可靠性是通过序列号、确认号、重传机制等来保证的，通过这种机制，TCP可以在<strong>一定程度</strong>的网络不稳定场景下，依然保证传输可靠，但不等于TCP可以无限容忍底层的不稳定，因为各种TCP拥塞控制算法都会因为这种问题，而极大地降低传输性能。</p><p>如果你想查看这两层的状况，可以用 <strong>ethtool</strong> 这个工具。比如这样：</p><pre><code class="language-plain"># ethtool -S enp0s3\nNIC statistics:\n&nbsp; &nbsp; &nbsp;rx_packets: 45897\n&nbsp; &nbsp; &nbsp;tx_packets: 9457\n&nbsp; &nbsp; &nbsp;rx_bytes: 59125524\n&nbsp; &nbsp; &nbsp;tx_bytes: 834625\n&nbsp; &nbsp; &nbsp;rx_broadcast: 0\n&nbsp; &nbsp; &nbsp;tx_broadcast: 17\n&nbsp; &nbsp; &nbsp;rx_multicast: 0\n&nbsp; &nbsp; &nbsp;tx_multicast: 59\n&nbsp; &nbsp; &nbsp;rx_errors: 0\n&nbsp; &nbsp; &nbsp;tx_errors: 0\n&nbsp; &nbsp; &nbsp;tx_dropped: 0\n</code></pre><p>它的原理，是网卡驱动会到内核中注册ethtool回调函数，然后我们用ethtool命令就可以查看这些信息了。由于信息是由网卡驱动提供的，所以十分“接地气”。</p><p>如果你在传输层和网络层的排查工具上，已经看到明确的链路不稳定的信息，那就直接找网络团队去处理吧。</p><h2>小结</h2><p>这节课，我们回顾了网络分层模型，也了解了OSI模型和TCP/IP模型的区别和联系。通过“抠字眼”的方式，我们把每层的术语搞清楚，由此对分层模型有了更加深入的理解，这个对我们开展网络排查工作，有很强的指导性意义。</p><p>然后，我们逐一学习了各层的常用排查工具。我来给你再梳理一下：</p><ol>\n<li>应用层以HTTP为例，可以用<strong>浏览器开发者工具</strong>，实现远程IP识别、耗时分析、Cookie删除等需求。</li>\n<li>会话层和表示层以TLS为主，我们还是用<strong>浏览器开发者工具</strong>，可以查看证书细节、协商后使用的Cipher suite等信息，属于静态信息。然后学习了<strong>用tcpdump和Wireshark</strong> 查看更详细的TLS握手细节的方法。这些信息是动态的，也只有用抓包分析的手段才能做到。</li>\n<li>在传输层，我们学到了 <strong>telnet、nc、netstat、ss</strong> 等命令，通过它们，我们可以测试连通性，也可以获取连接状况和统计信息，对于传输问题的排查都很有帮助。</li>\n<li>在网络层及以下的部分，我们学习了 <strong>traceroute、mtr、ip</strong> 等工具，可以检测网络路径状况。</li>\n<li>在数据链路层和物理层，我们可以做得不多，主要依靠网络层观察到的链路质量来推断这两次的情况。当然，也可以用 <strong>ethtool</strong> 这个工具查看这两层的详情。</li>\n</ol><p>最后，为了方便你复习，我也给你画了一张思维导图，让你能一目了然：</p><p><img src="https://static001.geekbang.org/resource/image/17/d2/179d2da5c5cc67a9b0f07af2cbc668d2.jpg?wh=1630x1107" alt=""></p><p>如果对这些命令的更多细节或者原理很感兴趣，在实战三模块里，我也会专门讨论这些工具相关的案例和使用技巧，相信会让你的网络排查技能变得更加丰富多元。</p><h2>思考题</h2><p>感谢你认真学完了这节课的内容，不过在结束之前，给你留几道思考题：</p><ol>\n<li>traceroute默认是用UDP来做探测的，那这个又是基于什么原理呢？通和不通，我们会收到怎样的回复？</li>\n<li>有时候运行telnet后命令就挂起，没有响应了，这说明了什么问题呢？</li>\n</ol><p>欢迎你把答案写到留言区，我们一起交流讨论。也欢迎你把今天的内容分享给更多的朋友，一同成长和进步。</p>',
        article_title: "01 | 网络模型和工具：网络为什么要分层？",
      },
      {
        title: "02 | 抓包分析技术初探：你会用tcpdump和Wireshark吗？",
        id: 478189,
        content:
          '<p>你好，我是胜辉。</p><p>咱们这门课最核心的内容，恐怕就是抓包分析了。在众多的排查技术中，抓包分析可以说是“皇冠上的明珠”，也是包括我自己在内的很多人一直努力的方向。所以，tcpdump和Wireshark这两个工具在工程师心目中的位置，自然不用我多提了。相信你能来上这门课，也很大程度上是想把这两个工具好好学一下的。</p><p>不过，你了解这两个工具的过去吗？它们最初是怎么出现的，又是什么样的机制使得它们如此强大呢？</p><p>这节课，我就带你走进抓包分析技术大家庭。你会从中了解到抓包分析技术的光荣历史和渊源，以及通过实际的例子，感受到它的精妙设计和强大能力。当你理解了tcpdump和Wireshark的初步用法之后，你对常见的抓包需求场景也就能心里有数，知道大概从哪里下手了。</p><h2>这些抓包技术名词，你分清楚了吗？</h2><p>首先，我帮你捋一下这些技术的来龙去脉甚至“八卦”，这样你在进入后面课程的具体技术学习时，就会多几分亲近感，也多几分底气了。</p><ul>\n<li><strong>tcpdump</strong></li>\n</ul><p>我们先来认识大名鼎鼎的tcpdump。1988年，劳伦斯伯克利国家实验室的四位工程师编写出了tcpdump这个殿堂级的工具。这个实验室呢，也很值得我们尊敬。这里涌现过<a href="https://www.lbl.gov/nobelists">13位</a>诺贝尔奖获得者，其中包括1997年获得物理学奖的华人巨匠朱棣文，可见这是多么耀眼的一块科学圣地。</p><!-- [[[read_end]]] --><p>这个地方能做出开创性的技术，确实一点都不令人意外。tcpdump可以工作在各种Unix类的操作系统上，包括Linux、FreeBSD、macOS、Solaris等，也是目前使用最为广泛的抓包工具之一。</p><p>但是tcpdump要过滤报文的话，还要依赖一个底层能力：BPF。</p><ul>\n<li><strong>BPF</strong></li>\n</ul><p>BPF全称是Berkeley Packet Filter（也叫BSD Packet Filter），它是tcpdump等抓包工具的底层基础。在BPF出现之前，虽然各家操作系统都有自己的抓包工具，但也都有这样或那样的不足。比如，有些系统把所有网络报文一股脑儿塞给用户空间程序，开销非常大；而有些系统虽然有报文过滤功能，但是工作很不稳定。</p><p>为了解决这些问题，1992年，也还是在劳伦斯伯克利国家实验室，当初tcpdump的两个作者史蒂文·麦克凯恩（Steven McCanne）和范·雅各布森（Van Jacobson）发表了关于BPF的<a href="https://www.tcpdump.org/papers/bpf-usenix93.pdf">论文</a>，它以一种新的基于寄存器的虚拟机方式，实现了高效稳定的报文过滤功能。从此以后，抓包技术这棵大树有了一个甚为强大的根基，而构建在BPF之上的libpcap、tcpdump等不断枝繁叶茂，进一步使得抓包工作变得方便、稳定，我们这些凡夫俗子才好在这棵大树底下，下棋乘凉。</p><ul>\n<li><strong>libpcap</strong></li>\n</ul><p>BPF实现了抓包虚拟机，但它是如何被用户空间程序使用的呢？于是，libpcap出现了，它提供了API给用户空间程序（包括tcpdump、Wireshark等），使得后者能方便地调用BPF实现抓包过滤等功能。也就是说，libpcap是BPF的一层API封装。</p><p>那么到目前为止，我们应该就能明白tcpdump是怎么工作的了：tcpdump调用了libpcap接口，后者调用BPF实现了报文过滤和抓取。我们来看一下示意图：</p><p><img src="https://static001.geekbang.org/resource/image/8a/2a/8aacf18dedb36c588977881b0a15542a.jpg?wh=1915x762" alt=""></p><ul>\n<li><strong>WinPcap</strong></li>\n</ul><p>Windows上也可以做到类似Linux这样的抓包，其底层就是依赖WinPcap，它是libpcap的Windows版本。微软很早就支持了图形界面抓包工具，从Windows NT时代开始，人们就可以用Network Monitor这个工具在Windows平台上进行网络排查。十多年前，我还在做Windows工程师时，也时常用到这个工具。算起来，我跟抓包工具结缘也有不少年头了（哎，又暴露年龄了）。</p><ul>\n<li><strong>eBPF</strong></li>\n</ul><p>Linux从3.18版本开始支持extended BPF，简称eBPF。这是一个更加通用的内核接口，不仅能支持网络抓包，还能支持网络以外的内核观测点的信息收集等工作。所以事实上，eBPF已经是一个通用工具，而不再局限在网络工具这个角色定位上了。</p><p>同时，也因为它在数据面上的性能很出色，所以现在不少公司正在探索，利用它实现一些数据面的开发工作，比如高性能的负载均衡。相信不久的将来，我们就能看到越来越多的eBPF的应用案例了。</p><h2>为什么抓包文件有好几种类型？</h2><p>如果你留意过抓包文件后缀名的话，会发现有pcap、cap、pcapng这几种不同的后缀名。为什么会有好几种类型呢？下面我来给你说道说道。</p><ul>\n<li><strong>pcap</strong></li>\n</ul><p>这个是libpcap的格式，也是tcpdump和Wireshark等工具默认支持的文件格式。pcap格式的文件中除了报文数据以外，也包含了抓包文件的元信息，比如版本号、抓包时间、每个报文被抓取的最大长度，等等。</p><ul>\n<li><strong>cap</strong></li>\n</ul><p>cap文件可能含有一些libpcap标准之外的数据格式，它是由一些tcpdump以外的抓包程序生成的。比如Citrix公司的netscaler负载均衡器，它的nstrace命令生成的抓包文件，就是以.cap为扩展名的。这种文件除了包含pcap标准定义的信息以外，还包含了LB的前端连接和后端连接之间的mapping信息。Wireshark是可以读取这些.cap文件的，只要在正确的版本上。</p><ul>\n<li><strong>pcapng</strong></li>\n</ul><p>pcap格式虽然满足了大部分需求，但是它也有一些不足。比如，现在多网口的情况已经越来越常见了，我们也经常需要从多个网络接口去抓取报文，那么在抓包文件里，如果不把这些报文跟所属的网口信息展示清楚，那我们的分析，岂不是要张冠李戴了吗？</p><p>为了弥补pcap格式的不足，人们需要有一种新的文件格式，pcapng就出现了。有了它，单个抓包文件就可以包含多个网络接口上，抓取到的报文了。</p><p><img src="https://static001.geekbang.org/resource/image/11/ce/11d149c07854bc496f79e039353046ce.jpg?wh=887x136" alt=""></p><p>我们可以看到，上图中右边的pcapng格式是包含报文的网络接口信息的，而左边的pcap就没有。</p><p>当然，pcapng还有很多别的特性，比如更细粒度的报文时间戳、允许对报文添加注释、更灵活的元数据，等等。如果你是用版本比较新的Wireshark和tshark做抓包，默认生成的抓包文件就已经是pcapng格式了。</p><h2>tcpdump怎么用？</h2><p>好了，现在我们对抓包分析的基础知识就有一定的了解了，下面我们就来着重学习下其中的重点，也就是tcpdump和Wireshark这两个分析工具。</p><p>我们先来看看tcpdump。</p><h3>tcpdump的基本用法</h3><p>虽然tcpdump有完备的文档和命令手册，但如果你不经常抓包，并不能掌握得特别熟练。我个人的经验是，<strong>抓包技术课是一门实践课，不是理论课</strong>。既然是实践课，就要多多实践，从鲜活的案例中积累起来的经验无比宝贵。</p><p>在这门课程里，我也会把自己过往多年的实践总结，毫无保留地分享给你。今天这节课，我们先初步学一些技巧，当作“开胃菜”，希望合你胃口。</p><p><strong>如何抓取报文？</strong></p><p>用tcpdump抓取报文，最常见的场景是要抓取去往某个ip，或者从某个ip过来的流量。我们可以用host {对端IP} 作为抓包过滤条件，比如：</p><pre><code class="language-plain">tcpdump host 10.10.10.10\n</code></pre><p>另一个常见的场景是抓取某个端口的流量，比如，我们想抓取SSH的流量，那么可以这样：</p><pre><code class="language-plain">tcpdump port 22\n</code></pre><p>还有不少参数我们也经常用到，比如：</p><ul>\n<li>-w 文件名，可以把报文保存到文件；</li>\n<li>-c 数量，可以抓取固定数量的报文，这在流量较高时，可以避免一不小心抓取过多报文；</li>\n<li>-s 长度，可以只抓取每个报文的一定长度，后面我会介绍相关的使用场景；</li>\n<li>-n，不做地址转换（比如IP地址转换为主机名，port 80转换为http）；</li>\n<li>-v/-vv/-vvv，可以打印更加详细的报文信息；</li>\n<li>-e，可以打印二层信息，特别是MAC地址；</li>\n<li>-p，关闭混杂模式。所谓混杂模式，也就是嗅探（Sniffering），就是把目的地址不是本机地址的网络报文也抓取下来。</li>\n</ul><p>这里还有个-X参数的用法，我用视频形式给你介绍一下。</p><p><video poster="https://media001.geekbang.org/45902b9bd771488081941ca29b0de23b/snapshots/cac6d1240502485db470aaae66378bcb-00005.jpg" preload="none" controls=""><source src="https://media001.geekbang.org/customerTrans/7e27d07d27d407ebcc195a0e78395f55/e1f3354-17e540c3c16-0000-0000-01d-dbacd.mp4" type="video/mp4"><source src=" https://media001.geekbang.org/5bf575bfc5c8441c87df5c27248a80c2/fe1313c1bcfd43f0b8ed938bdda35726-24243905bb2865feb941a544b3035e26-sd.m3u8" type="application/x-mpegURL"></video></p><p><strong>如何过滤报文？</strong></p><p>最近我们有个实际的需求，要统计我们某个HTTPS VIP的访问流量里，TLS版本（现在主要是TLS1.0、1.1、1.2、1.3）的分布。为了控制抓包文件的大小，我们又不想什么TLS报文都抓，只想抓取TLS版本信息。这该如何做到呢？要知道，针对这个需求，tcpdump本身没有一个现成的过滤器。</p><p>其实，BPF本身是基于偏移量来做报文解析的，所以我们也可以在tcpdump中使用这种偏移量技术，实现我们的需求。下面这个命令，就可以抓取到TLS握手阶段的Client Hello报文：</p><pre><code class="language-plain">tcpdump -w file.pcap \'dst port 443 &amp;&amp; tcp[20]==22 &amp;&amp; tcp[25]==1\'\n</code></pre><p>我给你解释一下上面的三个过滤条件。</p><ul>\n<li>dst port 443：这个最简单，就是抓取从客户端发过来的访问HTTPS的报文。</li>\n<li>tcp[20]==22：这是提取了TCP的第21个字节（因为初始序号是从0开始的），由于TCP头部占20字节，TLS又是TCP的载荷，那么TLS的第1个字节就是TCP的第21个字节，也就是TCP[20]，这个位置的值如果是22（十进制），那么就表明这个是TLS握手报文。</li>\n<li>tcp[25]==1：同理，这是TCP头部的第26个字节，如果它等于1，那么就表明这个是Client Hello类型的TLS握手报文。</li>\n</ul><p>下面是抓包文件里的样子：</p><p><img src="https://static001.geekbang.org/resource/image/6d/5e/6d4ba9b7f87424a24297166fa6a47f5e.jpg?wh=1288x1068" alt=""></p><p>这里你可能会有疑问：上面的图里，TCP[20]的位置的值不是16吗？其实，这个是十六进制的16，换算成十进制，就是22了。</p><p>我又画了一张示意图来表示报文偏移量及其含义，希望能帮你理解其中的奥妙：</p><p><img src="https://static001.geekbang.org/resource/image/9e/b9/9e3e35c5e706096d4a57eaed5a16d5b9.jpg?wh=2000x947" alt=""></p><p>不过看到这里，你会不会忽然觉得tcpdump有点陌生了？怎么使用门槛这么高了呢？</p><p>还好，不是每个过滤条件都要这么“艰难”的。对一些常见的过滤场景，tcpdump也<strong>预定义</strong>了一些相对方便的过滤器。比如我们想要过滤出TCP RST报文，那么可以用下面这种写法，相对来说比用数字做偏移量的写法，要更加容易理解和记忆：</p><pre><code class="language-plain">tcpdump -w file.pcap \'tcp[tcpflags]&amp;(tcp-rst) != 0\'\n</code></pre><p>如果是用偏移量的写法，会是下面这样：</p><pre><code class="language-plain">tcpdump -w file.pcap \'tcp[13]&amp;4 != 0\'\n</code></pre><p><strong>如何在抓包时显示报文内容？</strong></p><p>有时候你想看TCP报文里面的具体内容，比如应用层的数据，那么你可以用-X这个参数，以ASCII码来展示TCP里面的数据：</p><pre><code class="language-plain">$ sudo tcpdump port 80 -X\n......\n05:06:57.394573 IP _gateway.52317 &gt; victorebpf.http: Flags [P.], seq 1:17, ack 1, win 65535, length 16: HTTP: GET / HTTP/1.1\n\t0x0000:&nbsp; 4500 0038 282d 0000 4006 3a83 0a00 0202&nbsp; E..8(-..@.:.....\n\t0x0010:&nbsp; 0a00 020f cc5d 0050 0502 3a02 3ed1 3771&nbsp; .....].P..:.&gt;.7q\n\t0x0020:&nbsp; 5018 ffff 4421 0000 4745 5420 2f20 4854&nbsp; P...D!..GET./.HT\n\t0x0030:&nbsp; 5450 2f31 2e31 0d0a&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; TP/1.1..\n</code></pre><p>上面是我发起了一个简单的HTTP请求时候，服务端的抓包。我把一些内容略去了，保留了需要的报文。你可以拖动这块代码区域，在右边看到“<strong>GET / HTTP/1.1</strong>”字符串，这正是我从客户端发送过来的请求。</p><p><strong>如何读取抓包文件？</strong></p><p>这个比较简单，tcpdump加上-r参数和文件名称，就可以读取这个文件了，而且也可以加上过滤条件。比如：</p><pre><code class="language-plain">tcpdump -r file.pcap \'tcp[tcpflags] &amp; (tcp-rst) != 0\'\n</code></pre><p><strong>如何过滤后转存？</strong></p><p>有时候，我们想从抓包文件中过滤出想要的报文，并转存到另一个文件中。比如想从一个抓包文件中找到TCP RST报文，并把这些RST报文保存到新文件。那么就可以这么做：</p><pre><code class="language-plain">tcpdump -r file.pcap \'tcp[tcpflags] &amp; (tcp-rst) != 0\' -w rst.pcap\n</code></pre><p><strong>如何让抓包时间尽量长一点？</strong></p><p>前面我提到过-s这个长度参数，它的使用场景其实就包括了<strong>延长抓包时间</strong>。我们给tcpdump加上-s参数，指定抓取的每个报文的最大长度，就节省抓包文件的大小，也就延长了抓包时间。</p><p>一般来说，帧头是14字节，IP头是20字节，TCP头是20~40字节。如果你明确地知道这次抓包的重点是传输层，那么理论上，对于每一个报文，你只要抓取到传输层头部即可，也就是前14+20+40字节（即前74字节）：</p><pre><code class="language-plain">tcpdump -s 74 -w file.pcap\n</code></pre><p>而如果是默认抓取1500字节，那么生成的抓包文件将是上面这个抓包文件的20倍。反过来说，使用同样的磁盘空间，上面这种方式，其抓包时间可以长达默认的20倍！</p><p>但是，如果你还想看TLS甚至更上层的应用层的信息，这么做就不行了。比如下面这个抓包，我就是用了74字节作为每个报文的抓取长度，所以第五层开始的信息，就看不到或者看不全了。</p><p><img src="https://static001.geekbang.org/resource/image/42/68/422b4f548bb8b392ca25d6406e977668.jpg?wh=1860x548" alt=""></p><p>显然，TLS只分到了8个字节，信息不完整，所以Wireshark也无能为力，没法告诉我们这个TLS头部里的信息了。</p><p>那么，如果你怀疑TLS或者更上面的应用层也跟问题有关，我建议你就去掉size的限制，还是抓取全部数据为好。</p><p>这是因为，<strong>应用层头部的长度跟二到四层的情况非常不同</strong>，应用层头部可能非常大，甚至超过了TCP的MSS。比如HTTP头部，小的话可能只有几十个字节，大的话可能要几十个KB，也就是好多个TCP segment才放得下。这种时候，要是我们还在纠结如何节省抓取的报文长度，却放过了可能真正对排查有关键价值的数据，就得不偿失了。</p><h3><strong>还有tcptrace这个工具？</strong></h3><p>不过，有时候我们并不方便用Wireshark打开抓包文件做分析，比如抓包的机器不允许向外传文件，也就是可能只能在这台机器上做分析。我们可以用tcpdump -r的方式，打开原始抓包文件看看：</p><pre><code class="language-plain">$ tcpdump -r test.pcap | head -10\nreading from file test.pcap, link-type EN10MB (Ethernet)\n03:55:10.769412 IP victorebpf.51952 &gt; 180.101.49.12.https: Flags [S], seq 3448043385, win 64240, options [mss 1460,sackOK,TS val 237167733 ecr 0,nop,wscale 7], length 0\n03:55:10.779061 IP 180.101.49.12.https &gt; victorebpf.51952: Flags [S.], seq 156800001, ack 3448043386, win 65535, options [mss 1460], length 0\n03:55:10.779111 IP victorebpf.51952 &gt; 180.101.49.12.https: Flags [.], ack 1, win 64240, length 0\n03:55:10.784134 IP victorebpf.51952 &gt; 180.101.49.12.https: Flags [P.], seq 1:518, ack 1, win 64240, length 517\n03:55:10.784297 IP 180.101.49.12.https &gt; victorebpf.51952: Flags [.], ack 518, win 65535, length 0\n03:55:10.795094 IP 180.101.49.12.https &gt; victorebpf.51952: Flags [P.], seq 1:1502, ack 518, win 65535, length 1501\n03:55:10.795118 IP victorebpf.51952 &gt; 180.101.49.12.https: Flags [.], ack 1502, win 62739, length 0\n03:55:10.795327 IP 180.101.49.12.https &gt; victorebpf.51952: Flags [P.], seq 1502:3881, ack 518, win 65535, length 2379\n03:55:10.795356 IP victorebpf.51952 &gt; 180.101.49.12.https: Flags [.], ack 3881, win 61060, length 0\n03:55:10.802868 IP 180.101.49.12.https &gt; victorebpf.51952: Flags [P.], seq 3881:4228, ack 518, win 65535, length 347\n</code></pre><p>报文都是按时间线原样展示的，缺乏逻辑关系，是不是难以组织起有效的分析？比如，要搞清楚里面有几条TCP连接都不太容易。这时候怎么办呢？</p><p>其实，还有一个工具能帮上忙，它就是tcptrace。它不能用来抓包，但是可以用来分析。知道这个工具的人不算很多，但其实tcptrace是一个挺“古老”的工具了。在Wireshark工具集（Wireshark图形界面和tshark等命令行工具）还没一统江湖的时候，tcptrace也有其独到的价值，因为它不仅可以读取pcap格式的抓包文件，也可以读取snoop等其他格式的抓包文件。</p><p>比如下面这样，tcptrace告诉我们，这个抓包文件里有2个TCP连接，并且是以RST结束的：</p><pre><code class="language-plain">$ tcptrace -b test.pcap\n1 arg remaining, starting with \'test.pcap\'\nOstermann\'s tcptrace -- version 6.6.7 -- Thu Nov&nbsp; 4, 2004\n\n145 packets seen, 145 TCP packets traced\nelapsed wallclock time: 0:00:00.028527, 5082 pkts/sec analyzed\ntrace file elapsed time: 0:00:04.534695\nTCP connection info:\n&nbsp; 1: victorebpf:51952 - 180.101.49.12:443 (a2b)&nbsp; &nbsp;15&gt;&nbsp; &nbsp;15&lt;&nbsp; (complete)&nbsp; (reset)\n&nbsp; 2: victorebpf:56794 - 180.101.49.58:443 (c2d)&nbsp; &nbsp;56&gt;&nbsp; &nbsp;59&lt;&nbsp; (complete)&nbsp; (reset)\n</code></pre><h2>初识Wireshark</h2><p>经过前面这么多的铺垫，终于，Wireshark这位女神要掀开神秘的面纱了。接下来，我会给你讲讲Wireshark的历史渊源，然后聊聊使用Wireshark中容易产生的一些小疑问，让你能对这个工具有一个立体式的认知。这样，你下次再使用Wireshark时，就能增加很多信心了。</p><h3>Wireshark的前世今生</h3><p>毫无疑问，Wireshark是世界上最受欢迎的开源网络分析软件了。几乎我们做网络排查中，稍微复杂一点的情况，都会用到它。我经常会遇到，有工程师用略带傲娇的语气说：“我用Wireshark分析过了，这个问题的根因肯定是xxx”。</p><p>简单来说，能用Wireshark做分析，一则确实很管用，二则也“挺显档次”的。我有时候面试一些候选人，谈到Wireshark时，对方多半都会告诉我他们用过Wireshark，或者直接简历上就写着“擅长用Wireshark解决问题”。虽然真的问起来，每个人掌握的程度还是差异很大。不过，对于Wireshark的认同度，我相信没有人质疑。</p><p>可能很多人并不知道，最初Wireshark并不叫这个名字，这个项目原来的名字叫Ethereal。在英文中，ethereal这个词本意是“缥缈的，超凡的”，好像跟网络没有直接关系。不过巧合的是，以太网的英文是Ethernet，而这个项目的本意是为了还原网络的真实（real）情况，所以Ethereal可以理解为Ethernet + real。</p><p>1998年，在网络公司工作的小伙杰拉尔德·库姆斯（Gerald Combs）发布了Ethereal，本意是为了解决他自己在工作中解析网络协议的需求。因为当时商业的网络分析软件很贵，一个License就要几千美元，所以他就想自己写一个。嗯，觉得某个东西不好用就自己做一个，这好像也是很多牛人的普遍特征。</p><p>没想到Ethereal发布后，很快得到了越来越多的支持，并逐步成为了工程师们最喜爱的网络分析工具之一。在2006年，因为Gerald换了工作，但当时Ethereal的商标权还在原先的公司，所以他不得已就新启动了名为Wireshark的这个项目，继续他在这个工具上的投入。当时的宣告在<a href="https://www.linux.com/news/ethereal-changes-name-wireshark">这里</a>可以看到。</p><p>Wireshark既可以分析抓包文件，也可以直接用来抓包，起到跟tcpdump类似的作用。而且比tcpdump方便的是，如果直接用Wireshark发起抓包，窗口里就直接显示抓取到的报文了，这省去了tcpdump抓包后，再用Wireshark打开的小小的麻烦。</p><p>虽然只节省了一小步，但我们能在Wireshark图形界面里看到报文不断充实进来，这种感觉还是比较美妙的。</p><p>这里你也可以通过一个简短的视频，初步了解Wireshark。</p><p><video poster="https://media001.geekbang.org/07af1c1de9a949c8bc8601982cedf9a2/snapshots/70e079ce33684d9192ea4bdee3d1f52f-00004.jpg" preload="none" controls=""><source src="https://media001.geekbang.org/customerTrans/7e27d07d27d407ebcc195a0e78395f55/1e87a7aa-17e540c3973-0000-0000-01d-dbacd.mp4" type="video/mp4"><source src=" https://media001.geekbang.org/f7eca2d28e1b46b69a5764beab4d9322/a128a979a1a74f3f988240e5040dcf44-2903f01268be9809816009cc620ae761-sd.m3u8" type="application/x-mpegURL"></video></p><h3>Wireshark的一些知识和使用技巧</h3><p>下面，我们一起来探讨一些使用Wireshark时容易遇到的问题，帮你减少使用Wireshark时的“心理压力”，更好地跟这位女神“交朋友”。</p><ul>\n<li><strong>怎么知道抓包文件是在哪一端抓取的？</strong></li>\n</ul><p>这是一个有趣的问题。尽管我们做抓包的时候很清楚是在哪端做了抓包，但是把文件传给别人后，对方就未必知道这一点，甚至我们自己过段时间也迷糊了：我上次这个抓包文件到底是在客户端上，还是服务端上抓取的？</p><p>要搞清楚这一点也很简单，我们可以<strong>利用IP的TTL属性</strong>。显然，无论是哪一端，它的报文在发出时，其TTL就是原始值，也就是64、128、255中的某一个。而对端报文的TTL，因为会经过若干个网络跳数，所以一般都会比64、128、255这几个数值要小一些。</p><p>所以，我们只要看一下抓包文件中任何一个客户端报文（也就是源端口为高端口）的TTL，如果它是64、128或255，那说明这个抓包文件就是在客户端做的。反之，就是在服务端做的。</p><p><img src="https://static001.geekbang.org/resource/image/f3/ab/f38c4c40b4046b4ff935a8b2f9d03bab.jpg?wh=795x643" alt=""></p><ul>\n<li><strong>如何定位到应用层的请求和返回的报文？</strong></li>\n</ul><p>在众多报文中找到应用层请求和对应的响应报文，这个任务用人工去做的话是很繁琐的。还好，用Wireshark就很方便。在Wireshark界面中，我们很容易找到请求和返回的报文。比如这样：</p><p><img src="https://static001.geekbang.org/resource/image/47/e8/477eeaa1d3ab5cd592416c97589184e8.jpg?wh=893x150" alt=""></p><p>我们只要选中请求报文，Wireshark就会自动帮我们匹配到对应的响应报文，反过来也一样。从图上看，应用层请求（这里是HTTP请求）是一个向右的箭头，表示数据是进来的方向；应用层响应是一个向左的箭头，表示数据是出去的方向。</p><ul>\n<li><strong>只截到报文的一部分，这个问题有什么影响吗？</strong></li>\n</ul><p>有时候我们会遇到这种报错，这不免让人担心：这文件是真的有什么问题吗？</p><p><img src="https://static001.geekbang.org/resource/image/8a/2c/8a03036ff63c66cdf1cbb93e28e7592c.jpg?wh=532x269" alt=""></p><p>实际上，这不是什么大的问题，并不影响整体的抓包分析。造成这个报错的原因是，当时tcpdump程序并不是用Ctrl+C等正常方式终止的，而是可能用了操作系统的SIGKILL信号，比如用了kill -9命令。这样的话，tcpdump就被强行终止了，导致一部分被抓取的报文还在内存中，还没来得及由tcpdump程序正常写入到pcap文件里。</p><p>要避免这个报错，我们可以在停止tcpdump时，用正常退出的方式，比如Ctrl+C，或者timeout命令，或者kill但不要用-9作为参数。</p><ul>\n<li><strong>乱序一定会引起问题吗？</strong></li>\n</ul><p>乱序（Out-of-Order），也是我们时常能在Wireshark里看到的一类现象。那么，乱序一定会引起问题吗？有一句话叫“脱离了剂量谈毒性就是耍流氓”。其实，乱序是否是问题，也<strong>取决于乱序的严重程度</strong>。</p><p>那第二个问题随之就来了：乱序包达到什么程度，就会真的引起问题？</p><p>这个问题，跟<strong>实际场景的具体情况</strong>也有很大的关系，不同的操作系统以及TCP实现细节，都可能有挺大的差异。不过，我还是想正面回答一下这个问题。我的经验是，如果乱序报文达到<strong>10%以上</strong>，就是严重的传输质量问题了，甚至可能导致传输失败，或者应用层的各种卡顿、报错等症状。所以，你可以统计一下乱序包的占比，如果它超过了10%，就要重视了。</p><h2>Windows Network Monitor</h2><p>前面讲的tcpdump和Wireshark，主要是围绕Linux平台的，那Windows平台呢？事实上，从Windows 98/NT开始，Windows上就基于WinPcap实现了抓包工具Network Monitor。它的使用方法和界面，跟Wireshark也比较类似。</p><p>这个工具比较有特点的地方是，<strong>它能抓取到报文跟进程之间的关系</strong>，也就是在抓包文件中，可以查看到特定进程相关的报文。比如在下图的左侧栏，你选中一个进程，右侧展示的就是跟这个进程相关的报文了。</p><p><img src="https://static001.geekbang.org/resource/image/ea/ec/ea99132099b7dbe398758fc9eb383dec.jpg?wh=1838x1236" alt=""></p><p>不过，无论是WinPcap库，还是Network Monitor应用程序，它们的开发工作都已经停滞很久了。所以功能上也比较老旧。比如，Network Monitor的最后一个版本是<a href="https://www.microsoft.com/en-us/download/details.aspx?id=4865">3.4</a>，但那也已经是2010年的事了。建议你还是安装Wireshark好一些。</p><h2>小结</h2><p>这节课，我们一起学习回顾了tcpdump、BPF（包括eBPF）、libpcap，以及几种不同的抓包文件格式（pcap、cap、pcapng），了解了抓包工具的光荣历史和工作机理，这也有助于我们平时更好地使用这些工具。</p><p><img src="https://static001.geekbang.org/resource/image/e2/c3/e2b11ed52468ecde08f25e5fc3cb47c3.jpg?wh=2000x838" alt=""></p><p>另外，我们还需要掌握tcpdump的一些语法，特别是通过一个实际要抓取TLS版本的需求，这节课我们也学到了tcpdump抓包的精髓：<strong>报文偏移量的方法</strong>。这对于我们灵活地设置抓包条件，也提供了扎实的理论基础。</p><p>而在不方便用Wireshark，或者就是想要快速做分析的时候，如果机器上有tcptrace，我们也可以用它直接做一些快速的检查。此外，我们还了解了Wireshark的几个常见场景：</p><ul>\n<li>在不清楚抓包文件是在哪头做的时候，我们可以利用报文的TTL来识别；</li>\n<li>想快速定位应用层（比如HTTP）的请求和响应，可以利用Wireshark的图形提示；</li>\n<li>用Wireshark打开抓包文件，如果遇到cut short in the middle of a packet的提示，也不用担心，因为现在我们知道，这不是大问题；</li>\n<li>Wireshark时常会提示我们有乱序现象，我们也初步了解了这个现象对传输的影响。</li>\n</ul><p>实际上，Wireshark是抓包分析的核心工具，也是这门课程的主角之一。因为其主题十分宏大，在后面的课程里，我还会陆续介绍跟它相关的排查技巧，请耐心等待每一次新课的更新吧。</p><h2>思考题</h2><p>“学而不思则罔”，给你留几道思考题：</p><ol>\n<li>请你用偏移量方法，写一个tcpdump抓取TCP SYN包的过滤表达式。</li>\n<li>如果确定问题是在IP层，tcpdump命令如何写，可以做到既找到IP层的问题，又节约抓包文件大小呢？</li>\n</ol><p>欢迎你把答案写到留言区，我们一起交流讨论。也欢迎你把今天的内容分享给更多的朋友，一同成长和进步。</p>',
        article_title: "02 | 抓包分析技术初探：你会用tcpdump和Wireshark吗？",
      },
    ],
  },
  {
    chapterTitle: "实战一：TCP真实案例揭秘篇",
    children: [
      {
        title: "03 | 握手：TCP连接都是用TCP协议沟通的吗？",
        id: 479163,
        content:
          '<p>你好，我是胜辉。</p><p>在前面预习篇的两节课里，我们一起回顾和学习了网络分层模型与排查工具，也初步学习了一下抓包分析技术。相信现在的你，已经比刚开始的时候多了不少底气了。那么从今天开始，我们就要正式进入TCP这本大部头，而首先要攻破的，就是握手和挥手。</p><p>TCP的三次握手非常有名，我们工作中也时常能用到，所以这块知识的实用性是很强的。更不用说，技术面试里面，无论是什么岗位，似乎只要是技术岗，都可能会问到TCP握手。可见，它跟操作系统基础、编程基础等类似，同属于计算机技术的底座之一。</p><p>握手，说简单也简单，不就是三次握手嘛。说复杂也复杂，别看只是三次握手，中间还是有不少学问的，有些看似复杂的问题，也能用握手的技术来解决。不信你就跟我看这几个案例。</p><h2>TCP连接都是用TCP协议沟通的吗？</h2><p>看到这个小标题，可能你都觉得奇怪了：TCP连接不用TCP协议沟通还用什么呢？</p><p>确实，一般来说TCP连接是标准的TCP三次握手完成的：</p><ol>\n<li>客户端发送SYN；</li>\n<li>服务端收到SYN后，回复SYN+ACK；</li>\n<li>客户端收到SYN+ACK后，回复ACK。</li>\n</ol><p>这里面SYN会在两端各发送一次，表示“我准备好了，可以开始连接了”。ACK也是两端各发送了一次，表示“我知道你准备好了，我们开始通信吧”。</p><!-- [[[read_end]]] --><p>那既然是4个报文，为什么是三次发送呢？显然，服务端的SYN和ACK是合并在一起发送的，就节省了一次发送。这个在英文里叫Piggybacking，就是背着走，搭顺风车的意思。</p><p>如果服务端不想接受这次握手，它会怎么做呢？可能会出现这么几种情况：</p><ol>\n<li>不搭理这次连接，就当什么都没收到，什么都没发生。这种行为，也可以说是“装聋作哑”。</li>\n<li>给予回复，明确拒绝。相当于有人伸手过来想握手，你一巴掌拍掉，真的是非常刚了。</li>\n</ol><p>第一种情况，因为服务端做了“<strong>静默丢包</strong>”，也就是虽然收到了SYN，但是它直接丢弃了，也不给客户端回复任何消息。这也导致了一个问题，就是客户端无法分清楚这个SYN到底是下面哪种情况：</p><ol>\n<li>在网络上丢失了，服务端收不到，自然不会有回复；</li>\n<li>对端收到了但没回，就是刚才说的“静默丢包”；</li>\n<li>对端收到了也回了，但这个回包在网络中丢了。</li>\n</ol><p><img src="https://static001.geekbang.org/resource/image/43/5a/43eaba402d816fb9eac71de24062695a.jpg?wh=2000x777" alt=""></p><p>你看，就这么简单的一个SYN，还能引申出三种状况出来。感觉什么东西一沾上网络，就要变成麻烦事啊。所以，跟我们在<a href="https://time.geekbang.org/column/article/477510">第1讲</a>里学过的一样：设计网络协议真的不简单。</p><p>那么，从客户端的角度，对于SYN包发出去之后迟迟没有回应的情况，它的策略是做<strong>重试</strong>，而且不止一次。那会重试几次呢？重试多久呢？这个问题，一下子还不太好回答。不过，有tcpdump帮忙，我们可以搞清楚重试的问题，也可以搞清楚“TCP连接是否都用TCP协议沟通”的问题。</p><h3>动手实验</h3><p>你可以借助Iptables和tcpdump做个实验，来验证这件事。你需要一台测试用的服务端，安装Ubuntu等Linux类系统，然后用你的笔记本作为客户端发起测试。这里我也放了一个视频，展示了这个实验过程，你可以结合着对照来看。</p><p><video poster="https://media001.geekbang.org/8182de625b084859a9c4a0b294528730/snapshots/2423524caf814063bfc94b8e03df7bf6-00005.jpg" preload="none" controls=""><source src="https://media001.geekbang.org/customerTrans/7e27d07d27d407ebcc195a0e78395f55/3ecf30cd-17e63c17e92-0000-0000-01d-dbacd.mp4" type="video/mp4"><source src=" https://media001.geekbang.org/6d45e3e01cdc4a38a52f112f2a9094ba/afa53eaae3254628bfb9f4499232d026-a5eb420ed8725cc8ca8e05545e274b0b-sd.m3u8" type="application/x-mpegURL"></video></p><blockquote>\n<p>注意：在这个视频中，我是直接在tcpdump窗口里解读抓包结果的，而在下面我们是用Wireshark来解读，思路其实是一样的，只是操作方式略有不同，正好你可以都学习一下。</p>\n</blockquote><p>第一步，在服务端，执行下面的这条命令，让Iptables静默丢弃掉发往自己80端口的数据包：</p><pre><code class="language-plain">Iptables -I INPUT -p tcp --dport 80 -j DROP\n</code></pre><p>第二步，在客户端启动tcpdump抓包：</p><pre><code class="language-plain">sudo tcpdump -i any -w telnet-80.pcap port 80\n</code></pre><p>第三步，从客户端发起一次telnet：</p><pre><code class="language-plain">telnet 服务端IP 80\n</code></pre><p>这个时候，这个telnet会挂起：</p><p><img src="https://static001.geekbang.org/resource/image/be/79/bea7a95e1c15c9abcd6d14bcb2a90979.jpg?wh=644x104" alt=""></p><p>大约一两分钟后才会失败退出，你随后就会明白背后发生了什么。</p><p>这时，你可以把客户端的tcpdump停掉了（按下Ctrl+C）。然后用Wireshark打开这个抓包文件，看看里面是什么：</p><p><img src="https://static001.geekbang.org/resource/image/55/8d/552e7cb1263ffe018c3bc5cf946b3f8d.jpg?wh=1740x330" alt=""></p><p>telnet挂起的原因就在这里：<strong>握手请求一直没成功</strong>。客户端一共有7个SYN包发出，或者说，除了第一次SYN，后续还有6次重试。客户端当然也不是“傻子”，这么多次都失败，就放弃了连接尝试，把失败的消息传递给了用户空间程序，然后就是telnet退出。</p><p>这里有个信息很值得我们关注。第二列是数据包之间的时间间隔，也就是1秒，2秒，4.2秒，8.2秒，16.1秒，33秒，每个间隔是上一个的<strong>两倍</strong>左右。到第6次重试失败后，客户端就彻底放弃了。</p><p>显然，这里的翻倍时间，就是“<strong>指数退避</strong>”（<a href="https://en.wikipedia.org/wiki/Exponential_backoff">Exponential backoff</a>）原则的体现。这里的时间不是精确的整秒，因为指数退避原则本身就不建议在精确的整秒做重试，最好是有所浮动，这样可以让重试成功的机会变得更大一些。</p><p>这里实际上也是一个知识点了：<strong>TCP握手没响应的话，操作系统会做重试</strong>。在Linux中，这个设置是由内核参数net.ipv4.tcp_syn_retries控制的，默认值为6，也就是我们前面刚观察到的现象。以下就是我的Ubuntu 20.04测试机的配置：</p><pre><code class="language-plain">$ sudo sysctl net.ipv4.tcp_syn_retries\nnet.ipv4.tcp_syn_retries = 6\n</code></pre><p>还有另外好几个有关TCP重试的设置值，也都可以调整。更全面的内容呢，你可以直接<strong>man tcp</strong>，查看tcp的内核手册的信息。比如下面就是对于tcp_syn_retries的解释：</p><pre><code class="language-plain">tcp_syn_retries (integer; default: 5; since Linux 2.2)\n&nbsp; &nbsp; &nbsp; &nbsp;The&nbsp; maximum&nbsp; number of times initial SYNs for an active TCP connection attempt will be retransmitted.&nbsp; This value should not be higher than 255.&nbsp; The default value is 5, which corresponds to approximately 180 seconds.\n</code></pre><p>既然静默丢包会引起客户端空等待的问题，那我们直接拒绝，应该就能解决这个问题了吧？</p><p>正好，Iptables的规则动作有好几种，前面我们用DROP，那这次我们用REJECT，这应该能让客户端立刻退出了。执行下面的这条命令，让Iptables拒绝发到80端口的数据包：</p><pre><code class="language-plain">Iptables -I INPUT -p tcp --dport 80 -j REJECT\n</code></pre><p>跟前面的实验一样，我们在客户端发起telnet 服务端IP 80。果然，telnet立刻退出，显示：</p><pre><code class="language-plain">$ telnet 47.94.129.219 80\nTrying 47.94.129.219...\ntelnet: connect to address 47.94.129.219: Connection refused\ntelnet: Unable to connect to remote host\n</code></pre><p>可见，连接请求确实被拒绝了。我在telnet同时也抓了包，我们来看一下抓包文件：</p><p><img src="https://static001.geekbang.org/resource/image/cd/36/cd7228b8a38ec807263d980d5887d136.jpg?wh=1556x184" alt=""></p><p>奇怪，抓包文件里并没有期望的TCP RST？是我们抓包命令没写对吗？下面是这条命令，你已经初步学过tcpdump抓包命令了，看看有没有什么问题？</p><pre><code class="language-plain">sudo tcpdump -i any -w telnet-80-reject.pcap host 47.94.129.219 and port 80\n</code></pre><p>命令语法没问题，要不然命令都无法执行。那过滤条件呢？指定了远端IP和端口，这是很常见的用法，应该也没什么问题。</p><p>但是，<strong>这里隐藏了一个假设的前提</strong>，也就是我们认为，这次握手的所有过程都是通过这个80端口进行的。但事实上呢？我们稍微改一下抓包条件，只保留远端IP，去掉端口的限制：</p><pre><code class="language-plain">sudo tcpdump -i any -w telnet-80-reject.pcap host 47.94.129.219\n</code></pre><p>然后再来看看，我们抓到的报文是怎样的：</p><p><img src="https://static001.geekbang.org/resource/image/e2/83/e2b17ee170d80dbdbb0efb9d0828a183.jpg?wh=1542x126" alt=""></p><p>很意外，居然对端回复了一个<strong>ICMP消息：Destination unreachable (Port unreachable)</strong>。这还不是最意外的，我们选中这个报文，进一步看它的详情，可能会更惊讶：</p><p><img src="https://static001.geekbang.org/resource/image/1e/e9/1e63ce285a59f917149d4d55fed5a1e9.jpg?wh=1646x1200" alt=""></p><p>原来，这个ICMP消息不仅通过type=3表示，这是一个“端口不可达”的错误消息，而且在它的payload里面，还携带了完整的TCP握手包的信息，而这个握手包，可是客户端发过来的。</p><blockquote>\n<p>补充一下：如果我们回头再检查一下前面生成的Iptables规则，它是这样的：</p>\n</blockquote><pre><code>-A INPUT -p tcp -m tcp --dport 80 -j REJECT --reject-with icmp-port-unreachable\n</code></pre><blockquote>\n<p>原来，它自动补上了–reject-with icmp-port-unreachable，也就是说确实用ICMP消息做了回复。当然，你还可以把这个动作定义为–reject-with tcp-reset，那样的话就符合我们一开始的期望了。<br>\n&nbsp;<br>\n事实上，无论是收到TCP RST还是ICMP port unreachable消息，客户端的connect()调用都是返回ECONNREFUSED，这就是telnet都报“connection refused”的深层次原因。</p>\n</blockquote><p>所以，这个握手失败的情况终于搞清楚了，它是这么发生的：</p><p><img src="https://static001.geekbang.org/resource/image/3a/29/3af3ae7d7ed9a6662dd66a6beafd1829.jpg?wh=2000x702" alt=""></p><p>TCP握手拒绝这个事，竟然可以是<strong>ICMP报文</strong>来达成的。“握手过程用TCP协议做沟通”，看起来这么理所当然的事情居然也会反转，你是不是也有点自我怀疑了：是不是其他网络知识，也未必是我自己认为的那样呢？</p><p>这个知识点，其实是几年前我在处理一个客户的TCP连接问题时遇到的。剧情么，前面已经给你“演”过一遍了。当时我也深感TCP的水太深，快没过脖子了，甚至有点喘不过气来……从此以后，我再也不敢小看任何知识点，同时也领教了tcpdump和Wireshark在网络分析方面的威力。有了这两个大杀器的帮助，我的网络水平提高很快。这个经验我也分享给你，相信你也一定能从中受益。</p><h2>Windows服务器加域报RPC service unavailable?</h2><p>虽然tcpdump + Wireshark的组合威力强大，但用起来总是会稍微花点时间。<strong>有没有不用抓包分析，也能做排查TCP连接问题的方法呢？</strong>这样也好快一点啊。接下来这个例子，就是这样的。</p><p>我们eBay也有不少Windows服务器，这些机器都由Active Directory（简称AD）管理。有一次，我们有一台Windows服务器加入AD失败，相关同事已经排查了好久，一直没找到原因。操作过程就是最普通的加域动作：</p><p><img src="https://static001.geekbang.org/resource/image/76/aa/76770d2be1c188dfafd69c906cb5b1aa.jpg?wh=325x365" alt=""></p><p>然后，一开始显示加域成功，但是过一两分钟后，又会来个“回马枪”，冒出来一个The RPC server is unavailable的报错：</p><p><img src="https://static001.geekbang.org/resource/image/59/7f/59793b82eyy0e8855e1879e5943f057f.jpg?wh=255x138" alt=""></p><p>在Windows的体系里面，这个报错大体意思是连不上RPC服务器。同事检查过RPC服务端并没有问题，然后其他Windows客户端加域呢，也都正常，唯独这台就不行。</p><p>单独一台机器加不了域，本身也不是特别大的麻烦，但是同事还是想找一下根因，于是就让我帮忙。很幸运，当时我只用了大概十分钟就找到了原因（这里我有点不谦虚了，我对你扔过来的鸡蛋和番茄表示接受）。</p><p>这倒不是我对Windows多么精通，<strong>主要是正确的排查思路帮助了我</strong>。给你分享一下我当时的思路：</p><ol>\n<li>既然报错是RPC unavailable，那可能意味着有一个RPC服务没有得到响应。</li>\n<li>没有得到服务端的响应，那多半是跟网络有关系，特别是跟端口的连通性有关系。</li>\n<li>要知道，RPC使用的是动态端口，每次连接都可能连接到不同的服务端口。所以，我也<strong>没办法预先知道是具体哪几个端口</strong>，如果我知道的话，直接找防火墙团队去把那几个服务端口打开就好了，但这个做不到。这一点也是同事卡了许久的原因之一，他也不知道如何找到这些“动态会变的RPC端口”。</li>\n<li>要找到<strong>实时在用</strong>的动态RPC端口，最方便的方法就是<strong>运行netstat命令</strong>。无论连接是处在什么状态，比如是在传输数据的ESTABLISHED状态、新近关闭端口的TIME_WAIT状态，都可以用netstat命令看到。</li>\n<li>我运行了netstat，在当时的命令输出中，我注意到有一个 <strong>SYN_SENT状态</strong>的连接，它要连的就是服务端的一个高端口。</li>\n</ol><p>那么，这个SYN_SENT状态究竟说明了什么呢？</p><p><img src="https://static001.geekbang.org/resource/image/bd/46/bdd5af2530ef0b810863947af22be046.jpg?wh=633x23" alt=""></p><p>SYN_SENT是TCP的11个状态之一。要理解SYN_SENT的含义，我们首先要把整个TCP状态机的机制搞清楚。关于TCP状态机，目前流传比较广的是下面这张图。我没有考证过这张图的出处，不过在Stevens的<a href="https://book.douban.com/subject/4859464">《UNIX网络编程：套接字联网API》</a>里就有这张图，很有可能最早就是来自于Stevens：</p><p><img src="https://static001.geekbang.org/resource/image/69/55/6918091f084186bb968180c2e7aacf55.jpg?wh=754x858" alt=""></p><p>这张图浓缩了TCP状态转换的所有知识点，确实值得反复研读。不过，我鸡蛋里挑个骨头：这张图也有个小小的问题，就是对于初学者来说，它并不容易理解。</p><p>比如，多年前我自己在学习TCP的时候，就一直没有彻底看懂这张图。好笑的是，我经常假装自己看懂了，还拿这张图跟别人侃侃而谈，而对方还被我唬住了呢。所以你也要学会了：当大家都不是很懂的时候，你对自己的话越相信，你就越有说服力哦。</p><p>好了，当然是跟你开个玩笑，做学问还是要严谨。那么，这张图的难点在哪呢？我觉得主要是视角不固定，一会是发送方，一会是接收方，对初学者来说很容易混淆。实际上，在Stevens的这本书中，还有另外一张图，我认为更加清晰明了，也是我想推荐给你的：</p><p><img src="https://static001.geekbang.org/resource/image/4b/99/4b5dbdf4365d467cce222c9cf6707099.jpg?wh=607x513" alt=""></p><p>在上面这张图里，无论是客户端或者服务端，我们从上往下看，它要经历的各个TCP状态，都展示得十分清楚。我把这个过程解读如下：</p><p><img src="https://static001.geekbang.org/resource/image/ec/ce/ec2f7ee67f560420bced033059c31ece.jpg?wh=2000x1125" alt=""></p><p>后续的过程，不用我继续解读，你也会看得很清楚了：<strong>分别沿着左边和右边的垂直线从上往下看，就经历了客户端和服务端的TCP生命周期里的各种状态</strong>，这个过程中，视角保持一致。你觉得是否比前面那张转换图，更加容易理解呢？</p><p>看懂了这张图，你应该就明白了：SYN_SENT这个状态，意味着当时这个连接请求（SYN包），已经从这台Windows服务器发出，试图跟远端的AD域控制器进行连接。但由于对端迟迟没有回应SYN+ACK报文，那么客户端这个连接的状态，就只能“停留”在SYN_SENT状态，无法转化为ESTABLISHED状态。</p><p>等到达了SYN timeout时间后，Windows操作系统会放弃这次连接，而这个SYN_SENT状态的连接也会消失不见。所以，前面提到的“<strong>实时</strong>”两字，也是很关键的。如果不是在问题发生时运行netstat，哪怕是过了几分钟再去运行netstat，错过了这个SYN_SENT，我也不能发现这个失败的TCP连接企图，也就无法定位到真正的原因了。</p><p><img src="https://static001.geekbang.org/resource/image/83/18/8357d8df335042e2d781f540c9e85a18.jpg?wh=2000x920" alt=""></p><p>然后我们拿着这个端口去找防火墙团队，对方检查了配置，发现这个端口确实是禁止的。在开通后，问题就解决了。</p><p>所以说，<strong>真的不要小看任何知识点和小工具，你掌握以后，完全可以起到关键性的作用</strong>（对了，排查防火墙也时常是我们工作的痛点，我在第5和第6讲会专门讲解这方面的排查技巧，敬请期待）。</p><p>这里还有一个技术点我想给你展开一下。我们在前面已经讨论过了SYN重试的问题，显然，这次Windows的SYN_SENT的背后，我们相信，应该也是有数次的SYN重试的情况。同时，因为我观察到，这个SYN_SENT停留了大约有十几二十秒，所以我判断应该也有指数退避的存在，所以这个状态才保留了那么长时间。</p><p>也就是说，无论是Linux还是Windows，都实现了类似的TCP握手方面的容错手段。还是那句话：设计网络不容易。<strong>理解了设计者的初心</strong>，很多问题就不会那么模糊了，可能你一下子就能看清。</p><h2>发送的数据还能超过接收窗口？</h2><p>最后一个案例表面上并不直接跟握手相关，但背地里就……不剧透了，看剧情。</p><p>前段时间，有个朋友找到我咨询一个问题。他们最近处理了一个Redis相关的技术问题，让他们既开心又“闹心”。开心的是整体分析是正确的，问题也得以解决；“闹心”的是，唯独有个技术点好像无法自圆其说，所以想让我看看到底是怎么回事。</p><p>这个问题是：Redis服务告诉客户端它的接收窗口是190字节，但是客户端居然会发送308字节，<strong>大大超出了接收窗口</strong>。下图是他们用Wireshark打开抓包文件后的界面：</p><p><img src="https://static001.geekbang.org/resource/image/10/yy/1042be623e0390e3c96a9675a73a45yy.jpg?wh=2122x522" alt=""></p><p>我一开始也懵了：难道TCP的深水又到我脖子这儿了？在我多年的抓包分析经历中，数据超过接收窗口的情况，好像还没有遇到过，这次算是TCP准备再次让我“开开眼”吗？</p><p>不过我很快又稳定了下来，因为我想到了一个朋友他们没有注意到的细节。在说到TCP窗口的时候，一般都会提到一个很重要的概念：<strong>Window Scale</strong>。这是因为，TCP最初是七八十年代的产物，1981年9月定稿的<a href="https://datatracker.ietf.org/doc/html/rfc793">RFC793</a>才第一次正式确定了TCP的标准。当时的网络带宽还处于“石器时代”，机器的带宽只有现在的百分之一，那么TCP接收窗口自然也没必要很大，2个字节长度代表的65535字节的窗口足矣。</p><p>但是后来网络带宽越来越大，65535字节的窗口慢慢就不够用了，于是设计者们又想出了一个巧妙的办法。原先的Window字段还是保持不变，在TCP扩展部分也就是TCP Options里面，增加一个Window Scale的字段，它表示原始Window值的左移位数，最高可以左移14位。</p><p>如果你还没有完全忘记计算机课的基本知识，那么应该明白这是一个非常大的提升了（扩大了2的14次方，即16384倍）。16384乘以65535，这个数字就是1G字节，也就是说，一个启用了Window Scale特性的TCP连接，最大的接收窗口可以达到1GB。可以说，这个数字至今都是够用的。</p><p>说了这么多，我们用Wireshark来看看它究竟长啥样。选中一个SYN报文，在Wireshark窗口中部找到TCP的部分，展开Options就能看到了：</p><p><img src="https://static001.geekbang.org/resource/image/88/17/8818106efb1bd4313928bd66302ef417.jpg?wh=865x626" alt=""></p><p>我们逐一理解下。</p><ul>\n<li>Kind：这个值是3，每个TCP Option都有自己的编号，3代表这是Window Scale类型。</li>\n<li>Length：3字节，含Kind、Length（自己）、Shift count。</li>\n<li>Shift count：6，也就是我们最为关心的窗口将要被右移的位数，2的6次方就是64。</li>\n</ul><blockquote>\n<p>小小提醒：SYN包里的Window是不会被Scale放大的，只有握手后的报文才会。</p>\n</blockquote><p>当然，TCP的窗口也是TCP知识体系里一块挺大的分支领域，我会在当前这个“实战一”模块的传输效率部分，也就是第9~11讲里，详细讲解这方面的知识，帮你把这块的东西真正搞透。</p><p>回到握手。既然Window Scale这么有用，那每个TCP报文应该都是带上这个信息的吧，因为它在TCP头部里面嘛，而每个TCP报文都有头部的，不是吗？</p><p>你要这样想就错了。事实上，Window Scale只出现在TCP握手里面。你再想想就明白了：这个是“控制面”的信息，说一次让双方明白就够了，每次都说，不光显得“话痨”，也很浪费带宽啊。一般传输过程中的报文，完全不需要再浪费这3个字节来传送一个已经同步过的信息。所以，握手之后的TCP报文里面，是不带Window Scale的。</p><p>比如，我们来看一个传输中的报文，比如客户端发送的TLS Client Hello报文：</p><p><img src="https://static001.geekbang.org/resource/image/b8/fe/b8b79b774695579a065b2da5f62a33fe.jpg?wh=1478x800" alt=""></p><p>可见，原始窗口502字节，放大128倍后就是64256字节了。</p><p>说到这里，想必你已经明白了：我朋友这次的疑惑，其实就是<strong>缺少TCP握手包</strong>造成的。要知道，Wireshark也一样要依赖握手包，才能了解到这次连接用的Window Scale值，然后才好在原始Window值的基础上，对Window值进行右移（放大），得出真正的窗口值。于是，因为这次他们的抓包没有抓取到握手报文，所以Wireshark里看到的窗口，就是190字节，而不是190字节的某个倍数了！</p><p>当时通信的另一端当然知道这个信息，所以它发送308字节一点都不意外，因为这个值根本就没超出接收窗口。</p><p>那么，<strong>是不是没有抓取到握手包的话，Wireshark里读取到的Window就一定不对呢？</strong>大部分时候是这样的。不过，还有一部分老系统的TCP栈并没有启用Window Scale，那么抓包文件中有没有握手包都没关系，只要看基本Window就好了。</p><p>说到这里，你对TCP握手的印象，是不是又有改变呢？它简单，也丰富；它靠谱，也调皮。你只有真的读懂它，才不会被它牵着鼻子走。而读懂它的方法是什么呢？</p><p>就是多读些TCP理论，就是多做些抓包分析，就是多处理些案例，更是多走走，多看看。只要有心，你总有机会可以学会，可以成长。</p><h2>小结</h2><p>作为这个模块的第一课，这次我们围绕TCP握手展开了几个有趣的案例，并从中梳理了以下知识点：</p><ol>\n<li>客户端发起的连接请求可能因为各种原因没有回复，这时客户端会做<strong>重试</strong>。一般在Linux里，重试次数默认是6次，内核参数是net.ipv4.tcp_syn_retries。重试间隔遵循了<strong>指数退避原则</strong>。</li>\n<li>服务端拒绝TCP握手，除了用TCP RST，另外一种方式是通过<strong>ICMP Destination unreachable（Port unreachable）消息</strong>。从客户端应用程序看，这两种回复都属于“对端拒绝”，所以应用表面看不出区别，但我们在抓包的时候要注意，如果单纯抓取服务端口的报文，就会漏过这个ICMP消息，可能对排查不利。</li>\n<li>对于连通性相关的问题，除了用tcpdump+Wireshark这个黄金组合，我们还可以在理解TCP握手原理的基础上，使用小工具（比如netstat）来排查。特别是对于RPC服务场景，在问题发生时及时<strong>执行netstat -ant，找到SYN_SENT状态的连接</strong>，这个很可能是突破口。</li>\n<li>我们也学习了如何在Wireshark中查看Window Scale。握手包中的Window Scale信息十分重要，这会帮助我们知道正确的接收窗口。在分析抓包文件时，<strong>要注意是否连接的握手包被抓取到</strong>，没有握手包，这个Window值一般就不准。</li>\n</ol><p>可以说，<strong>应用都靠连接，连接都靠握手。</strong>掌握好了握手，你的TCP就算入门了。学完这节课之后，你有没有觉得，今天的你比昨天的你，要强一些了呢？加油！后面更多的知识在等你来发现。</p><h2>思考题</h2><p>最后，还是按照惯例，还是给你留几道思考题：</p><ol>\n<li>在Linux中，还有一个内核参数也是关于握手的，net.ipv4.tcp_synack_retries。你知道这个参数是用来做什么的吗？</li>\n<li>如果握手双方，一方支持Window Scale，一方不支持，那么在这个连接里，Window Scale最终会被启用吗？你可以参考<a href="https://datatracker.ietf.org/doc/html/rfc1323">RFC1323</a>，给出你的解答。</li>\n</ol><p>欢迎在留言区分享你的答案，如果觉得有收获，也欢迎你把今天的内容分享给更多的朋友。</p><h2>扩展知识：聊聊几个常见误区</h2><p>很多时候，我们的成长不仅是由于学到了正确的知识，更是由于纠正了“错误的认知”。下面列几个常见误区，你看看自己有没有“中招”。</p><h3>UDP也有握手?</h3><p>有些同学会有这个误解，可能是跟nc这个命令有关。我们来看一个TCP端口22的测试：</p><pre><code class="language-plain">victor@victorebpf:~$ nc -v -w 2 47.94.129.219 22\nConnection to 47.94.129.219 22 port [tcp/ssh] succeeded!\n</code></pre><p>同一时间的tcpdump抓包，显示这个TCP经历了成功的握手和挥手：</p><pre><code class="language-plain">$ sudo tcpdump -i any host 47.94.129.219\ntcpdump: verbose output suppressed, use -v or -vv for full protocol decode\nlistening on any, link-type LINUX_SLL (Linux cooked v1), capture size 262144 bytes\n11:58:10.749372 IP victorebpf.51072 &gt; 47.94.129.219.ssh: Flags [S], seq 966857509, win 64240, options [mss 1460,sackOK,TS val 1565897461 ecr 0,nop,wscale 7], length 0\n11:58:10.781734 IP 47.94.129.219.ssh &gt; victorebpf.51072: Flags [S.], seq 3170176001, ack 966857510, win 65535, options [mss 1460], length 0\n11:58:10.781880 IP victorebpf.51072 &gt; 47.94.129.219.ssh: Flags [.], ack 1, win 64240, length 0\n11:58:10.782344 IP victorebpf.51072 &gt; 47.94.129.219.ssh: Flags [F.], seq 1, ack 1, win 64240, length 0\n11:58:10.782586 IP 47.94.129.219.ssh &gt; victorebpf.51072: Flags [.], ack 2, win 65535, length 0\n11:58:10.821202 IP 47.94.129.219.ssh &gt; victorebpf.51072: Flags [P.], seq 1:42, ack 2, win 65535, length 41\n11:58:10.821292 IP victorebpf.51072 &gt; 47.94.129.219.ssh: Flags [R], seq 966857511, win 0, length 0\n</code></pre><p>如果我们用nc测试 <strong>UDP</strong> 22端口，看看会发生什么。注意，UDP 22是没有服务在监听的。但是nc一样告诉我们succeeded！这似乎在告诉我们，这个UDP 22端口确实是在监听的：</p><pre><code class="language-plain">$ nc -v -w 2 47.94.129.219 22\nConnection to 47.94.129.219 22 port [tcp/ssh] succeeded!\nvictor@victorebpf:~$ nc -v -w 2 47.94.129.219 -u 22\nConnection to 47.94.129.219 22 port [udp/*] succeeded!\n</code></pre><p>同一时间的抓包，显示客户端发送了4个UDP报文，但服务端没有任何回复：</p><pre><code class="language-plain">11:59:05.605556 IP victorebpf.54145 &gt; 47.94.129.219.22: UDP, length 1\n11:59:05.605995 IP victorebpf.54145 &gt; 47.94.129.219.22: UDP, length 1\n11:59:06.606436 IP victorebpf.54145 &gt; 47.94.129.219.22: UDP, length 1\n11:59:07.607134 IP victorebpf.54145 &gt; 47.94.129.219.22: UDP, length 1\n</code></pre><p>从表象上看，nc告诉我们：这个跟UDP 22端口的“连接”是成功的，这是nc的Bug吗？可能并不算是。原因就在于，<strong>UDP本身不是面向连接的</strong>，所以没有一个确定的UDP协议层面的“答复”。这种答复，需要由调用UDP的应用程序自己去实现。</p><p>那为什么在这里，nc还是要告诉我们成功呢？可能只是因为对端没有回复ICMP port unreachable。nc的逻辑是：</p><ul>\n<li>对于UDP来说，除非明确拒绝，否则可视为“连通”；</li>\n<li>对TCP来说，除非明确接受，否则视为“不连通”。</li>\n</ul><p>所以，当你下次用nc探测UDP端口，不通的结果是可信的，而能通（succeeded）的结果并不准确，只能作为参考。</p><h3>一台机器最多65535个TCP连接?</h3><p>这也是很常见的误区了。我还是小白的时候，也曾经深信不疑。当时读到一篇讨论服务器可以承受多少TCP连接（就是C10k问题）的文章时，还觉得奇怪，不是端口范围只有0~65535吗？为什么还会有几十万上百万连接呢？</p><p>这就是没有意识到，连接是四元组（咱们在<a href="https://time.geekbang.org/column/article/477510">第一节课</a>讲到过），并不是单纯的源端口或者目的端口。那么多个数相乘，这个乘积当然可以远远超过65535了。先不谈论海量级网站的场景，就算我们维护一台Web服务器，假如当前有10万台客户端连着你，平均每个客户端跟你有6个连接（这很常见），那么就是60万个连接了，是不是也早就超过6万了？</p><p>当然，在限定场景下，一个客户端（假设只有一个出口IP）和一个服务端（假设也只有一个IP和一个服务端口），那么确实只能最多发起6万多个连接。但你自己也已经明白，这跟前面的误解，已经是两回事了。</p><h3>不能同时发起握手?</h3><p>如果两端同时发送了SYN给对方，也就是双方都收到了一个SYN，那么接下来，它们会进入什么状态呢？你可能觉得这应该不行。</p><p>其实，通信双方还真的可以同时向对方发送SYN，也能建立起连接。你可以参考这节课里我提到的<strong>TCP状态转换图</strong>。在Richard Stevens的<a href="https://book.douban.com/subject/1088054">《TCP/IP详解（第一卷）》</a>里，也提到了这个知识点，参考下图：</p><p><img src="https://static001.geekbang.org/resource/image/02/12/0218c45cf90c06983e6e0fa6211f6912.jpg?wh=1384x614" alt=""></p><p>当然，这种情况是很罕见的，你可以参考一下，也丰富一下你对TCP握手的理解。</p>',
        article_title: "03 | 握手：TCP连接都是用TCP协议沟通的吗？",
      },
      {
        title: "04 | 挥手：Nginx日志报connection reset by peer是怎么回事？",
        id: 480068,
        content:
          '<p>你好，我是胜辉。今天这节课，我们要通过实际的案例，来学习下TCP挥手的知识，在实战中加深对这些知识的理解。</p><p>我们在做一些应用排查的时候，时常会在日志里看到跟TCP有关的报错。比如在Nginx的日志里面，可能就有connection reset by peer这种报错。“连接被对端reset（重置）”，这个字面上的意思是看明白了。但是，心里不免发毛：</p><ul>\n<li>这个reset会影响我们的业务吗，这次事务到底有没有成功呢?</li>\n<li>这个reset发生在具体什么阶段，属于TCP的正常断连吗？</li>\n<li>我们要怎么做才能避免这种reset呢？</li>\n</ul><p>要回答这类追问，Nginx日志可能就不够用了。</p><p>事实上，网络分层的好处是在于每一层都专心做好自己的事情就行了。而坏处也不是没有，这种情况就是如此：应用层只知道操作系统告诉它，“喂，你的连接被reset了”。但是为什么会被reset呢？应用层无法知道，只有操作系统知道，但是操作系统只是把事情处理掉，往内部reset计数器里加1，但也不记录这次reset的前后上下文。</p><p>所以，为了搞清楚connection reset by peer时具体发生了什么，我们需要<strong>突破应用层这口井，跳出来看到更大的网络世界</strong>。</p><h2>在应用层和网络层之间搭建桥梁</h2><!-- [[[read_end]]] --><p>首先，你需要理解下connection reset by peer的含义。熟悉TCP的话，你应该会想到这大概是对端（peer）回复了TCP RST（也就是这里的reset），终止了一次TCP连接。其实，这也是我们<strong>做网络排查的第一个要点：把应用层的信息，“翻译”成传输层和网络层的信息</strong>。</p><p>或者说，我们需要完成一件有时候比较有挑战的事情：把应用层的信息，跟位于它下面的传输层和网络层的信息联系起来。</p><p><img src="https://static001.geekbang.org/resource/image/86/0a/869e8f5d68484b4d2a4cac41c901070a.jpg?wh=2000x830" alt=""></p><p>这里我说的“应用层信息”，可能是以下这些：</p><ul>\n<li><strong>应用层日志</strong>，包括成功日志、报错日志，等等；</li>\n<li><strong>应用层性能数据</strong>，比如RPS（每秒请求数），transaction time（处理时间）等；</li>\n<li><strong>应用层载荷</strong>，比如HTTP请求和响应的header、body等。</li>\n</ul><p>而“传输层/网络层信息”，可能是以下种种：</p><ul>\n<li><strong>传输层</strong>：TCP序列号（Sequence Number）、确认号（Acknowledge Number）、MSS、接收窗口（Receive Window）、拥塞窗口（Congestion Window）、时延（Latency）、重复确认（DupAck）、选择性确认（Selective Ack）、重传（Retransmission）、丢包（Packet loss）等。</li>\n<li><strong>网络层</strong>：IP的TTL、MTU、跳数（hops）、路由表等。</li>\n</ul><p>可见，这两大类（应用vs网络）信息的视角和度量标准完全不同，所以几乎没办法直接挂钩。而这，也就造成了问题排查方面的两大鸿沟。</p><ul>\n<li><strong>应用现象跟网络现象之间的鸿沟</strong>：你可能看得懂应用层的日志，但是不知道网络上具体发生了什么。</li>\n<li><strong>工具提示跟协议理解之间的鸿沟</strong>：你看得懂Wireshark、tcpdump这类工具的输出信息的含义，但就是无法真正地把它们跟你对协议的理解对应起来。</li>\n</ul><p>也就是说，<strong>你需要具备把两大鸿沟填平的能力</strong>，有了这个能力，你也就有了<strong>能把两大类信息（应用信息和网络信息）联通起来的“翻译”的能力。</strong>这正是网络排查的核心能力。</p><p>既然我们是案例实战课程，这些知识从案例里面学，是最高效的方法了。接下来，就让我们一起看两个案例吧。</p><h2>案例1：connection reset by peer?</h2><p>前几年我在UCloud服务的时候，有个客户也是反馈，他们的Nginx服务器上遇到了很多connection reset by peer的报错。他们担心这个问题对业务产生了影响，希望我们协助查清原因。客户的应用是一个普通的Web服务，架设在Nginx上，而他们的另外一组机器是作为客户端，去调用这个Nginx上面的Web服务。</p><p>架构简图如下：</p><p><img src="https://static001.geekbang.org/resource/image/2d/32/2d02ff0c0d5416335cf7b6b749715832.jpg?wh=2000x539" alt=""></p><p>前面我说过，单纯从应用层日志来看的话，几乎难以确定connection reset by peer的底层原因。所以，我们就展开了抓包工作。具体做法是：</p><ul>\n<li>我们需要选择一端做抓包，这次是客户端；</li>\n<li>检查应用日志，发现没几分钟就出现了connection reset by peer的报错；</li>\n<li>对照报错日志和抓包文件，寻找线索。</li>\n</ul><p>我们先看一下，这些报错日志长什么样子：</p><pre><code class="language-plain">2015/12/01 15:49:48 [info] 20521#0: *55077498 recv() failed (104: Connection reset by peer) while sending to client, client: 10.255.252.31, server: manager.example.com, request: "POST /WebPageAlipay/weixin/notify_url.htm HTTP/1.1", upstream: "http:/10.4.36.207:8080/WebPageAlipay/weixin/notify_url.htm", host: "manager.example.com"\n2015/12/01 15:49:54 [info] 20523#0: *55077722 recv() failed (104: Connection reset by peer) while sending to client, client: 10.255.252.31, server: manager.example.com, request: "POST /WebPageAlipay/app/notify_url.htm HTTP/1.1", upstream: "http:/10.4.36.207:8080/WebPageAlipay/app/notify_url.htm", host: "manager.example.com"\n2015/12/01 15:49:54 [info] 20523#0: *55077710 recv() failed (104: Connection reset by peer) while sending to client, client: 10.255.252.31, server: manager.example.com, request: "POST /WebPageAlipay/app/notify_url.htm HTTP/1.1", upstream: "http:/10.4.36.207:8080/WebPageAlipay/app/notify_url.htm", host: "manager.example.com"\n2015/12/01 15:49:58 [info] 20522#0: *55077946 recv() failed (104: Connection reset by peer) while sending to client, client: 10.255.252.31, server: manager.example.com, request: "POST /WebPageAlipay/app/notify_url.htm HTTP/1.1", upstream: "http:/10.4.36.207:8080/WebPageAlipay/app/notify_url.htm", host: "manager.example.com"\n2015/12/01 15:49:58 [info] 20522#0: *55077965 recv() failed (104: Connection reset by peer) while sending to client, client: 10.255.252.31, server: manager.example.com, request: "POST /WebPageAlipay/app/notify_url.htm HTTP/1.1", upstream: "http:/10.4.36.207:8080/WebPageAlipay/app/notify_url.htm", host: "manager.example.com"\n</code></pre><blockquote>\n<p>补充：因为日志涉及客户数据安全和隐私，我已经做了脱敏处理。</p>\n</blockquote><p>看起来最“显眼”的，应该就是那句connection reset by peer。另外，我们其实也可以关注一下报错日志里面的其他信息，这也可以帮助我们获取更全面的上下文。</p><ul>\n<li><strong>recv() failed</strong>：这里的recv()是一个系统调用，也就是Linux网络编程接口。它的作用呢，看字面就很容易理解，就是用来接收数据的。我们可以直接<strong>man recv</strong>，看到这个系统调用的详细信息，也包括它的各种异常状态码。</li>\n<li><strong>104</strong>：这个数字也是跟系统调用有关的，它就是recv()调用出现异常时的一个状态码，这是操作系统给出的。在Linux系统里，104对应的是ECONNRESET，也正是一个TCP连接被RST报文异常关闭的情况。</li>\n<li><strong>upstream</strong>：在Nginx等反向代理软件的术语里，upstream是指后端的服务器。也就是说，客户端把请求发到Nginx，Nginx会把请求转发到upstream，等后者回复HTTP响应后，Nginx把这个响应回复给客户端。注意，这里的“客户端&lt;-&gt;Nginx”和“Nginx&lt;-&gt;upstream”是两条独立的TCP连接，也就是下图这样：</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/cc/70/cc2e1bc3669138f6bc0f9222ae4cf770.jpg?wh=2000x504" alt=""></p><blockquote>\n<p>补充：你可能觉得奇怪，明明数据是从外面进入到里面的，为什么里面的反而叫upstream？其实是这样的：<strong>在网络运维的视角上，我们更关注网络报文的流向</strong>，因为HTTP报文是从外部进来的，那么我们认为其上游（upstream）是客户端；<strong>但是在应用的视角上，更关注的是数据的流向</strong>，一般来说HTTP数据是从内部往外发送的，那么在这种视角下，数据的上游（upstream）就是后端服务器了。<br>\n&nbsp;<br>\nNginx、Envoy都属于应用网关，所以在它们的术语里，upstream指的是后端环节。这里没有对错之分，你只要知道并且遵照这个约定就好了。</p>\n</blockquote><p>到这里，我们既然已经解读清楚报错日志了，接下来就进入到抓包文件的分析里吧。</p><h3>先写过滤器</h3><p>虽然在上节课，我们也使用Wireshark对握手相关的案例做了不少分析，但对它的使用还是相对简单的。那今天这节课开始，我们就要深度使用Wireshark了。比如在接下来的内容里，我会用到很多Wireshark的过滤器（也可以叫过滤表达式或者过滤条件）。因为步骤稍多，所以我会多花一些时间来讲解，希望能给你讲透。</p><p>一般来说，在我们抓到的原始抓包文件里，我们真正关心的报文只占整体的一小部分。那么，如何从中定位跟问题相关的报文，就是个学问了。</p><p>就当前这个案例而言，我们既然有应用层日志，也有相关的IP地址等明确的信息，这些就为我们做报文过滤创造了条件。我们要写一个过滤器，这个过滤器以IP为条件，先从原始文件中过滤出<strong>跟这个IP相关的报文</strong>。</p><p>在Wireshark中，以IP为条件的常用过滤器语法，主要有以下几种：</p><pre><code class="language-plain">ip.addr eq my_ip：过滤出源IP或者目的IP为my_ip的报文\nip.src eq my_ip：过滤出源IP为my_ip的报文\nip.dst eq my_ip：过滤出目的IP为my_ip的报文\n</code></pre><p>不过，这还只是第一个过滤条件，仅通过它过滤的话，出来的报文数量仍然比我们真正关心的报文要多很多。我们还需要第二个过滤条件，也就是要<strong>找到TCP RST报文</strong>。这就要用到另外一类过滤器了，也就是tcp.flags，而这里的flags，就是SYN、ACK、FIN、PSH、RST等TCP标志位。</p><p>对于RST报文，过滤条件就是：</p><pre><code class="language-plain">tcp.flags.reset eq 1\n</code></pre><p>我们可以选中任意一个报文，注意其TCP的Flags部分：</p><p><img src="https://static001.geekbang.org/resource/image/82/76/8297712dde99ecaef1b27db608561176.jpg?wh=505x214" alt=""></p><p>好了，我们打开抓包文件，输入这个过滤条件：</p><pre><code class="language-plain">ip.addr eq 10.255.252.31 and tcp.flags.reset eq 1\n</code></pre><p>我们会发现有很多RST报文：</p><p><img src="https://static001.geekbang.org/resource/image/69/2c/69196eb1f280bfed8c8997e869899c2c.jpg?wh=781x522" alt=""></p><p>在Wirershark窗口的右下角，就有符合过滤条件的报文个数，这里有9122个，占所有报文的4%，确实是非常多。由此推测，日志里的很多报错估计应该就是其中一些RST引起的。我们选一个先看一下。</p><p>我们在<a href="https://time.geekbang.org/column/article/478189">第2讲</a>的时候，就学习了如何在Wireshark中，基于一个报文，找到它所在的整个TCP流的所有其他报文。这里呢，我们选择172号报文，右单击，选中Follow -&gt; TCP Stream，就找到了它所属的整个TCP流的报文：</p><p><img src="https://static001.geekbang.org/resource/image/35/01/35bf3f90770f808515a112ec5694eb01.jpg?wh=858x101" alt=""></p><p>咦，这个RST处在握手阶段？由于这个RST是握手阶段里的第三个报文，但它又不是期望的ACK，而是RST+ACK，所以握手失败了。</p><p>不过，你也许会问：<strong>这种握手阶段的RST，会不会也跟Nginx日志里的connection reset by peer有关系呢？</strong></p><p>要回答这个问题，我们就要先了解应用程序是怎么跟内核的TCP协议栈交互的。一般来说，客户端发起连接，依次调用的是这几个系统调用：</p><ul>\n<li>socket()</li>\n<li>connect()</li>\n</ul><p>而服务端监听端口并提供服务，那么要依次调用的就是以下几个系统调用：</p><ul>\n<li>socket()</li>\n<li>bind()</li>\n<li>listen()</li>\n<li>accept()</li>\n</ul><p>服务端的用户空间程序要使用TCP连接，首先要获得上面最后一个接口，也就是<strong>accept()</strong>调用的返回。而accept()调用能成功返回的前提呢，是正常完成三次握手。</p><p>你看，这次客户端在握手中的第三个包不是ACK，而是RST（或者RST+ACK），握手不是失败了吗？那么自然地，这次失败的握手，也不会转化为一次有效的连接了，所以<strong>Nginx都不知道还存在过这么一次失败的握手</strong>。</p><p>当然，在客户端日志里，是可以记录到这次握手失败的。这是因为，客户端是TCP连接的发起方，它调用connect()，而connect()失败的话，其ECONNRESET返回码，还是可以通知给应用程序的。</p><p>我们再来看一下这张系统调用跟TCP状态关系的示意图：</p><p><img src="https://static001.geekbang.org/resource/image/36/97/3623f2877b7464yy630yy5b14b214697.jpg?wh=1268x456" alt=""></p><p>所以，上面这个虽然也是RST，但并不是我们要找的那种<strong>“在连接建立后发生的RST”</strong>。</p><h3>继续打磨过滤器</h3><p>看来，我们还需要进一步打磨一下过滤条件，把握手阶段的RST给排除。要做到这一点，首先要搞清楚：<strong>什么是握手阶段的RST的特征呢？</strong></p><p>我们关注一下上面的截图，其实会发现：这个RST的序列号是1，确认号也是1。因此，我们可以在原先的过滤条件后面，再加上这个条件：</p><pre><code class="language-plain">tcp.seq eq 1 and tcp.ack eq 1\n</code></pre><p>于是过滤条件变成：</p><pre><code class="language-plain">ip.addr eq 10.255.252.31 and tcp.flags.reset eq 1 and !(tcp.seq eq 1 and tcp.ack eq 1)\n</code></pre><p>注意，这里的<code>(tcp.seq eq 1 and tcp.ack eq 1)</code>前面是一个感叹号（用not也一样），起到“取反”的作用，也就是排除这类报文。</p><p>让我们看下，现在过滤出来的报文是怎样的：</p><p><img src="https://static001.geekbang.org/resource/image/52/b2/52881809f2d090e658314945f8de83b2.jpg?wh=933x455" alt=""></p><p>我们又发现了序列号为2的很多RST报文，这些又是什么呢？我们选包号115，然后Follow -&gt; TCP Stream看一下：</p><p><img src="https://static001.geekbang.org/resource/image/e2/b4/e2ebcf2e81c8cb6759b254676f69a7b4.jpg?wh=934x78" alt=""></p><p>原来这是挥手阶段的RST，并且没有抓取到数据交互阶段，那跟日志里的报错也没关系，也可以排除。这样的话，我们可以把前面的过滤条件中的and改成or，就可以同时排除握手阶段和挥手阶段的RST报文了。我们输入过滤器：</p><pre><code>ip.addr eq 10.255.252.31 and tcp.flags.reset eq 1 and !(tcp.seq eq 1 or tcp.ack eq 1)\n</code></pre><p>得到下面这些报文：</p><p><img src="https://static001.geekbang.org/resource/image/bd/65/bdf63b498d1b258879ef977a1bfd5265.jpg?wh=856x333" alt=""></p><p>虽然排除了握手阶段的RST报文，但是剩下的也还是太多，我们要找的“造成Nginx日志报错”的RST在哪里呢？</p><p>为了找到它们，我们需要再增加一些明确的搜索条件。还记得我提到过的两大鸿沟吗？一个是应用现象跟网络现象之间的鸿沟，一个是工具提示跟协议理解之间的鸿沟。</p><p>现在为了跨越第一个鸿沟，我们需要把搜索条件落实具体，针对当前案例来说，就是基于以下条件寻找数据包：</p><ul>\n<li>既然这些网络报文跟应用层的事务有直接关系，那么报文中应该就包含了请求相关的数据，比如字符串、数值等。当然，这个前提是数据本身没有做过特定的编码，否则的话，报文中的二进制数据，跟应用层解码后看到的数据就会完全不同。</li>\n</ul><blockquote>\n<p>补充：编码的最典型的场景就是TLS。如果我们不做解密，那么直接tcpdump或者Wireshark抓取到的报文就是加密过的，跟应用层（比如HTTP）的数据完全不同，这也给排查工作带来了不小的困难。关于如何对TLS抓包数据进行解密，我在“实战二”的TLS排查的课程里会提到，你可以期待一下。</p>\n</blockquote><ul>\n<li>这些报文的发送时间，应该跟日志的时间是吻合的。</li>\n</ul><p>对于条件1，我们可以利用Nginx日志中的URL等信息；对于条件2，我们就要利用日志的时间。其实，在开头部分展示的Nginx日志中，就有明确的时间（2015/12/01 15:49:48），虽然只是精确到秒，但很多时候已经足以帮助我们进一步缩小范围了。</p><p>那么，在Wireshark中搜索“特定时间段内的报文”，又要如何做到呢？这就是我要介绍给你的又一个<strong>搜索技巧：使用frame.time过滤器</strong>。比如下面这样：</p><pre><code class="language-plain">frame.time &gt;="dec 01, 2015 15:49:48" and frame.time &lt;="dec 01, 2015 15:49:49"\n</code></pre><p>这就可以帮助我们定位到跟上面Nginx日志中，第一条日志的时间匹配的报文了。为了方便你理解，我直接把这条日志复制到这里给你参考：</p><pre><code class="language-plain">2015/12/01 15:49:48 [info] 20521#0: *55077498 recv() failed (104: Connection reset by peer) while sending to client, client: 10.255.252.31, server: manager.example.com, request: "POST /WebPageAlipay/weixin/notify_url.htm HTTP/1.1", upstream: "http:/10.4.36.207:8080/WebPageAlipay/weixin/notify_url.htm", host: "manager.example.com"\n</code></pre><p>我们再结合前面的搜索条件，就得到了下面这个更加精确的过滤条件：</p><pre><code class="language-plain">frame.time &gt;="dec 01, 2015 15:49:48" and frame.time &lt;="dec 01, 2015 15:49:49" and ip.addr eq 10.255.252.31 and tcp.flags.reset eq 1 and !(tcp.seq eq 1 or tcp.ack eq 1)\n</code></pre><p>好长的一个过滤器！不过没关系，人读着觉得长，Wireshark就未必这么觉得了，也许还觉得很顺眼呢。就好比机器语言，人读着感觉是天书，机器却觉得好亲近，“这可是我的母语啊！”</p><p>好，这次我们终于非常成功地锁定到只有3个RST报文了：</p><p><img src="https://static001.geekbang.org/resource/image/2b/f0/2b370fc667acf40c88d2502536fcf3f0.jpg?wh=1088x108" alt=""></p><p>接下来要做的事情就会简单很多：只要对比这三个RST所在的TCP流里的应用层数据（也就是HTTP请求和返回），跟Nginx日志中的请求和返回进行对比，就能找到是哪个RST引起了Nginx报错了。</p><h3>对问题报文的深入分析</h3><p>我们先来看看，11393号报文所属的流是什么情况？</p><p><img src="https://static001.geekbang.org/resource/image/04/ce/043f9bc2bcc71eda60b891fedb7285ce.jpg?wh=1028x211" alt=""></p><p>然后我们来看一下11448号报文所属的TCP流。</p><p><img src="https://static001.geekbang.org/resource/image/4a/8b/4aaef0f94ccd160269c3a87fe1bed78b.jpg?wh=1027x262" alt=""></p><p>原来，11448跟11450是在同一个流里面的。现在清楚了，3个RST，分别属于2个HTTP事务。</p><p>你再仔细对比一下两个图中的红框部分，是不是不一样？它们分别是对应了一个URL里带“weixin”字符串的请求，和一个URL里带“app”字符串的请求。那么，在这个时间点（15:49:48）对应的日志是关于哪一个URL的呢？</p><pre><code class="language-plain">2015/12/01 15:49:48 [info] 20521#0: *55077498 recv() failed (104: Connection reset by peer) while sending to client, client: 10.255.252.31, server: manager.example.com, request: "POST /WebPageAlipay/weixin/notify_url.htm HTTP/1.1", upstream: "http:/10.4.36.207:8080/WebPageAlipay/weixin/notify_url.htm", host: "manager.example.com"\n</code></pre><p>你只要往右拖动一下鼠标，就能看到POST URL里的“weixin”字符串了。而包号11448和11450这两个RST所在的TCP流的请求，也是带“weixin”字符串的，所以它们就是匹配上面这条日志的RST！</p><p>如果你还没有完全理解，我这里帮你小结一下，为什么我们可以确定这个TCP流就是对应这条日志的，主要三点原因：</p><ul>\n<li>时间吻合；</li>\n<li>RST行为吻合；</li>\n<li>URL路径吻合。</li>\n</ul><p>通过解读上面的TCP流，我们终于跨过了这道“应用现象跟网络报文”之间的鸿沟：</p><p><img src="https://static001.geekbang.org/resource/image/30/e2/30d7677724c5f6ffc8635f2ab9fa0fe2.jpg?wh=2000x562" alt=""></p><p>再进一步，我给你画一下这个HTTP事务的整体过程，帮你进一步搞清楚为什么这个RST，会引起Nginx记录connection reset by peer的报错：</p><p><img src="https://static001.geekbang.org/resource/image/7c/2c/7c193479ac85275f411a3de778a1ae2c.jpg?wh=2000x1125" alt=""></p><p>也就是说，握手和HTTP POST请求和响应都正常，但是客户端在对HTTP 200这个响应做了ACK后，随即发送了RST+ACK，而正是这个行为<strong>破坏了正常的TCP四次挥手</strong>。也正是这个RST，导致服务端Nginx的recv()调用收到了ECONNRESET报错，从而进入了Nginx日志，成为一条connection reset by peer。</p><p><strong>这个对应用产生了什么影响呢？</strong>对于服务端来说，表面上至少是记录了一次报错日志。但是有意思的是，这个POST还是成功了，已经被正常处理完了，要不然Nginx也不会回复HTTP 200。</p><p>对于客户端呢？还不好说，因为我们并没有客户端的日志，也不排除客户端认为这次是失败，可能会有重试等等。</p><p>我们把这个结论告诉给了客户，他们悬着的心稍稍放下了：至少POST的数据都被服务端处理了。当然，他们还需要查找客户端代码的问题，把这个不正常的RST行为给修复掉，但是至少已经不用担心数据是否完整、事务是否正常了。</p><p>现在，回到我们开头的三连问：</p><ul>\n<li>这个reset会影响我们的业务吗，这次事务到底有没有成功呢?</li>\n<li>这个reset发生在具体什么阶段，属于TCP的正常断连吗？</li>\n<li>我们要怎么做才能避免这种reset呢？</li>\n</ul><p>我们现在就可以回答了：</p><ul>\n<li>这个reset是否影响业务，还要继续查客户端应用，但服务端事务是成功被处理了。</li>\n<li>这个reset发生在事务处理完成后，但不属于TCP正常断连，还需要继续查客户端代码问题。</li>\n<li>要避免这种reset，需要客户端代码进行修复。</li>\n</ul><blockquote>\n<p>补充：客户端用RST来断开连接并不妥当，需要从代码上找原因。比如客户端在Receive Buffer里还有数据未被读取的情况下，就调用了close()。对应用的影响究竟如何，就要看具体的应用逻辑了。</p>\n</blockquote><p>网络中的环节很多，包括客户端、服务端、中间路由交换设备、防火墙、LB或者反向代理等等。在这么多环节中定位到具体的问题节点，一直以来是很多工程师的痛点。比如，网络不稳定，或者防火墙来几个RST，也都有可能导致类似的connection reset by peer的问题。</p><p>通过抓包分析，我们抽丝剥茧，定位到具体的问题环节不在Nginx，也不在网络本身，而是在客户端代码这里。也正因为有了这样的分析，写代码的同学就可以专心做代码修复，而不用一直怀疑问题在其他环节了。</p><p>好，讨论完RST，你可能会问了：TCP挥手一般是用FIN的，这个知识点还没讨论呢。别急，这第二个案例就是关于<strong>FIN</strong>的。</p><h2>案例2：一个FIN就完成了TCP挥手？</h2><p>你应该知道，TCP挥手是“四次”，这几乎也是老生常谈的知识点了。不过这里，我也再带你来看一下常规的四次挥手的过程：</p><p><img src="https://static001.geekbang.org/resource/image/82/71/82d23321527b3530d40c67yy37670f71.jpg?wh=2000x908" alt=""></p><p>我在图上没有用“客户端”和“服务端”这种名称，而是叫“发起端”和“接收端”。这是因为，<strong>TCP的挥手是任意一端都可以主动发起的</strong>。也就是说，挥手的发起权并不固定给客户端或者服务端。这跟TCP握手不同：握手是客户端发起的。或者换个说法：<strong>发起握手的就是客户端</strong>。在握手阶段，角色分工十分明确。</p><p>另外，FIN和ACK都各有两次，这也是十分明确的。</p><p>可是有一次，一个客户向我报告这么一个奇怪的现象：他们偶然发现，他们的应用在TCP关闭阶段，只有一个FIN，而不是两个FIN。这好像不符合常理啊。我也觉得有意思，就一起看了他们这个抓包文件：</p><p><img src="https://static001.geekbang.org/resource/image/cd/76/cd8e66de72bab07244442ae290940f76.jpg?wh=1034x173" alt=""></p><p>确实奇怪，真的只有一个FIN。这两端的操作系统竟然能容忍这种事情发生？瞬间感觉“塌房”了：难道一向严谨的TCP，它的分手也可以这么随意吗？“当初是你要分开，分开就分开，一个FIN，就足够，眼泪落下来”？</p><p>玩笑归玩笑，很快，我就意识到还有一种可能性。在上节课我介绍TCP握手的时候提到过，TCP里一个报文可以搭另一个报文的顺风车（Piggybacking），以提高TCP传输的运载效率。所以，TCP挥手倒不是一定要四个报文，Piggybacking后，就可能是3个报文了。看起来就类似三次挥手：</p><p><img src="https://static001.geekbang.org/resource/image/be/e1/be46316edd5128c572a991b76fe9dbe1.jpg?wh=2000x639" alt=""></p><p>那这次的案例，我们在Wireshark中看到了后两个报文，即接收端回复的FIN+ACK和发起端的最后一个ACK。<strong>那么，第一个FIN在哪里呢？</strong>从Wireshark的截图中，确实看不出来。</p><p>当然，从Wireshark的图里，我们甚至可以认为，这次连接是服务端发起的，它发送了FIN+ACK，而客户端只回复了一个ACK，这条连接就结束了。这样的解读更加诡异，却也符合Wireshark的展示。</p><p><img src="https://static001.geekbang.org/resource/image/74/75/741c44e6e20b7d75a316f7b3a0cf8175.jpg?wh=2008x384" alt=""><br>\n<img src="https://static001.geekbang.org/resource/image/bb/f8/bbfdfc004bd5aa7a0e8d6db1979bcaf8.jpg?wh=2000x746" alt=""></p><p>但是，Wireshark的主界面还有个特点，就是当它的<strong>Information列展示的是应用层信息时，这个报文的TCP层面的控制信息就不显示了</strong>。</p><p>所以，上面的POST请求报文，其Information列就是POST方法加上具体的URL。它的TCP信息，包括序列号、确认号、标志位等，都需要到详情里面去找。</p><p>我们先选中这个POST报文，然后到界面中间的TCP详情部分去看看：</p><p><img src="https://static001.geekbang.org/resource/image/fc/b9/fc400e5bc462d752846a921938cae1b9.jpg?wh=999x702" alt=""></p><p>原来，第一个FIN控制报文，并没有像常规的那样单独出现，而是<strong>合并（Piggybacking）在POST报文里</strong>！所以，整个挥手过程，其实依然十分标准，完全遵循了协议规范。仅仅是因为Wireshark的显示问题，带来了一场小小的误会。虽然还有一个“为什么没有HTTP响应报文”的问题，但是TCP挥手方面的问题，已经得到了合理的解释了。</p><p><img src="https://static001.geekbang.org/resource/image/23/2c/2359b320a409263092b5b9ayybd7802c.jpg?wh=2000x864" alt=""></p><p>这也提醒我们，理解TCP知识点的时候需要真正理解，而不是生搬硬套。这一方面需要对协议的仔细研读，另一方面也离不开实际案例的积累和融会贯通，从量变引起质变。</p><p>我们自己也要有个态度：大部分时候，当看到TCP有什么好像“不合规的行为”，<strong>我们最好先反思自己是不是对TCP的掌握还不够深入，而不是先去怀疑TCP</strong>，毕竟它也久经考验，它正确的概率比我们高得多，那我们做“自我检讨”，其实是笔划算的买卖，基本“稳赢”。</p><h2>小结</h2><p>在这节课里，我们通过回顾案例，把TCP挥手的相关技术细节给梳理了一遍。在案例1里面，我们用抓包分析的方法，打通了“应用症状跟网络现象”以及“工具提示与协议理解”这两大鸿沟，你可以再重点关注一下这里面用到的推进技巧：</p><ul>\n<li>首先根据应用层的表象信息，抽取出IP和RST报文这两个过滤条件，启动了报文过滤的工作。</li>\n<li>分析第一遍的过滤结果，得到进一步推进的过滤条件（在这个案例里是排除握手阶段的RST）。</li>\n<li>结合日志时间范围，继续缩小范围到3个RST报文，这个范围足够小，我们可以展开分析，最终找到报错相关的TCP流。这种“迭代式”的过滤可以反复好几轮，直到你定位到问题报文。</li>\n<li>在这个TCP流里，结合对TCP协议和HTTP的理解，定位到问题所在。</li>\n</ul><p>此外，通过这个案例，我也给你介绍了一些Wireshark的使用技巧，特别是各种过滤器：</p><ul>\n<li>通过<strong>ip.addr eq my_ip</strong>或<strong>ip.src eq my_ip</strong>，再或者<strong>ip.dst eq my_ip</strong>，可以找到跟my_ip相关的报文。</li>\n<li>通过<strong>tcp.flags.reset eq 1</strong>可以找到RST报文，其他TCP标志位，依此类推。</li>\n<li>通过<strong>tcp.ack eq my_num</strong>可以找到确认号为my_num的报文，对序列号的搜索，同理可用<strong>tcp.seq eq my_num</strong>。</li>\n<li>一个过滤表达式之前加上“<strong>!</strong>”或者<strong>not</strong>起到取反的作用，也就是排除掉这些报文。</li>\n<li>通过frame.time &gt;="dec 01, 2015 15:49:48"这种形式的过滤器，我们可以根据时间来过滤报文。</li>\n<li>多个过滤条件之间可以用and或者or来形成复合过滤器。</li>\n<li>通过把<strong>应用日志中的信息</strong>（比如URL路径等）和Wireshark里的<strong>TCP载荷的信息</strong>进行对比，可以帮助我们定位到跟这个日志相关的网络报文。</li>\n</ul><p>而在案例2里面，我们对“四次挥手”又有了新的认识。通过这个真实案例，我希望你能够了解到：</p><ul>\n<li>实际上TCP挥手可能不是表面上的四次报文，因为并包也就是Piggybacking的存在，它可能<strong>看起来是三次</strong>。</li>\n<li>在某些特殊情况下，在Wireshark里看不到第一个FIN。这个时候你不要真的把后面那个被Wireshark直接展示的FIN当作是第一个FIN。你需要选中挥手阶段附近的报文，<strong>在TCP详情里面查看是否有报文携带了FIN标志位</strong>。这确实是个非常容易掉坑的地方，所以我要提醒你一下。</li>\n</ul><h2>思考题</h2><p>好了，挥手相关的知识点给你复习到这里。给你留几道思考题：</p><ol>\n<li>如果要在Wireshark中搜索到挥手阶段出现的RST+ACK报文，那么这个过滤器该如何写呢？</li>\n<li>你有没有通过抓包分析，解决过应用层的奇怪问题呢？你是怎么做的呢？</li>\n</ol><p>欢迎在留言区分享你的答案和思考，我们一起交流探讨。如果觉得有收获，也欢迎你把今天的内容分享给更多的朋友。</p><h2>扩展知识：聊一聊挥手的常见误区</h2><p>我们案例也讲了两个了，相信你也对非正常挥手（RST）和正常挥手（FIN）有了更加深入的认识了。接下来，我再给你介绍几个常见误区，希望给你起到“有则改之，无则加勉”的效果。</p><h3>连接关闭由客户端发起？</h3><p>其实不对，连接关闭可以是客户端，也可以是服务端发起。造成这个误解的原因，其实也跟这张图有关系：</p><p><img src="https://static001.geekbang.org/resource/image/a2/83/a2d8d9a106760e8a0de2d880af0bd883.jpg?wh=1370x1184" alt=""></p><p>你有没有发现，图中第一个FIN是从客户端发起的。但服务端就不会主动发起关闭/挥手吗？当然会，只是图中没有标明这种情况。挥手跟握手不同，握手一定是客户端发起的（所以才叫客户端），但挥手是双方都可以。</p><p>其实上节课我们也讲到过这张图，它出自Richard Stevens的<a href="https://book.douban.com/subject/4859464">《UNIX网络编程：套接字联网API》</a>。那是不是Stevens自己就搞错了呢？我觉得，这个可能性比我中彩票的概率还要低好几个数量级。</p><p>Stevens当然清楚双方都可以发起挥手，他只是为了突出重点，就没有把多种情况都画到同一张图里，因为这张图的重点是<strong>把TCP连接状态的变迁展示清楚</strong>，而不是要突出“谁可以发起挥手”这个细节。</p><h3>挥手不能同时发起？</h3><p>有的同学觉得挥手是客户端发起的，或者是服务端发起，反正就不能是双方同时发起。事实上，如果双方同时都主动发起了关闭，TCP会怎么处理这种情况呢？我们看下图：</p><p><img src="https://static001.geekbang.org/resource/image/c2/c4/c22911ba09ca2300f1377387b4a41cc4.jpg?wh=2000x757" alt=""></p><ul>\n<li>双方同时发起关闭后，也同时进入了FIN_WAIT_1状态；</li>\n<li>然后也因为收到了对方的FIN，也都进入了CLOSING状态；</li>\n<li>当双方都收到对方的ACK后，最终都进入了TIME_WAIT状态。</li>\n</ul><p>这也意味着，两端都需要等待2MSL的时间，才能复用这个五元组TCP连接。这种情况是比较少见的，但是协议设计需要考虑各种边界条件下的实现，比普通的应用程序所要考虑的事情要多不少。所以也许<strong>有些RFC看似简单，但背后其实都十分不简单</strong>。</p><h3>TCP挥手时双方同时停止发送数据？</h3><p>一方发送FIN，表示这个连接开始关闭了，双方就都不会发送新的数据了？这也是很常见的误区。</p><p>实际上，一方发送FIN只是表示这一方不再发送新的数据，但对方仍可以发送数据。</p><p>还是在Richard Stevens的<a href="https://book.douban.com/subject/1088054">《TCP/IP详解（第一卷）》</a>中，明确提到TCP可以有“半关闭”的做法，也就是：</p><ul>\n<li>一端（A）发送FIN，表示“我要关闭，不再发送新的数据了，但我可以接收新的数据”。</li>\n<li>另一端（B）可以回复ACK，表示“我知道你那头不会再发送了，我这头未必哦”。</li>\n<li>B可以继续发送新的数据给A，A也会回复ACK表示确认收到新数据。</li>\n<li>在发送完这些新数据后，B才启动了自己的关闭过程，也就是发送FIN给A，表示“我的事情终于忙好了，我也要关闭，不会再发送新数据了”。</li>\n<li>这时候才是真正的两端都关闭了连接。</li>\n</ul><p>还是搬运了Stevens的图过来供你参考，也再次致敬Stevens大师！</p><p><img src="https://static001.geekbang.org/resource/image/12/2c/128cea235e031fd471b501f7532a372c.jpg?wh=1190x876" alt=""></p>',
        article_title:
          "04 | 挥手：Nginx日志报connection reset by peer是怎么回事？",
      },
      {
        title: "05 | 定位防火墙（一）：传输层的对比分析",
        id: 481042,
        content:
          '<p>你好，我是胜辉。今天我们来聊一个有趣的话题：防火墙。</p><p>在网络排查中，防火墙作为一个隐形神秘的存在，时常给排查工作带来一定的不确定性。有时候，你不知道为什么一些网络包能正常发出，但对端就是没收到。有时候，同样的两端之间，有些连接可以通信，有些就是不行。</p><p>这个时候，你很可能会怀疑是防火墙在从中作祟了，但是你有什么证据吗？</p><p>你不是防火墙工程师，就没有查看它的配置的权限。但是这样一个看不见摸不着的东西，却可能正在影响着你的应用。相信你也一定想彻底转变被动的状态，把问题搞定。</p><p>其实，无论防火墙有多么神秘，<strong>它本质上是一种网络设备</strong>。既然是网络设备，那么它必然同样遵循我们知道的技术原理和网络规范。所以，防火墙的踪迹，虽然表面上给人一种虚无缥缈的感觉，但从理论上说，总是有迹可循的。所以这次，我就帮助你抓住防火墙的蛛丝马迹。</p><p>当然，防火墙的排查技巧确实并不简单，为了把这个主题讲透，我将用两节课的时间，来给你专门讲解这方面的排查技巧：一节是结合传输层和应用层的分析推理，一节是聚焦在网络层的精确打击。相信你通过这两讲的学习，一定能掌握不少独门秘技，从而为你的应用保驾护航。</p><p>今天这一讲，我们先学第一种方法：结合传输层和应用层的分析推理。</p><!-- [[[read_end]]] --><h2>结合传输层和应用层的分析推理</h2><p>这里传输层当然就是指TCP/UDP，应用层就是问题表象，比如超时、报错之类。我们来看一个具体的例子。</p><p>这是我在2017年处理的一个案例。当时eBay内部的一个应用A访问应用B的时候，经常耗时过长，甚至有时候事务无法在限定时间内完成，就导致报错。而且我们发现，问题都是在访问B的HTTPS时发生的，访问B的HTTP就一切正常。</p><p>因为应用A对时间比较敏感，开发团队希望能既解决事务失败的问题，也改善事务处理慢的问题，于是我们运维团队开始调查。</p><p>那么，我们先来看一下这个案例中，应用请求路径的大体示意图：</p><p><img src="https://static001.geekbang.org/resource/image/11/de/11yy1ab28e81ba7062c909fd16668fde.jpg?wh=2000x546" alt=""></p><p>应用A和应用B各自都是一组独立的集群（多台服务器）。A的众多机器，都访问B的位于负载均衡（LB）上的VIP（虚拟IP），然后LB再把请求转发给B的机器。这里的HTTP和HTTPS都位于同一个虚拟IP，只是服务端口不同而已（一个是80，一个是443）。</p><p>既然问题是“A觉得B很慢”，那么，我们除了<strong>听听A的抱怨，是否也该问问B的解释，才显得比较公正呢？</strong>这就是我们选择做“两侧抓包”的背后的考量了。</p><p>否则，假如我们只是在客户端（即A应用）上抓包，看到的报文显然也属片面。比如，客户端看到自己发出的报文迟迟未被服务端确认的话，那么这个报文究竟是丢失在网络路径途中，还是已经到达服务端但是被服务端丢弃了呢？显然，只在客户端抓包，是无法把这些事实弄得很清楚的。</p><p>所以，我们就在A中选择了一台机器（即客户端）做tcpdump抓包，同时在LB的HTTPS VIP（即服务端）上也进行抓包。</p><p>这里我要给你友情提醒一下，做这种双向抓包，需要注意：</p><ul>\n<li><strong>各端的抓包过滤条件一般以对端IP作为条件</strong>，比如tcpdump dst host {对端IP}，这样可以过滤掉无关的流量。</li>\n<li><strong>两端的抓包应该差不多在同时开始和结束</strong>，这样两端的报文就有尽量多的时间是重合的，便于对比分析。</li>\n<li>在同时抓包的时间段内，要把问题重现，也就是<strong>边重现，边抓包</strong>。至于如何重现，又分两种情况：一种是我们知道触发条件，那么直接操作发起就好了；另一种是触发条件未知，那么只有在抓包的同时，耐心等待问题出现，然后再停止抓包。</li>\n</ul><p>这次我们也是如此处理：一边抓包，一边跟开发团队配合观察应用日志。当观察到了日志中有意外事件（exception）出现后，停止了抓包。那么在这段抓包里，应该就含有跟这个意外事件相关的报文了。</p><p><strong>我们先来看一下客户端的报文情况。</strong>打开抓包文件后，我一般会按部就班地做以下几件事：</p><ul>\n<li>查看Expert Information；</li>\n<li>重点关注可疑报文（比如Warining级别），追踪其整个TCP流；</li>\n<li>深入分析这个可疑TCP流的第二到四层的报文，再结合应用层表象，综合分析其根因；</li>\n<li>结合两侧的抓包文件，找到属于同一个TCP流的数据包，对比分析，得出结论。</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/dd/ac/dd1958be82b3ea019ffef41ffbc120ac.jpg?wh=2000x360" alt=""></p><p>那么接下来，我们就根据以上的步骤，来具体分析分析当前这个抓包文件。</p><h3>查看Expert Information</h3><p>这一步主要是为了获取整体的网络传送情况，这对于我们大体判断问题方向很有帮助。</p><p>那么在查看的时候，一种方式是打开Analyze菜单，选择最底部的Expert Information，如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/bb/ea/bb90fc78160c081efb2ab5625ce813ea.jpg?wh=1176x694" alt=""></p><p>另一种方式是直接在窗口左下角，点击那个黄色的小圆圈，如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/a9/96/a928335e53603a514123754a72f40b96.jpg?wh=1158x702" alt=""></p><p>不过，我们要怎么理解Expert Information里面的各种信息呢？我来给你挨个介绍一下：</p><ul>\n<li>Warning条目的底色是黄色，意味着可能有问题，应重点关注。</li>\n<li>Note条目的底色是浅蓝色，是在允许范围内的小问题，也要关注。什么叫“允许范围内的小问题”呢？举个例子，TCP本身就是容许一定程度的重传的，那么这些重传报文，就属于“允许范围内”。</li>\n<li>Chat条目的底色是正常蓝色，属于TCP/UDP的正常行为，可以作为参考。比如你可以了解到，这次通信里TCP握手和挥手分别有多少次，等等。</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/5f/43/5fed46d83eaa6yy40a0f57801794d643.jpg?wh=941x233" alt=""></p><p>上图展示的就是客户端抓包文件的情况，我们逐个来解读这三种不同级别的Severity（严重级别）。</p><ul>\n<li>Warning：有7个乱序（Out-of-Order）的TCP报文，6个未抓到的报文（如果是在抓包开始阶段，这种未抓到报文的情况也属正常）。</li>\n<li>Note：有1个怀疑是快速重传，5个是重传（一般是超时重传），6个重复确认。</li>\n<li>Chat：有TCP挥手阶段的20个FIN包，握手阶段的10个SYN包和10个SYN+ACK包。</li>\n</ul><p>一般来说，<strong>乱序</strong>是应该被重点关注的。因为正常情况下，发送端的报文是按照先后顺序发送的，如果到了接收端发生了乱序，那么很可能是中间设备出现了问题，比如中间的交换机路由器（以及这节课的主角防火墙）做了一些处理，引发了报文乱序。</p><p>所以自然，这个乱序也容易引发应用层异常。</p><h3>重点关注问题报文</h3><p>理解了这几个严重级别所代表的含义之后，我们还是回到Expert Information的进一步解读上来。点击Warning左边的小箭头，展开乱序的报文集合，我们就能看到这些报文的概览信息，如下所示：</p><p><img src="https://static001.geekbang.org/resource/image/b4/6f/b44d3f3b10e24ee47088bfcbe479be6f.jpg?wh=1854x326" alt=""></p><p>我习惯上会选择靠后一点的报文（因为相对靠后的报文所属的TCP流相对更完整），然后跟踪这些报文（Follow -&gt; TCP Stream），找到所属的TCP流来进一步分析。比如选中191号报文，这时主窗口自动定位到了这条报文，我们在主窗口中选中该报文后右单击，选择Follow，在次级菜单中点击TCP Stream：</p><p><img src="https://static001.geekbang.org/resource/image/7y/eb/7yyd9d3eb185a05f3f2e86f7e9645beb.jpg?wh=550x602" alt=""></p><p>然后，我们就能看到过滤出来的这个TCP流的全部报文了！</p><p><img src="https://static001.geekbang.org/resource/image/03/a4/03c89f95e198deb4c6a44d772da267a4.jpg?wh=1156x472" alt=""></p><p>细心的你可能会发现，界面里很多字段不是默认有的，比如Seq、NextSeq、TCP Seglen等等，这些其实都是我自定义添加的，目的就是<strong>便于分析</strong>（添加的办法我会在下一讲里介绍）。</p><p>这时，Wireshark的显示过滤器栏出现了<code>tcp.stream eq 8</code>这个过滤器，这是我们刚刚点击Follow -&gt; TCP Stream后自动生成的。</p><p>一眼看去，整串数据流确实有点问题，因为有好几个被Wireshark标注红色的报文。我们重点关注下189、190、191、193、195这几个报文。</p><ul>\n<li>189：服务端（HTTPS）回复给客户端的报文，TCP previous segment not captured意思是，它之前的报文没有在应该出现的位置上被抓到（并不排除这些报文在之后被抓到）。</li>\n<li>190：客户端回复给服务端的重复确认报文（DupAck），可能（DupAck报文数量多的话）会引起重传。</li>\n<li>191：服务端（HTTPS）给客户端的报文，是TCP Out-of-Order，即乱序报文。</li>\n<li>193：服务端（HTTPS）给客户端的TCP Retransmission，即重传报文。</li>\n<li>195：也是服务端（HTTPS）给客户端的重传报文。</li>\n</ul><p>以上都是根据Wireshark给我们提示的信息所做的一些解读，主要是针对TCP<strong>行为</strong>方面的，这也是从Wireshark中读取出来的重要信息之一。另外一个重要的信息源是<strong>耗时</strong>（也就是时间列展示的时间间隔）。显然，在192和193号报文之间，有1.020215秒的时间间隔。</p><p>要知道，对于内网通信来说，时间是以毫秒计算的。<strong>一般内网的微服务的处理时间，等于网络往返时间+应用处理时间</strong>。比如，同机房环境内，往返时间（Round Trip Time）一般在1ms以内。比如一个应用本身的处理时间是10ms，内网往返时间是1ms，那么整体耗时就是11ms。</p><p>然而，这里单单一个193号报文就引发了1秒的耗时，确实出乎意料。因此我们可以基本判定：<strong>这个超长的耗时，很可能就是导致问题的直接原因。</strong></p><h3>结合应用层做深入分析</h3><p>那么，为什么会有这个1秒的耗时呢？</p><p>TCP里面有<strong>重传超时</strong>的设计，也就是如果发送端发送了一个数据包之后，对方迟迟没有回应的话，可以在一定时间内重传。这个“一定时间”就是TCP重传超时（Retransmission Timeout）。显然，这里的1秒，很可能是这个重传超时的设计导致的。</p><p>为了确认这件事，我们就需要做这次分析里最为关键的部分了：<strong>两端报文的对比分析</strong>。我们最好有一个大一点的显示屏，打开这两个抓包文件，并且把两个Wireshark窗口靠近一些，更方便我们肉眼对两边报文进行比较。</p><p>不过，当我们打开服务端（LB）的抓包文件，看到的却也是一大片报文。而要比较，必然要找到同样的报文才能做比较。这也是一个不小的难点：如何才能在服务端抓包文件里，定位到客户端的TCP流呢？我们接着往下看。</p><h3>对比两侧文件</h3><p>其实，找到另一端的对应TCP流的技巧是：<strong>用TCP序列号</strong>。</p><p>我们知道，TCP序列号的长度是4个字节，其本质含义就是网络IO的字节位置（等价于文件IO的字节位置）。因为是4个字节，最大值可代表4GB（即2的32次方）的数据，也就是如果一个流的数据超过4GB，其序列号就要回绕复用了。不过一般来说，因为这个值的范围足够大，在短时间（比如几分钟）碰巧相同的概率几乎为零，因此我们可以以它作为线索，来精确定位这个TCP流在两端抓包文件中的位置。</p><p><strong>首先</strong>，我们可以记录下客户端侧抓包文件中，那条TCP流的某个报文的TCP序列号。比如选择SYN包的序列号，是4022234701：</p><p><img src="https://static001.geekbang.org/resource/image/fd/82/fd2c6380e0a21fd78514c57276de0c82.jpg?wh=636x178" alt=""></p><p>注意，这里必须选<strong>裸序列号</strong>（Raw Sequence Number）。Wireshark主窗口里显示的序列号是处理过的“相对序列号”，也就是为了方便我们阅读，把握手阶段的初始序列号当1处理，后续序列号相应地也都减去初始序列号，从而变成了相对序列号。</p><p>但是显然，这样处理后，无论在哪个TCP流里面，Wireshark展示的握手阶段序列号都是1，后续序列号也都是1+载荷字节数。相对序列号肯定是到处“撞车”的，所以不能作为选取的条件。</p><p>那么，查看裸序列号的方法是怎么样的呢？</p><ul>\n<li>打开Wireshark的Preference（配置）菜单：</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/9b/f5/9b4f554de149411f8549f94b190460f5.jpg?wh=284x209" alt=""></p><ul>\n<li>在弹出菜单的左侧选择Protocols，选中其中的TCP，然后在右侧的选项中，把“Relative sequence numbers”前面的勾去掉，就可以显示裸序列号了：</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/ab/3d/ab2dd24667f0f6b2d7eaaa8bf114923d.jpg?wh=729x505" alt=""></p><p><strong>然后</strong>，我们再到服务端抓包文件里输入过滤器：<code>tcp.seq_raw eq 4022234701</code>，得到同样的这个SYN包：</p><p><img src="https://static001.geekbang.org/resource/image/4a/b4/4a48ebb360a52a1e37b96d23809ef0b4.jpg?wh=970x78" alt=""></p><p>正是我们的搜索条件是裸序列号，所以打开服务端抓包文件的那个Wireshark窗口里才可以搜到这个报文。这也是利用了裸序列号在网络上传输是不会发生变化的这一特性。</p><p><strong>接下来</strong>还是Follow -&gt; TCP Stream，翻出来这个SYN包所属的服务端抓包里的TCP流。</p><p>好了，现在我们睁大眼睛，来仔细比对这些报文的对应情况：</p><p><img src="https://static001.geekbang.org/resource/image/86/a5/867c14e8167d2877ba76b38e87dc12a5.jpg?wh=1303x303" alt=""></p><p>左侧是服务端抓包，右侧是客户端抓包。前4个报文的顺序没有任何变化，但服务端随后一口气发送的4个包（这里叫它们1、2、3、4吧），到了客户端却变成了4、1、2、3！这也就是Wireshark提示我们的：</p><ul>\n<li>Out-of-Order：包1、2、3。</li>\n<li>TCP previous segment not captured：包4。</li>\n</ul><p>下面，<strong>我们再从服务端的角度</strong>，来看一下报文顺序、重传、1秒耗时这三者间的关系：</p><p><img src="https://static001.geekbang.org/resource/image/56/ec/56607756479b51f1005894dd7b4cbbec.jpg?wh=1029x291" alt=""></p><p>前面刚说到“服务端发送4个报文后，客户端收到的是4、1、2、3”。因为后面3个报文的顺序还是正确的，真正乱序的其实只是4，所以就导致了这样一个状况：乱序是乱序的，但是“不够乱”，也就是不能满足快速重传的条件“3个重复确认”。</p><p>这样的话，服务端就不得不用另外一种方式做重传，即<strong>超时重传</strong>。当然，这里的1秒超时是硬件LB的设置值，而Linux的默认设置是200毫秒。</p><p>不过，撇开这些细节不谈，我们现在知道了一个重要的事实：<strong>客户端和服务端之间，有报文乱序的情况</strong>。</p><p>我们查看了其他TCP流，也有很多类似的乱序报文，而这种程度的乱序发生在内网是不应该的，因为内网比公网要稳定很多。以我个人的经验，内网环境常见的丢包率在万分之一上下，乱序的几率我没有严格考证过，因为跟各个环境的具体拓扑和配置的关系太大了。但从经验上看，乱序几率大概在百分之一以下到千分之一左右都属正常。</p><p>我们把这两个抓包文件以及分析过程和推论，发给了网络安全部门。他们对于实际的抓包信息也很重视，经过排查，发回了一个我们“期待已久”但一直无法证实的推测：问题出现在防火墙上！</p><p>具体来说，是这样两个事实：</p><ol>\n<li>在客户端和服务端之间，各有一道防火墙，两者之间设立有隧道；</li>\n<li>因为软件Bug的问题，这个隧道在大包的封包拆包的过程中，很容易发生乱序。</li>\n</ol><p>就像下图这样：</p><p><img src="https://static001.geekbang.org/resource/image/52/37/52f8f2bc8b73a2391fef44caa2d82837.jpg?wh=2000x397" alt=""></p><p><strong>两侧抓包，对比分析</strong>。就是这样一个方法，最终让我们发现了防火墙方面的问题。我们做了设备升级，效果是立竿见影，事务慢的问题完全消失了。开发团队也非常感谢我们的排查工作。</p><p>那么换位思考，你如果是开发团队，你会查哪些层面呢？对，一般是集中在应用程序上，可能也会做一点ping、telnet、traceroute之类的检查。但术业有专攻，难以把排查工作下探到传输层和网络层。</p><p>而如果你是网络安全团队，你又会怎么查呢？没错，也是集中在网络层或者传输层上面，不太会把排查上升到应用层。</p><p><img src="https://static001.geekbang.org/resource/image/d4/e6/d4b3afb2f47cd5e7fdfb7071c0584ee6.jpg?wh=2000x806" alt=""></p><p>显然，两边团队没法“会合”。这种状况，在很多组织里并不少见。不过还好，我所在的团队正好既熟悉网络也熟悉应用，能把网络和应用之间的联动细节搞清楚，进而排查出根因。故事也算有了一个圆满的结局。你了解了整个排查过程，是否也有所启发呢？</p><p>不过，这里我们还有两个小的疑问没有解决：</p><ul>\n<li><strong>为什么隧道会引发乱序？</strong></li>\n</ul><p>首先，隧道本身并不直接引起乱序。隧道是在原有的网络封装上再加上一层额外的封装，比如IPIP隧道，就是在IP头部外面再包上一层IP头部，于是形成了在原有IP层面里的又一个IP层，即“隧道”（各种隧道技术也是SDN技术的核心基础）。由于这个封装和拆封都会消耗系统资源，加上代码方面处理不好，那么出Bug的概率就大大增加了。这就是在这个案例里，隧道会引发乱序的原因。</p><ul>\n<li><strong>为什么HTTP事务没有被影响，只有HTTPS被影响？</strong></li>\n</ul><p>在这个案例里，HTTP确实一直没有被影响到。因为从抓包来看，这个场景的HTTP的TCP载荷，其实远没有达到一个MSS的大小。我们来看一下当时的HTTP抓包：</p><p><img src="https://static001.geekbang.org/resource/image/93/75/93176ff385b0984d91144eb725160675.jpg?wh=2048x424" alt=""></p><p>TCP载荷只有两三百字节，远小于MSS的1460字节。这个跟隧道的关系是很大的，因为<strong>隧道会增加报文的大小</strong>。</p><p>比如通常MTU为1500字节的IP报文，做了IPIP隧道封装后，就会达到1520字节，所以一般有隧道的场景下，主机的MTU都需要改小以适配隧道需求。如果网络没有启用Jumbo Frame，那这个1520字节的报文，就会被路由器/防火墙拆分为2个报文。而到了接收端，又得把这两个报文合并起来。这一拆一合，出问题的概率就大大增加了。</p><blockquote>\n<p>补充：在Linux中，设置了ipip隧道后，这个隧道接口的MTU会自动降低20字节，也就是从默认1500降低到1480字节。<br>\n这个案例里是特殊的防火墙，它的MTU的逻辑跟Linux有所不同。</p>\n</blockquote><p>事实上，在大包情况下，这个隧道引发的是两种不同的开销：</p><ul>\n<li>IPIP本身的隧道头的封包和拆包；</li>\n<li>IP层因为超过MTU而引发的报文分片和合片。</li>\n</ul><p>因为HTTPS是基于TLS加密的，TLS握手阶段的多个TCP段（segment）就都撑满了MSS（也就是前面分析的1、2、3的数据包），于是就触发了防火墙隧道的Bug。</p><p>到这里，你可能又会问了：这个例子中的丢包和乱序问题，其实也不限于防火墙，在路由器交换机层面也是有可能发生的，有没有办法可以更加确定地定位到防火墙，而不是其他网络设备呢？</p><p>这就是我们在下节课要进行深入解析的内容了，即聚焦在网络层的精确打击，你可以期待一下。</p><h2>小结</h2><p>这次的“两侧抓包”，实际上就起到了决定性的作用。那么除了当前这个案例，总的来说，还有<strong>哪些情况下适合做“两侧抓包”</strong>呢？我个人的看法是这样的：</p><ol>\n<li><strong>有条件的话（比如对两侧设备都有权限），就尽量做两侧抓包。</strong>这样可以收集到更多的信息。有更全面的信息，也就更容易作出更准确的判断。这好比我们做数学或者物理题，条件越充足，解题也相对越顺利。即使信息有所富余，也不会干扰到排查工作的正确性。当然这会损失一些效率，就看你怎么权衡了。</li>\n<li><strong>有丢包或者重传的情况的话，更应该做两侧抓包。</strong>因为只有通过比较两侧报文，才能确定具体的丢包位置等信息，而这些信息对于排查工作十分关键。我们经常会出现的情况是，完全不同的两种故障原因，在一侧（比如客户端）看起来很可能是相同的现象。这就好比，一个一半黑一半白的球体，当其中的一面正对着我们的时候，我们是完全不知道另外一面可能是完全不同的颜色。对于网络排查也是如此。</li>\n<li><strong>有些信息在单侧抓包里就能明确下来的，一般就没必要做两侧抓包了。</strong>比如下节课要讲的方法就是这样，这里先给你卖个关子，下节课我们再深入探讨。</li>\n</ol><p>另外，在课程中我还介绍了<strong>在两个不同的抓包文件中如何定位到同个报文的方法，也就是使用裸序列号</strong>。</p><p>在一侧的文件中找到某个报文的裸序列号，作为搜索条件，在另外一侧的报文中搜索得到同样这个报文。这正是利用了TCP裸序列号在网络中传输的一致性（不变性）。后面的课程中，我还将介绍更多这种“寻找同样报文”的方法 ，基本思想也都是基于某些信息在网络传输的一致性。</p><p>下次你遇到乱序、重传等情况时，也都可以运用我介绍的排查方法。当然，未必要照搬这里的每个步骤，但整体思想是类似的，希望通过我自己的经验总结，能给你一些启发。</p><h2>思考题</h2><p>这节课里，我介绍了使用裸序列号作为定位两侧同个报文的手段。那么要定位两侧的同个报文，除了这个方法，还有哪些方法呢？你可以从网络七层模型出发，给出自己的思考。</p><p>欢迎在留言区分享你的答案，我们一起讨论。如果觉得有收获，也欢迎你把今天的内容分享给更多的朋友，我们下节课再见。</p>',
        article_title: "05 | 定位防火墙（一）：传输层的对比分析",
      },
      {
        title: "答疑（一）| 第1~5讲思考题答案",
        id: 494510,
        content:
          '<p>你好，我是胜辉。</p><p>不知不觉，咱们已经完成预习篇、实战一的TCP真实案例篇，还有实战二的应用层真实案例篇的学习了。一路走来，我自己完成了20多讲的备课，过程中是辛劳和满足相伴。当然你也很不容易，坚持学完20多讲，一共投入的时间也接近20个小时，这个过程本身就是一种小小的成就。</p><p>由于课程涉及的网络知识面还是比较广泛的，如果你还有不少没看懂的地方，也别着急，这是很正常的情况。就我自己来说，最初看那些TCP/IP概念的时候也是摸不到头脑，后来我就是反复看，从不同的资料、不同的案例来入手，才逐步建立起自己的理解。</p><p>正因为<strong>通过问题来理解概念</strong>是一种很好的学习方法，所以我在每节课的后面，都留有一两个思考题，希望可以促进思考。这些问题，有些比较容易，有些可能略有难度。所以我整理了答疑篇，一方面给出答案供你参考，一方面也正好把课程里没有覆盖到的细节给你展开一下，作为内容上的补充。总之，我希望通过这个答疑环节，能帮助你把看过的知识都真的消化掉，成为你自己知识体系的一部分。</p><h2>01讲的答疑</h2><h3>思考题</h3><ol>\n<li>traceroute默认是用UDP来做探测的，那这个又是基于什么原理呢？通和不通，我们会收到怎样的回复？</li>\n<li>有时候运行telnet后命令就挂起，没有响应了，这说明了什么问题呢？</li>\n</ol><!-- [[[read_end]]] --><h3>答案</h3><p>关于第一个问题，我看到很多同学的回答都已经很到位了，比如以<strong>@webmin</strong>同学的回答为例：</p><blockquote>\n<p>traceroute使用UDP探测时，初始时把TTL设置为1，经过路由器时TTL会被减1，当TTL变为0时，包被丢弃，路由器向源地址发回一个ICMP超时通知（ICMP Time Exceeded Message）。源收到这个通知，就会把下一次发送的包的TTL在原来的基础加1，这样就可以多前进一步。探测时使用了一个大于30000的端口号去连接，随着TTL的增加端口也会加1，目地服务器在收到这个数据包的时候，会返回一个端口不可达的ICMP错误信息（ICMP Port Unreachable），当源地址收到ICMP Port Unreachable包时，会停止traceroute。</p>\n</blockquote><p>在我的Ubuntu虚拟机上，traceroute使用的起始UDP目的端口是33434，然后每次探测的TTL加一的同时，UDP目的端口也加一，每次探测会发送3次探测报文。也就是下图这样：</p><p><img src="https://static001.geekbang.org/resource/image/00/b1/004a31737f71e76c5c18f8a6152370b1.jpg?wh=943x395" alt="图片"></p><p>当时的traceroute命令行的输出是这样的：</p><pre><code class="language-bash">traceroute www.baidu.com\ntraceroute to www.a.shifen.com (180.101.49.11), 64 hops max\n&nbsp; 1&nbsp; &nbsp;10.0.2.2&nbsp; 0.082ms&nbsp; 0.177ms&nbsp; 0.088ms\n&nbsp; 2&nbsp; &nbsp;192.168.1.1&nbsp; 4.634ms&nbsp; 1.811ms&nbsp; 1.488ms\n&nbsp; 3&nbsp; &nbsp;100.65.0.1&nbsp; 9.972ms&nbsp; 11.511ms&nbsp; 6.189ms\n&nbsp; 4&nbsp; &nbsp;61.152.54.125&nbsp; 16.296ms&nbsp; 3.576ms&nbsp; 4.095ms\n&nbsp; 5&nbsp; &nbsp;61.152.24.102&nbsp; 7.329ms&nbsp; 6.786ms&nbsp; 11.987ms\n&nbsp; 6&nbsp; &nbsp;202.97.71.6&nbsp; 28.938ms&nbsp; 10.568ms&nbsp; 10.069ms\n&nbsp; 7&nbsp; &nbsp;58.213.95.2&nbsp; 15.036ms&nbsp; 9.831ms&nbsp; 17.043ms\n&nbsp; 8&nbsp; &nbsp;*&nbsp; *&nbsp; *\n&nbsp; 9&nbsp; &nbsp;58.213.96.54&nbsp; 16.843ms&nbsp; 10.329ms&nbsp; 15.989ms\n&nbsp;10&nbsp; &nbsp;*&nbsp; *&nbsp; *\n&nbsp;11&nbsp; &nbsp;*&nbsp; *&nbsp; *\n&nbsp;12&nbsp; &nbsp;*&nbsp; *&nbsp; *\n&nbsp;13&nbsp; &nbsp;*&nbsp; *&nbsp; *\n&nbsp;14&nbsp; &nbsp;*&nbsp; *&nbsp; *\n&nbsp;15&nbsp; &nbsp;*&nbsp; *&nbsp; *\n&nbsp;16&nbsp; &nbsp;*&nbsp; *&nbsp; *\n</code></pre><p>这次traceroute没有停住，而是一直到64跳的限制后才停止（因为输出过长，我没有把全部的信息复制过来）。当然，一般实践中你看到连续六七个以上的<code>*</code>，基本就可以判定是目的端对UDP无响应，可以按下 Ctrl + C 终止了。</p><p>我把整个流程的实现原理，概括成了下面的示意图，希望能帮助你理解：</p><p><img src="https://static001.geekbang.org/resource/image/47/16/47d5afb17a8a478e3bc1e0700e09d116.jpg?wh=2000x1125" alt=""></p><p>然后，我们再用ICMP模式做一次traceroute，输出如下：</p><pre><code class="language-bash">$ traceroute -I www.baidu.com\ntraceroute to www.a.shifen.com (180.101.49.12), 64 hops max\n&nbsp; 1&nbsp; &nbsp;10.0.2.2&nbsp; 0.105ms&nbsp; 0.229ms&nbsp; 0.173ms\n&nbsp; 2&nbsp; &nbsp;192.168.1.1&nbsp; 8.972ms&nbsp; 1.725ms&nbsp; 1.326ms\n&nbsp; 3&nbsp; &nbsp;100.65.0.1&nbsp; 9.850ms&nbsp; 7.893ms&nbsp; 7.311ms\n&nbsp; 4&nbsp; &nbsp;*&nbsp; *&nbsp; 61.152.53.149&nbsp; 5.579ms\n&nbsp; 5&nbsp; &nbsp;*&nbsp; 61.152.25.230&nbsp; 20.238ms&nbsp; 5.204ms\n&nbsp; 6&nbsp; &nbsp;*&nbsp; *&nbsp; *\n&nbsp; 7&nbsp; &nbsp;*&nbsp; 58.213.94.114&nbsp; 21.249ms&nbsp; *\n&nbsp; 8&nbsp; &nbsp;58.213.94.126&nbsp; 13.869ms&nbsp; *&nbsp; *\n&nbsp; 9&nbsp; &nbsp;58.213.96.54&nbsp; 11.736ms&nbsp; 10.094ms&nbsp; 10.644ms\n&nbsp;10&nbsp; &nbsp;*&nbsp; *&nbsp; *\n&nbsp;11&nbsp; &nbsp;*&nbsp; *&nbsp; *\n&nbsp;12&nbsp; &nbsp;*&nbsp; *&nbsp; *\n&nbsp;13&nbsp; &nbsp;180.101.49.12&nbsp; 24.780ms&nbsp; 9.443ms&nbsp; 10.201ms\n</code></pre><p>这次就能顺利完成了，这是因为目的IP对ICMP是有回复的，所以traceroute就在最后一跳，也就是第13跳顺利停止了。</p><p>这里还有个小技巧。你有没有发现 <strong>UDP和ICMP这两种模式下，路径信息是可以互补的</strong>？比如UDP模式下没有显示第5和第7跳的IP，但是ICMP模式下就有。这就是为什么我建议你把两种模式分别跑一下，这样可以获取到尽可能完整的路径。</p><p><img src="https://static001.geekbang.org/resource/image/86/40/86ae98fb8478cb41eb9fc93a49561d40.jpg?wh=852x290" alt="图片"></p><p>第二个问题是关于telnet挂起。这里说的“挂起”，是指没有进一步的反应了，比如像下图这样：</p><p><img src="https://static001.geekbang.org/resource/image/0f/46/0f8c8b279ae1910bc52678a724e75f46.jpg?wh=271x54" alt="图片"></p><p>究其原因，就是telnet发送了SYN后没有收到SYN+ACK。我们在<a href="https://time.geekbang.org/column/article/493040">第21讲</a>提到了系统调用，那么对于telnet这样的用户空间程序来说，它要发起TCP连接，就必须调用 <strong>connect()</strong> 这个系统调用，后者会负责发出SYN。但是SYN发出去后，对端没有回复SYN+ACK，这就导致connect()阻塞，telnet程序也只好等在那里，表象上就是挂起了。</p><p>你可以参考这个示意图来理解整个过程：</p><p><img src="https://static001.geekbang.org/resource/image/2e/ce/2ee1c167f34cdecec04d2e7116a8b2ce.jpg?wh=2000x682" alt=""></p><p>当然，在第21讲我们也学习过strace这个工具了，所以我们可以用strace来验证这个过程。直接运行下面的命令：</p><pre><code class="language-bash">strace telnet www.baidu.com 500\n</code></pre><p>就能看到，strace停留在connect()系统调用这里了：</p><pre><code class="language-bash">......\nsocket(AF_INET, SOCK_STREAM, IPPROTO_IP) = 3\nsetsockopt(3, SOL_IP, IP_TOS, [16], 4)&nbsp; = 0\nconnect(3, {sa_family=AF_INET, sin_port=htons(500), sin_addr=inet_addr("45.113.192.102")}, 16\n</code></pre><h2>02讲的答疑</h2><h3>思考题</h3><ol>\n<li>请你用偏移量方法，写一个tcpdump抓取TCP SYN包的过滤表达式。</li>\n<li>如果确定问题是在IP层，tcpdump命令如何写，可以做到既找到IP层的问题，又节约抓包文件大小呢？</li>\n</ol><h3>答案</h3><p>很多同学都写出了正确的偏移量的过滤表达式，也就是：</p><pre><code class="language-bash">tcpdump \'tcp[13]&amp;2 != 0\'\n</code></pre><p>当然，这个题目没有指明是一定只要SYN包，我们用上面这条命令可以抓到SYN和SYN+ACK这两种报文。而如果要指定只抓取SYN包而不抓取SYN+ACK，可以用下面的表达式：</p><pre><code class="language-bash">tcpdump \'tcp[13]|2 = 2\'\n</code></pre><blockquote>\n<p>注意：这里的运算符是“<code>|</code>”，也就是字符或运算符。</p>\n</blockquote><p>第二个问题的关键点是tcpdump的 <strong>-s参数</strong>，我们通过它可以指定要抓取的每个报文的大小。不过那就有个问题了，这里的“报文”是指IP分组还是二层帧呢？</p><p>虽然tcpdump名称里是tcp，实际上它能抓取udp、ip、icmp等等各种报文，因为它抓取的就是第二层的帧。所以说，<strong>这里的“报文大小”，指的是二层帧的大小</strong>。我在<a href="https://time.geekbang.org/column/article/478189">第2讲</a>里也贴了一张图，展示了在某一次抓包里，一个报文的各层占用的字节数：</p><p><img src="https://static001.geekbang.org/resource/image/0c/b6/0c23b60f7dcb62843e331e9b2972c8b6.png?wh=1860x548" alt="图片"></p><p>一般来说，IP头部就是20字节，加上帧头14字节，所以<strong>理论上我们只要抓取前34字节的报文，就可以获取到二层和三层的信息了</strong>。</p><p>不过且慢，有个同学报告了一个问题：</p><blockquote>\n<p>老师，对于“问题2”我有个疑问。我最开始用的命令是“<code>tcpdump -i any -s 34</code>”，发现-s写成34，就抓不到网络层的目的地址字段，用Wireshark分析后发现，帧头（不知道还叫不叫这个，Wireshark显示为 <code>Linux cooked capture v1</code>）占了16个字节，写成36就能把信息抓全了，但是写成“<code>tcpdump -i eth0 -s 34</code>”就可以抓全。</p>\n</blockquote><p>这个问题我之前也没留意到，于是去查证了一下。原来在tcpdump里，对于any这个接口，它在输出帧头的时候用了一种叫 <strong>Linux cooked capture v1</strong> 的格式，这个格式的长度是16个字节，而以太网帧是14字节。这就造成了2个字节的差异，也就是按原先的34字节去抓取就不够了，所以要扩大到36字节才可以抓到完整的IP头部。</p><p>我们看一个例子：</p><p><img src="https://static001.geekbang.org/resource/image/49/e1/49494a525f79d26abb3d23cefc075fe1.jpg?wh=668x280" alt="图片"></p><blockquote>\n<p>补充：上面的traceroute udp模式的示例文件就是用-i any模式抓取的，所以它的二层帧头就是 <code>Linux cooked capture v1</code> 格式。</p>\n</blockquote><p>另外，“IP头部20字节”这个描述，其实还有一个隐含的前提：<strong>IP头部没有做扩展</strong>（Options）。跟TCP类似，IP头部也可以扩展，但是一般用的比较少，在公网上IP Options的报文有可能被丢弃，所以很少被采用。我们在内网也曾经做过实验，启用了IP Options，结果发现虽然内网设备可以支持IP Options，但是延迟增加了很多，所以最后还是取消了它。</p><h2>03讲的答疑</h2><h3>思考题</h3><ol>\n<li>在Linux中，还有一个内核参数也是关于握手的，net.ipv4.tcp_synack_retries。你知道这个参数是用来做什么的吗？</li>\n<li>如果握手双方，一方支持Window Scale，一方不支持，那么在这个连接里，Window Scale最终会被启用吗？你可以参考<a href="https://datatracker.ietf.org/doc/html/rfc1323">RFC1323</a>，给出你的解答。</li>\n</ol><h3>答案</h3><p>第一个问题是关于Linux内核参数的。对于各种网络相关的内核参数来说，最快捷的方法可能就是直接在Linux主机里面查看TCP的手册了，也就是执行：</p><pre><code class="language-bash">man tcp\n</code></pre><p>然后搜索tcp_synack_retries就可以了，也就是这个部分：</p><blockquote>\n<p>tcp_synack_retries (integer; default: 5; since Linux 2.2)<br>\nThe maximum number of times a SYN/ACK segment for a passive TCP connection will be retransmitted.&nbsp; This number should not be higher than 255.</p>\n</blockquote><p>也就说，<strong>这是TCP回复SYN+ACK后等不到ACK时，需要重试的次数</strong>。</p><p>第二个问题是关于Window Scale（下面简称WS）在握手中的具体实现。就像我题目里说的，你可以直接去找到<a href="https://datatracker.ietf.org/doc/html/rfc1323#page-8">RFC1323</a>，然后找到这部分的内容：</p><blockquote>\n<p>This option may be sent in an initial <syn> segment (i.e., a segment with the SYN bit on and the ACK bit off). It may also be sent in a &lt;SYN,ACK&gt; segment, but only if a Window Scale option was received in the initial <syn> segment. A Window Scale option in a segment without a SYN bit should be ignored.</syn></syn></p>\n</blockquote><p>直译过来就是：这个WS选项可以在SYN包中，也可以在SYN+ACK包中。但只有收到的SYN中有这个WS选项，回复的SYN+ACK才可以加上这个选项。如果收到的报文不带SYN标志位但却带上了WS选项，这样的WS应该被忽略。</p><p>也就是说，如果发过来的SYN里不带WS选项，那回复的SYN+ACK也不应该带WS，自然这次的连接里也就不会用上WS了。简单来说，只要有一方不支持WS，这次的连接里就不会用WS。</p><h2>04讲的答疑</h2><h3>思考题</h3><ol>\n<li>如果要在Wireshark中搜索到挥手阶段出现的RST+ACK报文，那么这个过滤器该如何写呢？</li>\n<li>你有没有通过抓包分析，解决过应用层的奇怪问题呢？你是怎么做的呢？</li>\n</ol><h3>答案</h3><p>第一个问题里挥手阶段出现RST的现象， 这个已经不是用FIN完成的标准挥手过程了，而是出现了异常。不过从报文特征来说还是比较直接的，就是既要带RST标志位，也要带ACK标志位，所以答案就是：<strong>tcp.flags.ack==1 and tcp.flags.reset == 1</strong>。</p><p>第二个问题是开放式的，比如<strong>@江山如画</strong>同学分享了一个挺有意思的案例，是双方时钟不同步导致了TLS握手失败。时钟问题也是一个挺有“存在感”的问题，时不时会在各种故障中扮演一点角色，我们可以在排查没什么方向的时候，也可以查一下时钟。</p><p>另外说到TLS握手的问题，我在<a href="https://time.geekbang.org/column/article/491674">第19讲</a>里介绍了两个典型案例，也是刚过去不久的一讲，我想你应该还有印象。</p><h2>05讲的答疑</h2><h3>思考题</h3><p>这节课里，我介绍了使用裸序列号作为定位两侧同个报文的手段。那么要定位两侧的同个报文，除了这个方法，还有哪些方法呢？你可以从网络七层模型出发，给出自己的思考。</p><h3>答案</h3><p>定位两侧同个报文，是一个比较关键的技术点，因为抓包分析的一个重要任务，就是对比两侧抓包文件中的报文，从而判定丢包、乱序、重传等行为的严重程度和原因，由此才能针对性地做进一步排查乃至找到解决方案。如果不能找到“同个报文”，那刚才说的一系列工作就无从谈起了。</p><p><img src="https://static001.geekbang.org/resource/image/d7/e8/d75d0d7acd0654648d39f00d8b027ee8.jpg?wh=2000x894" alt=""></p><blockquote>\n<p>补充：A和B对报文的编号一般是不同的，因为各自抓取报文的起始时间、抓取条件等都可能不同，就会导致抓取到的报文互有交集。</p>\n</blockquote><p>比如上面就是一个典型的TCP问题的场景，也就是报文顺序发生了变化，这个现象就是“乱序”：发出1、2、3，收到的却是2、1、3。那么显然，我们需要在两侧的抓包文件里，找到对应的同个报文。事实上，现在说的1、2、3，是A的抓包文件里的报文编号，而B那边有自己的视角和报文编号，跟A是完全不同的。</p><p>我在<a href="https://time.geekbang.org/column/article/481042">第5讲</a>里介绍的方法，是用了裸序列号（原始序列号），因为TCP报文的这个序列号在传输中是不会变化的，所以这就是一个很好的确定报文的方法。其实，这属于<strong>第一大类方法：按照TCP报文的元信息，也就是TCP头部的特征去找到对应的报文</strong>。</p><p>既然确定了这个大类的方法原则，你很容易举一反三，想到其他的方法。比如：</p><ul>\n<li>用TCP确认号，比如下面这个过滤器：</li>\n</ul><pre><code class="language-bash">tcp.ack_raw == 754313633\n</code></pre><ul>\n<li>如果有TCP timestamp扩展，就用TCP  timestamp，比如下面两种过滤器：</li>\n</ul><pre><code class="language-bash">tcp.options.timestamp.tsval == 2947948748\ntcp.options.timestamp.tsecr == 3209788920\n</code></pre><ul>\n<li>如果有TCP SACK扩展，就用SACK号，比如用过滤器：</li>\n</ul><pre><code class="language-bash">tcp.options.sack_le == 1234\n</code></pre><p>而<strong>第二个大类的方法，是利用TCP的载荷本身的特征去找到对应的报文</strong>。比如HTTP是文本协议，那么我们用下面这个过滤器，就可以找到含有这个字符串的报文：</p><pre><code class="language-bash">tcp contains "id=abcdafeafeagfeagfaraera1242dfea"\n</code></pre><p>其实换成以下这几种方式，也是一样的效果：</p><pre><code class="language-bash">frame contains "id=abcdafeafeagfeagfaraera1242dfea"\nip contains "id=abcdafeafeagfeagfaraera1242dfea"\nhttp contains "id=abcdafeafeagfeagfaraera1242dfea"\n</code></pre><p>这个示意图，也表示了这样的两大类寻找对应报文的方法：</p><p><img src="https://static001.geekbang.org/resource/image/50/25/502a6786f47a81306b62c60f883f8525.jpg?wh=2000x1125" alt=""></p><p>不过，你看了这张图，会不会以为我们每个报文都要这么找对应关系呢？那当然不用了，每个TCP流只要找一个关键报文，然后根据这个报文去做Follow -&gt; TCP Stream就好了。</p><h2>小结</h2><p>以上就是针对课程前5讲思考题的参考答案和拓展解读，希望能给你一些启发。另外，也非常感谢你对课后思考题的仔细思考和认真解答。在看留言的过程中，我从大家的答复中也看到了更加全面或是更加深入的思考，我自己受益匪浅。</p><p>接下来，我还会针对剩余的课后思考题，以及你的提问来作出解答。有任何问题，还是跟以前一样，欢迎你在留言区跟我交流，我们一同成长。</p>',
        article_title: "答疑（一）| 第1~5讲思考题答案",
      },
      {
        title: "06 | 定位防火墙（二）：网络层的精确打击",
        id: 481782,
        content:
          '<p>你好，我是胜辉。今天我们接着上节课的学习和思考，继续来探讨如何定位防火墙问题。</p><p>在上节课里，我们用两侧抓包并对比分析的方法，首先定位到了引发长耗时的数据包，然后对比两侧抓包文件，定位到了包乱序的现象及其原因。最后，我们综合这些有利证据，跟安全部门沟通后，拿到了真正的根因，并彻底解决了问题。</p><p>在那个案例中，大量的分析技术是位于传输层，而且要结合应用层（超时的问题）做综合分析。总的来说，难度还是不小的。而且还有一个不可回避的问题：<strong>包乱序难道只有防火墙才会引发吗？</strong></p><p>其实不是的。包乱序是一种相对来说比较普遍的现象，除了防火墙，还有网络设备、主机本身都可能引起乱序。所以，单纯根据包乱序就断定是防火墙在中间捣鬼，就有点以偏概全了。</p><p>那么有没有一种方法，不需要借助那么多的传输层的复杂知识，就可以让我们更加明确地判断出，问题是在防火墙呢？</p><p>这节课，我就给你介绍这种方法，即<strong>聚焦在网络层的精确打击</strong>。这是一种更加直接、更加高效的办法。</p><p>你可能又会疑惑了：难道说我们上节课学的东西，其实是多余的吗？那倒不是。这两节讲防火墙的课程，各自有不同的侧重点和不同的适用场景。这次我们介绍的方法，在上节课的案例里就不会起到作用；反过来也是如此。<strong>技术上没有“一招鲜”</strong>，只是这次课讲的内容相对上节课来说，确实更加直接，这也是它的一大特点。</p><!-- [[[read_end]]] --><p>好了，废话不多说，咱们来看案例吧。</p><h2>案例1：Web站点访问被reset</h2><p>几年前我在公有云公司就职，当时公司乔迁入住了新的办公大楼，没过半天，不少同事陆续报告，他们无法访问某个内部Web工具。而且报告的人都有个共同点：他们使用的是二楼的有线网络。</p><p>我们这个内部工具是以Web网站形式运行的，报错页面就是类似下面这种（不同浏览器会有不同的错误页面）：</p><p><img src="https://static001.geekbang.org/resource/image/ba/7f/baa46539d6992c656e6469726b9a9f7f.jpg?wh=524x450" alt=""></p><blockquote>\n<p>补充：不要误会，这可不是我们极客时间App的图片加载的报错，这本身就是当时的报错截图。</p>\n</blockquote><p>我们让二楼同事连接到有线网络并做了抓包，然后传给我们做分析。因为抓包的同事没有做过滤，所以抓上来的包比较大，包含了各种其他无关的数据包。不过没关系，我们可以按下面的顺序来。</p><ul>\n<li><strong>第一步：过滤IP</strong></li>\n</ul><p>在这个案例里面，我们就可以把Web站点的IP作为过滤条件。Web站点的IP是253.61.239.103，所以我们的过滤器就是：</p><pre><code class="language-plain">ip.addr == 253.61.239.103\n</code></pre><blockquote>\n<p>补充：因为信息安全的原因，这里的报文信息我做过脱敏了（也就是修改或者删除了敏感信息），所以IP和载荷等都已经不是原始信息了。</p>\n</blockquote><p>我们得到下面的过滤结果：</p><p><img src="https://static001.geekbang.org/resource/image/9e/2c/9ea738e159bd4f2d5cf00e17ea69102c.jpg?wh=1076x239" alt=""></p><ul>\n<li><strong>第二步：选中可能有问题的数据包，然后过滤出整个TCP流</strong></li>\n</ul><p>从上图中已经能看到RST报文了。当然，在<a href="https://time.geekbang.org/column/article/480068">第4讲</a>里，我们也学习了如何在Wireshark里面，用过滤器来找到TCP RST报文。那么这个案例里面，我们同样可以这么做。在过滤输入框里输入：</p><pre><code class="language-plain">ip.addr eq 253.61.239.103 and tcp.flags.reset eq 1\n</code></pre><p>我们觉得某个RST包比较可疑，那么还是用Follow -&gt; TCP Stream的方法，找到对应的TCP流：</p><p><img src="https://static001.geekbang.org/resource/image/b1/ca/b1e20926e584bbd335be84442d9fedca.jpg?wh=747x519" alt=""></p><ul>\n<li><strong>第三步：在过滤出来的TCP流中进一步分析</strong></li>\n</ul><p>在我们点击TCP Stream的时候，Wireshark会弹出一个窗口，显示解读好的应用层信息。如果是HTTP明文（非HTTPS加密），那就可以直接看到里面的内容了。</p><p><img src="https://static001.geekbang.org/resource/image/bb/ce/bb5faedc20ae2150a66f0b8e3c0e35ce.jpg?wh=906x251" alt=""></p><p>回到主窗口，此时过滤生效，所以只有属于这个TCP流的包才被显示，其他无关的数据包都已经被隐藏了。</p><p><img src="https://static001.geekbang.org/resource/image/fd/68/fda1ff0889d2a2a614a210028eea5a68.jpg?wh=634x121" alt=""></p><p>可见，TCP三次握手后，客户端发起了一个HTTP请求，即GET /overview HTTP/1.1，但服务端回复的是TCP RST，怪不得访问失败了。我画了下面这张示意图供你参考：</p><p><img src="https://static001.geekbang.org/resource/image/59/2f/59f9dc3eed0186d8503eb935yyf3982f.jpg?wh=2000x904" alt=""></p><p>握手能成功，看起来网络连通性没有问题，然后发送HTTP请求后收到RST，感觉服务端的嫌疑很大。</p><p>我们找到了负责服务端的同事，不过，他们说完全没有做RST这种设置，而且反问：“不是说Wi-Fi就都正常吗？二楼以外的其他楼层也都可以，不是可以排除服务端原因了吗？”</p><p>客户端没问题，服务端也没问题，是不是就要怀疑是防火墙了？但还是那个困境：我们并没有防护墙的权限，无法证实这件事。</p><p>那我们还是回到网络本身来查看，静下心来思考一下。</p><p><img src="https://static001.geekbang.org/resource/image/9f/76/9f9c388a51e688cf4e3df44628194576.jpg?wh=2000x520" alt=""></p><p>其实，虽然抓包分析是一个很好的排错方法，但是一般在中间设备（交换机、防火墙等）上不方便抓包，所以主要途径还是在客户端或者服务端的抓包文件里寻找端倪。当然，我们也可以争取条件到服务器上抓个包，然后再结合两侧抓包文件，进行对比分析，会更容易得出结论，这也是上节课我介绍过的方法。</p><p>不过，<strong>有没有办法只根据客户端的抓包，就能确定这次问题的根因呢？</strong>这个比较悬。但是，对于当前这种场景，如果能掌握到一个关键信息，我们就可以直接得出结论。</p><p>它就是<strong>TTL</strong>（Time To Live）。你可能会觉得，这个知识点简直太简单了，真的能解决我们的问题吗？且听我慢慢说。</p><h3>TTL详解</h3><p>TTL是IP包（网络层）的一个属性，字面上就差不多是生命长度的意思，每一个三层设备都会把路过的IP包的TTL值减去1。而IP包的归宿，无非以下几种：</p><ul>\n<li>网络包最终达到目的地；</li>\n<li>进入路由黑洞并被丢弃；</li>\n<li>因为网络设备问题被中途丢弃；</li>\n<li>持续被路由转发并TTL减1，直到TTL为0而被丢弃。</li>\n</ul><p>在<a href="https://www.rfc-editor.org/rfc/pdfrfc/rfc791.txt.pdf">RFC791</a>中规定了TTL值为8位，所以取值范围是0~255。</p><p><img src="https://static001.geekbang.org/resource/image/ff/7f/ffe7b69e697e52549ea194c2f6e3fb7f.jpg?wh=546x269" alt=""></p><p>因为TTL是从出发后就开始递减的，那么必然，网络上我们能抓到的包，它的当前TTL一定比出发时的值要小。而且，我们可能也早就知道，TTL从初始值到当前值的差值，就是经过的三层设备的数量。</p><p>不同的操作系统其初始TTL值不同，一般来说Windows是128，Linux是64。由此，我们就可以做一些快速的判断了。比如我自己测试ping <a href="http://www.baidu.com">www.baidu.com</a>，收到的TTL是52，如下图：</p><p><img src="https://static001.geekbang.org/resource/image/87/2f/870b69feb1406c744fe33f75607e212f.jpg?wh=528x106" alt=""></p><p>这个百度服务端大概率是Linux类系统，因为用Linux类系统的TTL 64减去52，得到12。这意味着这个回包在公网上经过了12跳的路由设备（三层设备），这个数量是符合常识的。</p><p>假如百度服务端是Windows，那么Windows类系统一般TTL为128，减去52，得到76。那就意味着这个回包居然要途经76个三层设备，这显然就违背常识了，所以反证这个百度服务端不会是Windows。这就是我想说的<strong>第一点：TTL的值是反映了网络路径跳数的，也可以通过它间接推导出对端的OS类型。</strong></p><blockquote>\n<p>补充：现今绝大部分网站的接入环节都是*nix类系统，所以这里只是单纯关于TTL这个知识点的技术讨论，不涉及“哪种OS是服务端主流”这种有争议的话题。</p>\n</blockquote><p>接下来第二点更加关键了。同样两个通信方之间的数据交互，其数据包在公网上容易出现路径不同的情况。就好比你每天开车上班，一般也有不止一条线路，无论走哪一条，只要能到公司就可以了。那么你和百度之间的路径，上午是12跳，下午变成13跳，或者11跳，也都属于正常。</p><p>但是内网不同，因为内网路径相对稳定，一般不会变化。如果出现TTL值的波动，特别是当这个波动值比较大（比如超过2）的时候，那几乎就说明这些包的不同寻常了。这就是我想说的<strong>第二点：内网同一个连接中的报文，其TTL值一般不会变化。</strong></p><p>回到我们这次的案例。我们就按这个思路，来排查下这仅有的5个数据包：</p><p><img src="https://static001.geekbang.org/resource/image/63/32/63f0f21dded5e5fd7f91d0175b3bc432.jpg?wh=595x131" alt=""></p><p>因为TTL并不是默认的展示列，所以我们需要选中报文，然后到IP详情部分去查看，像下面这样：</p><p><img src="https://static001.geekbang.org/resource/image/76/f7/767yy4d279bd8d6834e77a2c423b14f7.jpg?wh=1102x578" alt=""></p><p>不过，这样看几个报文的TTL还行，再多点就十分困难了。怎么做才能更有效率呢？答案是借助Wireshark的自定义列。我们可以这样做：</p><ul>\n<li>选中任意一个报文，点开IP详情，找到Time to live，然后右单击后选中Apply as column。</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/5e/71/5ec050646e50d0f373279b6ef3462571.jpg?wh=819x416" alt=""></p><ul>\n<li>然后在主窗口中，就多出了一列Time to live，对比起来非常方便。</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/bc/36/bcb25393d56c8aeceab1825a17f8ca36.jpg?wh=1984x276" alt=""></p><p>你也能很清楚地看到，同样的服务端，在三次握手中（SYN+ACK报文）的TTL是59，在导致连接中断的RST包里却变成了64！显然，<strong>这个RST包并不是跟我们握手的那个服务端发出的，</strong>否则TTL值就不会变化。</p><p>发出这个包的会是谁呢？其实，一般就是防火墙设备。由于防火墙也遵循IP协议，而这里的TTL值是64，这就说明这个防火墙跟客户端之间没有别的三层环节，或者说是三层直连的。</p><p>我们可以用一张简单的图来概括这个案子：</p><p><img src="https://static001.geekbang.org/resource/image/04/29/042f29c03e83332c9dbce93383d89129.jpg?wh=2000x525" alt=""></p><p>这样，我们底气大增！根据我们提供的信息，负责防火墙的同事就去复查了下，果然有发现：防火墙上对二楼有线网络有一条可疑的策略，跟其他线路不同。这条策略的出发点是：每个网络协议规定了协议数据格式以及标准端口号，所以协议数据跟端口号不匹配的话，就可以认为是“有害”流量。因为HTTP协议标准端口是TCP 80，但是我们这个Web站点是3001端口的，被防火墙认为不一致，所以就拒掉了。</p><p>我们来看一下当时的防火墙的配置：</p><p><img src="https://static001.geekbang.org/resource/image/74/86/749e8905b7b1728f7c523c309fa3b886.jpg?wh=705x219" alt=""></p><p>这里的application-default就是说，端口需要跟协议匹配。要不然就会被禁止，也就是回复RST给客户端，终止这条连接。这个防火墙策略被修正后，问题也立刻被解决了。</p><h2><strong>案例2：访问LDAPS服务报connection reset by peer</strong></h2><p>案例1中，我们在有权限的客户端抓了包。那么，如果两端都做了抓包，那么是否可以看到更多的风景呢？</p><p>这是我们最近遇到的一个例子。eBay内部有一个用户报告，他的服务到LDAPS服务失败了（这个LDAPS在我们维护的Windows AD域控上），遇到了connection reset by peer的报错。LDAPS就是LDAP的TLS版本，它监听的端口是636。正好两端的操作权限我们都有，就可以做两侧抓包了。</p><p>我们先看一下客户端这边的抓包文件，确实有看到服务端返回了TCP RST包：</p><p><img src="https://static001.geekbang.org/resource/image/bb/f3/bb993ceb3733edbb80fdc38559709cf3.jpg?wh=850x146" alt=""></p><p>图中可见，TCP三次握手正常完成了，随后的TLS握手Client Hello包也发送出来了，但是服务端回复的是RST包，这也就是引起客户端显示connection reset by peer报错的原因。</p><p>接着，我们再来看下服务端抓包文件的情况：</p><p><img src="https://static001.geekbang.org/resource/image/a9/9f/a9a90767e9678151f3137eace0d35a9f.jpg?wh=875x114" alt=""></p><p>显然，这里也有一个从客户端回复的TCP RST。</p><p>不过，再仔细对比服务端抓包和客户端抓包，是不是有些异样呢？为什么客户端抓包里有TLS Client Hello报文，但在服务端抓包里却没有呢？是因为Client Hello报文延迟了吗？</p><p><img src="https://static001.geekbang.org/resource/image/fb/7d/fb59565b2yyf833fa7b39610bf87637d.jpg?wh=2000x1125" alt=""></p><p>如果是延迟，那么这个Hello报文还是会出现在服务端抓包文件里，比如出现在RST报文的后面。但事实上，服务端抓包里并没有这个Hello报文的存在。所以这就反证了：这个Hello不是延迟，而是<strong>确实就没到服务端</strong>。</p><p>那么问题来了：</p><ul>\n<li>客户端：发送了TCP握手 + 发送了Client Hello。</li>\n<li>服务端：收到了TCP握手 + 没有收到Client Hello。</li>\n</ul><p>这个现象足以让我们怀疑：两者之间存在着一个“看不见的东西”，这个东西把Client Hello报文给“吃了”。</p><p>我们通过案例1的学习，已经了解到TTL是判断“是否有防火墙”的非常直接方便的方法。那么对于当前这个案例，我们也一样检查一下TTL。</p><p><strong>从服务端视角来看</strong>，它收到的报文只有三个：SYN包、ACK包、最后的RST包。我们选择SYN和RST，来对比看下它们的TTL是多少。下图是SYN包：</p><p><img src="https://static001.geekbang.org/resource/image/37/df/3704b5045ec543b4yy6a4e2909620cdf.jpg?wh=583x301" alt=""></p><p>下图是RST包：</p><p><img src="https://static001.geekbang.org/resource/image/cd/81/cd5ffe857073fd7f997a0yy891b6ab81.jpg?wh=513x292" alt=""></p><p>显然，跟之前的案例类似，这里的TTL也发生了明显的变化。你应该也明白了，<strong>这两个包并不是同一个设备发出的</strong>。</p><p>因为这个案例里，客户端我们也抓包了，所以可以来看看<strong>客户端抓包</strong>里，是否也有TTL不同的现象呢？</p><p>客户端收到的SYN+ACK包的TTL是110：</p><p><img src="https://static001.geekbang.org/resource/image/45/ac/45ea9fe82772b67cca4ebae84248caac.jpg?wh=1124x440" alt=""></p><p>客户端收到的RST包的TTL是54：</p><p><img src="https://static001.geekbang.org/resource/image/c8/ee/c887e9a8fd6af5b20b34c5d23f7175ee.jpg?wh=1080x426" alt=""></p><p>终于，结合两侧的抓包，我们就可以把这个拼图给拼完整了，而Client Hello报文丢失之谜，也将揭晓。更新一下前面的示意图，会变成下面这样。显然，Client Hello报文就是被防火墙丢弃了。</p><p><img src="https://static001.geekbang.org/resource/image/0f/c8/0f4277b5b171eb189f487d32e7a420c8.jpg?wh=2000x1125" alt=""></p><p>可见，防火墙的拦截行为可能出现在多个方向上（面对客户端时代表服务端，面对服务端时代表客户端），毕竟报文都要经过它，它如果想乱来，通信两端还真的无法控制它。从上图来看，防火墙两边“截胡”，两边拒绝，两边还都只好乖乖地听话，结束了连接。你都不能说它“没有武德”，因为整个过程都是完全遵照了TCP规范的，防火墙做得不可谓不周到。</p><p>不过百密一疏，它偏偏<strong>在TTL上露出了马脚</strong>，被我们抓了个现行。</p><p>你看，小小一个TTL，在这里能起到如此关键的作用，真是不可小视。connection reset by peer的问题，我们在第4讲专门讲TCP挥手的时候就仔细分析过了。不过，你还记得为什么那次的情况跟这次不同吗？对，那次的场景里，<strong>RST真的是对端发出来的</strong>，并不是防火墙从中作梗，所以TTL也都正常。</p><p>那么，今天的课程给你介绍的就是connection reset by peer的另外一种可能。你可以仿照我这里介绍的排查过程，然后拿证据去跟网络安全团队沟通就行了。相信此时的你，说服力远远超过单纯抱怨而没有实质证据的你。</p><h2>如何应对？</h2><p>防火墙简单插入一个RST，就可以终止连接，确实令人无奈。当然，这对网络安全来说，也许就是一种特性（Feature）了。正所谓“他人的美味，可能是你的毒药”。</p><p>我们通过上面介绍的排查过程，确认了防火墙的存在。那么，你可能会问了：接下来我们有什么办法可以规避这个问题呢？</p><p>这个问题的核心，就是既然我们可以准确地定义什么是防火墙插入的RST报文，那是否意味着我们就可以避开它了？比如你有没有想到：</p><p><strong>干脆直接丢弃这个报文！</strong></p><p>这个想法很大胆，好像也挺合理。但稍一细想觉得有几个现实问题得先解决：</p><ul>\n<li>我们用什么手段来达到“报文丢弃”这个目的呢？</li>\n<li>我们做丢弃的时候，内核的TCP协议栈是否已经被“毒害”了呢？如果真是那样的话，即使我们丢弃了RST报文，实际上还是没有帮助的吧？</li>\n</ul><p>对于第一个问题，我想如果你熟悉Linux网络的话，可能第一时间就会想到 <strong>iptables</strong> 了。是的，我们就是可以利用它来达到“丢弃RST报文”的目的。当然，你做这个事情的时候必须十分小心，毕竟RST也是TCP协议里定义的标准，无论你是否喜欢它，RST在很多场景里是必需的，一股脑地丢弃一定会引来难以预料的麻烦。</p><p>所以，我们可以对这条iptables规则设定精确的限定条件，使得它既能帮助我们丢弃“有害”的RST报文，同时也不影响到其他正常连接的交互。</p><p>因为我们很难有条件拥有一台真实的防火墙设备来帮助我们做这个实验，所以只能找一个办法来模拟防火墙。怎么做呢？我们还是借助iptables。</p><p>这里，真是要感谢创造了iptables的内核子模块——Netfilter的内核开发团队了，有了Netfilter，基于它的iptables等工具在我们日常工作中起到了很大的作用。</p><p>对于第二个问题，其实不用担心，在报文进来的方向，报文会经过这样的处理流程：</p><pre><code class="language-plain">PREROUTING -&gt; INPUT -&gt; 本地处理 -&gt; OUTPUT -&gt; POSTROUTING\n</code></pre><p>所以，当我们在INPUT链上创建了丢弃RST报文的规则，那么当这个RST报文进入到机器时，会被这条规则丢弃，进不到本地处理，也就是不会有被内核协议栈处理的机会！那么，我们的TCP连接就不会被“毒害”了。还好，我们还有这张王牌可以打，只要对系统和网络足够熟悉，总还是有一线生机。</p><h2>动手实践</h2><p>现在“理都懂”了，让我们来动手实操一下。我们需要搭建这么一个测试环境：</p><ul>\n<li>虚拟机1（下面简称为1）：配置为客户端，实验时会在这台上执行telnet，模拟访问行为。</li>\n<li>虚拟机2（下面简称为2）：配置为客户端的网关，这样它就可以劫持流量，模拟防火墙行为。</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/bd/14/bd1ae50db55ea5d960787a241efff914.jpg?wh=2000x740" alt=""></p><h3>实验1：telnet第三方站点</h3><p>在1上，直接telnet <a href="https://www.baidu.com">www.baidu.com</a> 443，可以成功。</p><p><img src="https://static001.geekbang.org/resource/image/f3/19/f352e677d3eb81f7bdd2122a3dd83719.jpg?wh=666x164" alt=""></p><p>然后我们需要配置一下，让1访问<a href="https://www.baidu.com">www.baidu.com</a>的流量强制经过2，这样后续我们就可以让2来操控1和baidu之间的连接了。接下来步骤稍多，感谢你的耐心。</p><p>在虚拟机1上，我们需要完成这么几件事。</p><p>创建隧道，隧道另一头就是虚拟机2，我们将在那里模拟一个“防火墙”。在上节课里，我们了解了ipip隧道，这里的GRE隧道也是类似的工作原理。</p><pre><code class="language-plain">ip tun add tun0 mode gre remote 172.17.158.46 local 172.17.158.48 ttl 64\nip link set tun0 up\nip addr add 100.64.0.1 peer 100.64.0.2 dev tun0\n</code></pre><p>添加路由项，使得本地去往第三方站点的流量，都走这条路由，也就是通过隧道到达虚拟机2，然后2来转发报文。</p><pre><code class="language-plain">ip route add 110.242.68.0/24 via 100.64.0.2 dev tun0\n</code></pre><p>当然了，虚拟机2上面也需要做对等的隧道配置：</p><pre><code class="language-plain">ip tun add tun0 mode gre remote 172.17.158.48 local 172.17.158.46 ttl 64\nip link set tun0 up\nip addr add 100.64.0.2 peer 100.64.0.1 dev tun0\n</code></pre><p>虚拟机1把报文发到虚拟机2，但是如果后者不做配置，默认是会丢弃这些报文的，所以还需要在2上开启ip_forward：</p><pre><code class="language-plain">sysctl net.ipv4.ip_forward=1\n</code></pre><p>我们在2上运行tcpdump port 443，然后1上运行telnet <a href="https://www.baidu.com">www.baidu.com</a> 443。在2的tcpdump窗口里，已经可以看到从1过来的流量了！</p><pre><code class="language-plain">root@server$tcpdump port 443\ntcpdump: verbose output suppressed, use -v or -vv for full protocol decode\nlistening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes\n16:53:36.054124 IP 100.64.0.1.34396 &gt; 110.242.68.3.https: Flags [S], seq 3364415625, win 64620, options [mss 1436,sackOK,TS val 2049305210 ecr 0,nop,wscale 7], length 0\n16:53:37.084514 IP 100.64.0.1.34396 &gt; 110.242.68.3.https: Flags [S], seq 3364415625, win 64620, options [mss 1436,sackOK,TS val 2049306241 ecr 0,nop,wscale 7], length 0\n16:53:39.100482 IP 100.64.0.1.34396 &gt; 110.242.68.3.https: Flags [S], seq 3364415625, win 64620, options [mss 1436,sackOK,TS val 2049308257 ecr 0,nop,wscale 7], length 0\n</code></pre><p>咦？抓包里只有SYN包而没有SYN+ACK，1这头的telnet也挂起，没响应了，这是怎么回事？</p><p><img src="https://static001.geekbang.org/resource/image/0d/80/0df85df7ba457f5f5a6476c0b52cfe80.jpg?wh=830x106" alt=""></p><p>原来，我们还需要设置一下NAT，要不然出去的报文源IP是100.64.0.1，回包也会回这个地址，显然回不到2了。</p><p><img src="https://static001.geekbang.org/resource/image/0e/b2/0ef388aa8ba51dd283cc465110df37b2.jpg?wh=2000x779" alt=""></p><p>所以还需要在2上启用SNAT：</p><pre><code class="language-plain">iptables -t nat -A POSTROUTING -d 110.242.68.0/24 -j MASQUERADE\n</code></pre><p>我们再试试在1上发起telnet www.baidu.com 443，果然成功了。</p><p><img src="https://static001.geekbang.org/resource/image/fd/32/fd31162403e5efe41a3f044e1f787832.jpg?wh=680x172" alt=""></p><p>2上的tcpdump也抓取到了正常连接的报文（这里就不贴了）。</p><h3>实验2：插入RST报文，连接失败</h3><p>现在，我们需要在2上配置一个“插入RST报文”的动作，这样就可以模拟“防火墙阻隔TCP连接”的效果了。</p><p><img src="https://static001.geekbang.org/resource/image/07/58/07a9100e6dbb2f963062039611911158.jpg?wh=2000x785" alt=""></p><p>我们可以在2上运行这条iptables命令：</p><pre><code class="language-plain">iptables -I FORWARD -p tcp -m tcp --tcp-flags SYN SYN -j REJECT --reject-with tcp-reset\n</code></pre><p>有了这条命令，2就用TCP RST拒绝了转发链（也就是命令中的FORWARD链）上的SYN报文。1上的telnet立刻收到了拒绝：</p><p><img src="https://static001.geekbang.org/resource/image/89/bb/898e33a81b5bfdecc78e22a98d2415bb.jpg?wh=906x138" alt=""></p><p>2上的tcpdump抓包窗口里也看到了握手和拒绝的报文：</p><pre><code class="language-plain">root@server$tcpdump -i any port 443\ntcpdump: verbose output suppressed, use -v or -vv for full protocol decode\nlistening on any, link-type LINUX_SLL (Linux cooked v1), capture size 262144 bytes\n17:02:16.623480 IP 100.64.0.1.34428 &gt; 110.242.68.3.https: Flags [S], seq 3314573698, win 64620, options [mss 1436,sackOK,TS val 2049825780 ecr 0,nop,wscale 7], length 0\n17:02:16.623518 IP 110.242.68.3.https &gt; 100.64.0.1.34428: Flags [R.], seq 0, ack 3314573699, win 0, length 0\n</code></pre><p>可见，这个RST实实在在地起到了类似防火墙的作用，让你的连接无法建立。你看，其实防火墙也没那么神秘，我们也可以实现。可以小小地鼓励一下你自己！</p><h3>实验3：丢弃RST报文，连接成功</h3><p>这套实验的核心目标是实现对RST干扰报文的规避，也就是丢弃这类报文。让我们继续实验，在1上添加这么一条iptables规则：</p><pre><code class="language-plain">iptables -I INPUT -s 110.242.68.0/24 -p tcp --sport 443 -m tcp --tcp-flags RST RST -m ttl --ttl-eq 64 -j DROP\n</code></pre><p><img src="https://static001.geekbang.org/resource/image/8a/09/8a5afaa9f464219e6a0cb1cf526f4809.jpg?wh=2000x778" alt=""></p><p>有了这条规则，我们就对符合条件的TCP报文进行了丢弃，这个条件就是“来自110.242.68.0/24网段的TCP源端口为443的，带RST标志位的，TTL等于64的报文”。</p><p>这里的TTL条件就是关键了。在实际场景下，你就可以根据防火墙插入的RST报文的TTL的实际特征，写一条精确匹配的规则，把它跟正常报文区分开，进行精准的丢弃。</p><p>我们还是在1上telnet，然后发现这次不再被reset，而是挂起了。在1的tcpdump中，也看到SYN发出了，对方也回复了RST，但是我们并没有被真的被reset。这里，正是这条丢弃RST报文的iptables规则起到了效果。</p><pre><code class="language-plain">root@client2:~# tcpdump -i any host 110.242.68.4 and port 443\ntcpdump: verbose output suppressed, use -v or -vv for full protocol decode\nlistening on any, link-type LINUX_SLL (Linux cooked v1), capture size 262144 bytes\n17:27:50.748194 IP client2.53438 &gt; 110.242.68.4.https: Flags [S], seq 1164905016, win 64620, options [mss 1436,sackOK,TS val 2201853747 ecr 0,nop,wscale 7], length 0\n17:27:50.748433 IP 110.242.68.4.https &gt; client2.53438: Flags [R.], seq 0, ack 1164905017, win 0, length 0\n</code></pre><p>你可能这里稍有疑惑：1持续发SYN，2持续回复RST，那我们这个连接不是依然没建立起来吗？</p><p>其实，一般防火墙工作模式跟实验里的稍有不同。这次两个案例中的防火墙都运行在应用层模式，也就是会先让TCP连接建立，然后检查后续的报文，发现不符合安全策略的时候，就插入一个RST报文，终止这条连接。这种动作一般是<strong>一次性</strong>的。</p><p>而在这个实验中，我们只要让iptables的REJECT只生效一两次（比如用下面的复合命令），TCP连接就只是在最初几秒被短暂干扰，之后就依然能成功建立。</p><pre><code class="language-plain">iptables -I FORWARD -p tcp -m tcp --tcp-flags SYN SYN -j REJECT --reject-with tcp-reset;sleep 5;iptables -D FORWARD -p tcp -m tcp --tcp-flags SYN SYN -j REJECT --reject-with tcp-reset\n</code></pre><p><video poster="https://media001.geekbang.org/a66f5657807047f4811dc247681f36b9/snapshots/6b36bf1a402a456faa5e1f37a7a4a376-00005.jpg" preload="none" controls=""><source src="https://media001.geekbang.org/customerTrans/7e27d07d27d407ebcc195a0e78395f55/4ab8302d-17e8785b6ad-0000-0000-01d-dbacd.mp4" type="video/mp4"><source src=" https://media001.geekbang.org/ee3cd191ebdf42ff92a82068295c24f6/e58a72675f2647c186f56f74d4ac05ac-7fd3a8647b364c84ffe42760de9fa4e2-sd.m3u8" type="application/x-mpegURL"></video></p><p>在实际场景中，只要设置前面提到的iptables丢弃特定RST报文的规则，就还有很大的几率能让这条连接继续保持下去，应用也运行下去。防火墙居然对你无效了，你除了长舒一口气，会不会心里也冒出“终于翻身当主人”的感觉？</p><h3>拓展思考</h3><p>那么，除了这种丢弃有害RST的办法，还有没有别的办法呢？</p><p>就上面探讨的丢弃RST的方法来说，这是一个“<strong>应对式</strong>”的策略，也就是有人要“害我”，那我把“毒药”给扔了。但仍然是“被动”的方法。如果思考得更进一步，我们有没有办法，使得别人都没有机会来害我呢？就是你连“下毒”的机会也没有？这就是“<strong>主动</strong>”的策略了。</p><p>我个人看法是，可以到<strong>网络层</strong>（IP层）去寻找机会。利用IPSec（比如IPv6默认启用了IPsec），我们就获得了在第三层加密的能力。因为就连IP报文本身都是加密的，那么即使防火墙要插入报文，因为它不具备密钥，所以这个报文会被接收端认为非法而被丢弃。这样就有希望真正摆脱防火墙对传输层（TCP/UDP）的这种控制。</p><h2>小结</h2><p>上节课我们采用的方法，是通过两端抓包后进行网络包的对比分析，排查定位到防火墙的存在，这种方法对于<strong>丢包、乱序等场景</strong>特别有用。而这节课的方法呢，是通过分析TTL值的变化，快速定位到防火墙的存在。这种方法，对于<strong>连接被重置（RST）的场景</strong>，十分有效。</p><p>那么在学完这节课之后，你需要记住以下几个关键要点：</p><ul>\n<li><strong>需要在受影响的客户端或者服务端进行抓包。</strong>这样你才能获取到你需要的关键信息，而这种信息，单纯通过应用层日志等途径，是很难获取的，这也是应用层排查的天然的不足。对此，你需要有清醒的认识，并深刻理解网络层排查技术的重要性和不可替代性。</li>\n<li><strong>分析抓包文件，识别TTL的变化。</strong>这里，你需要了解网络层和IP协议的相关知识点。同时也要明白，即使一个知识点看似简单，其背后的设计原理，都大有文章。对每个技术细节的推敲，能帮助我们打造出更为强大的技术底蕴。</li>\n<li><strong>灵活运用Wireshark自定义列</strong>。我们通过添加自定义列，让每个报文的TTL值都在主视图中展现，极大地方便了对这些TTL的比较。所以我们除了掌握协议知识以外，也要挖掘各类工具的使用技巧。所谓“工欲善其事，必先利其器”也。</li>\n</ul><p>另外，在这节课的最后，我们也通过一系列实验，再一次深入理解了RST报文的作用，以及可能的规避方法。在这个过程中，我们学习了：</p><ul>\n<li><strong>GRE隧道的搭建和用途</strong>：你可以用ip tunnel add命令创建GRE隧道，并用ip route add命令配置路由项，让某些网络的流量转而走这个隧道网关。注意，即使是一个二层不可达的IP，通过隧道也可以“包装成”二层可达，进而可以配置为网关。这一点，如果不借助隧道，是无法实现的。</li>\n<li><strong>用iptables实现对报文的操控</strong>：在你需要模拟一些问题场景的时候，不妨多发掘一下iptables的“潜能”，比如可以丢弃符合某种条件的报文：</li>\n</ul><pre><code class="language-plain">iptables -I INPUT -s 110.242.68.0/24 -p tcp --sport 443 -m tcp --tcp-flags RST RST -m ttl --ttl-eq 64 -j DROP\n</code></pre><ul>\n<li>我们也学习了如何用iptables结合内核配置，<strong>实现一个简单的NAT网关</strong>：</li>\n</ul><pre><code class="language-plain">iptables -t nat -A POSTROUTING -d 110.242.68.0/24 -j MASQUERADE\nsysctl net.ipv4.ip_forward=1\n</code></pre><h2>思考题</h2><p>这节课，我给你介绍了利用防火墙的TTL跟原先的正常报文的TTL不一致的特点，从而识别出防火墙的方法。</p><p>那么，假设有一天，防火墙公司把这个特性也完善了，我们再也不能仅仅凭TTL的突变而发现防火墙了，你觉得<strong>还有什么办法可以识别出防火墙吗？</strong>这是一个开放式的话题，欢迎你把答案写在留言区，与同学们一同分享。</p><h2>附录</h2><p>抓包文件：<a href="https://gitee.com/steelvictor/network-analysis/tree/master/06">https://gitee.com/steelvictor/network-analysis/tree/master/06</a></p>',
        article_title: "06 | 定位防火墙（二）：网络层的精确打击",
      },
      {
        title: "07 | 保活机制：心跳包异常导致应用重启？",
        id: 482610,
        content:
          '<p>你好，我是胜辉。这节课，咱们来聊聊TCP的保活机制。</p><p>以前的电视剧里经常会有这样的剧情：女主因为车祸失去了记忆，男主一边摇着女主的肩膀，一边痛苦地问道：“还记得我吗？我是欧巴啊！”可是女主已经对此毫无记忆，迷茫地反问道：“欧巴是谁？”</p><p>类似地，TCP其实也需要一种机制，让双方能保持这种“记忆”。Keep-alive这个词，你可能也听说过。特别是当遇到一些连接方面的报错的时候，可能有人会告诉你“嗯，你需要设置下Keep-alive”，然后问题确实解决了。</p><p>不过，你有没有深入思考过这样几个问题呢：</p><ul>\n<li>Keep-alive跟长连接是什么关系？</li>\n<li>它是应用层代码独立实现，还是依赖操作系统的TCP协议栈去实现？</li>\n<li>在HTTP层面也有一个Keep-alive的概念，它跟TCP的Keep-alive，是同一个东西吗？</li>\n</ul><p>如果你对这几个问题的答案还不清楚，那么这节课，我就来帮助你厘清这些概念。以后你再遇到长连接失效、被重置、异常关闭等问题的时候，就知道如何通过抓包分析，解读出心跳包相关的信息，然后运用Keep-alive的相关知识点，去真正解决前面说的一系列问题。</p><p>好，按惯例，我们还是从案例说起。</p><h2>TCP长连接为何总中断？</h2><p>当时我在云计算公司就职，有个客户的应用基于TCP长连接，但长连接经常中断，引起了应用方面的大量报错。由于客户的业务是支付相关的，对实时性和安全性要求很高，这类报错就产生了较大的负面影响，所以急需解决。</p><!-- [[[read_end]]] --><h3>应用概况</h3><p>这个应用的架构比较简单：客户在云平台上部署了多台云主机，其中一台云主机专门做加解密，称之为加密服务器。另外几台云主机作为这台加密服务器的客户端，跟这台加密服务器保持TCP长连接。这些客户端会不时地跟加密服务器进行通信，完成加密操作。每45秒，客户端还会发送一次心跳包，这有两个作用：</p><ul>\n<li>维持这个长连接不被中断，即<strong>心跳保活</strong>，让长连接在两端保持下去；</li>\n<li>探测加密服务器的可用性，即<strong>健康检查</strong>，一旦服务不可用，客户端就要去掉这个失效的连接。</li>\n</ul><p>就像下图这样：</p><p><img src="https://static001.geekbang.org/resource/image/ac/49/ac7d92d0d0a77e425byy00ef52b0a149.jpg?wh=2000x1125" alt=""></p><p>如果加密服务器能在<strong>1秒内</strong>对心跳包进行回复，那么客户端就认为服务端正常可用，后续的数据交互（即加密请求）将继续在这条长连接上进行。而如果服务端未能在1秒内回复，那么客户端会认为该长连接已经中断，于是重启应用，发起一条新的长连接，并在日志中记录一次报错。</p><p>用类Python语法来描述，大体是这样的：</p><pre><code class="language-plain">while true:\n&nbsp; &nbsp; sleep(45)\n&nbsp; &nbsp; if Keep_alive_probe() is true:\n&nbsp; &nbsp; &nbsp; &nbsp; continue\n&nbsp; &nbsp; else:\n&nbsp; &nbsp; &nbsp; &nbsp; restart()\n&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;log_error()\n</code></pre><h3>排查思路</h3><p>整体的排查思路跟<a href="https://time.geekbang.org/column/article/477510">网络分层模型</a>有点类似，逐层往下。我们需要先从应用层查找原因，然后是操作系统层面，最后是网络层分析。排查的顺序就是这样的：</p><blockquote>\n<p><strong>应用层代码 -&gt; 操作系统的时间配置 -&gt; 网络的抓包分析</strong></p>\n</blockquote><p>首先，客户自查了应用，没有发现可疑代码，并且尝试过重新部署代码、重装系统、更换IP等，但都未奏效。</p><p>那么，会不会是时间服务（ntp）的问题呢？我们检查了两侧机器的ntpd服务的状态，都是正常的。对比了两端机器的实际时间，也没有发现明显的误差。于是这个可能性也被排除了。</p><p>然后就需要排查网络了。我们在一台客户端上启动了抓包程序tcpdump，过滤条件是对端加密服务器的IP和端口。抓多久呢？因为问题大约十几分钟出现一次，那么按照这个频率，我们设定抓包时长为半个小时，得到了抓包文件。这样就能覆盖到一到两次的错误了。</p><blockquote>\n<p>补充：抓包示例文件已经上传至<a href="https://gitee.com/steelvictor/network-analysis/blob/master/07/heartbeat.pcap">Gitee</a>，建议用Wireshark打开文件，结合文稿学习。</p>\n</blockquote><p>因此，通过检查应用日志，我们发现在抓包时间段内，应用日志又记下了两次报错，比如下面这个：</p><p><img src="https://static001.geekbang.org/resource/image/84/89/84d35533cb2e21e39f5c06504dcdb689.png?wh=498x63" alt="图片"></p><p>从日志上看，17:08:02这个时间点发生了一次报错，17:08:10发生了一次程序的重启。</p><p>有了应用层的信息，接下来，我们就需要在抓包文件中，<strong>找到传输层和网络层的信息来与应用层信息对应</strong>，这样排查工作才能继续推进。</p><p>但是日志记录的时间戳粒度太粗了，只精确到了秒级，而网络报文在Wireshark中都是微秒级的，一秒内可能会有成百上千个报文。所以，仅凭精确到秒的一个时间戳，还不足以让我们在几十个报文中，精确地找到对应的报文。这个时候，我们需要调整一下抓包文件分析工作的切入点。</p><p>一个常规的做法是，<strong>跳过案例本身的问题特殊性不谈，先从宏观上把握一下排查思路</strong>。也就是先不纠结于根据日志时间来寻找报文的难题，而是先看Expert Information，找一找有没有比较可疑的报文。打开Expert Information窗口，如下：</p><p><img src="https://static001.geekbang.org/resource/image/58/73/587e830687d8b9786f09dffe8fba7773.png?wh=1880x306" alt="图片"></p><p>可见，有80个Error级别的Malformed Packet（格式错误）报文和2个RST报文，都很可疑。</p><p>我们先看这80个Malformed Packet报文，这传递了什么信息呢？在Protocol栏显示SIGCOMP，说明Wireshark认为这80个报文是SIGCOMP协议的（SIP的信令压缩协议）。不过，客户的应用不是SIP相关的，显然这些报文应该是被Wiresharek误会了。</p><blockquote>\n<p>在这里，我也给你一个小小的<strong>提醒</strong>：毕竟只有被公开广泛支持的数据格式和协议才能在Wireshark中正确展示，所以如果你看到类似这种Malformed Packet的时候，还是要统筹考虑当前案例的实际情况，未必Malformed报文就真的是格式错误，也可能只是Wireshark不了解某种“方言”而已。</p>\n</blockquote><p>让我们看看这些报文究竟是什么。点开Error，选中一个报文。比如我这里选择37号报文：</p><p><img src="https://static001.geekbang.org/resource/image/07/fd/0760eb16f56ec70837ab52e27894affd.png?wh=1880x372" alt="图片"></p><p>随后光标会自动定位到37号报文这里：</p><p><img src="https://static001.geekbang.org/resource/image/b5/51/b50d75ae1b923d05a6b5997067416751.png?wh=1213x177" alt="图片"></p><p>然后选了另外几个Malformed Packet，发现它们都是成对出现的。它们还有一个特征是：</p><ul>\n<li>前一个报文的TCP载荷数据是01（十六进制）；</li>\n<li>后一个报文的TCP载荷数据是41（十六进制）。</li>\n</ul><p>这就像是某种“联动”机制了，会不会就是心跳报文呢？你注意下TCP Seglen这一栏（这是我添加的表示TCP载荷长度的列），有没有发现这类报文的长度都是1 ？我们就以 <code>tcp.len eq 1</code> 为过滤器，过滤出报文：</p><p><img src="https://static001.geekbang.org/resource/image/9e/91/9e44b98d20161159yyceabc3a11cfc91.png?wh=1225x274" alt="图片"></p><p>可见，37和38是0秒发生的，76和77是45秒后发生的，然后98和154以及后面每一对报文，也都符合45秒间隔的规律，正好跟客户程序的<strong>45秒</strong>心跳包机制对上了。我们可以确认这些报文就是<strong>心跳报文</strong>了！更确切地说，每次成对出现的两个报文中：</p><ul>\n<li>前一个是心跳探测包，它的TCP载荷数据为01（十六进制）；</li>\n<li>后一个是心跳回复包，它的TCP载荷数据为41（十六进制）。</li>\n</ul><p><strong>这是第一个比较明显的进展。</strong>也就是说，我们已经可以找到应用层的心跳包在网络层的展示形式了，这对于后续的排查非常有帮助。</p><p>不过，也不要忘了，我们是来排查“心跳包失败导致连接中断”的问题的，现在只是找到了心跳包的特征，还需要找到真正的跟日志报错相关的报文，而这个报文是跟“连接中断”现象有关的。</p><p>那么会是什么样的报文呢？其实，我们在第4讲就研究过<a href="https://time.geekbang.org/column/article/480068">TCP挥手</a>。你应该还记得，TCP的连接断开，无非跟两种报文有关：<strong>FIN和RST</strong>。</p><p>我们先尝试寻找FIN报文。输入过滤器：</p><pre><code class="language-plain">tcp.flags.fin eq 1\n</code></pre><p>结果发现什么报文都没有出来。可见，这次抓包里，连接的断开并不是用FIN完成的。</p><p><img src="https://static001.geekbang.org/resource/image/77/c1/7751974162ebff9452195c208f2752c1.png?wh=783x202" alt="图片"></p><p>接着就是寻找RST报文。其实，在Expert Information里面，就有2个RST报文，我们可以直接从那里入手。当然，用过滤器也同样方便，输入：</p><pre><code class="language-plain">tcp.flags.reset eq 1\n</code></pre><p>我们能找到2个RST报文：</p><p><img src="https://static001.geekbang.org/resource/image/25/e3/25f575477b2af6be54f51340a39040e3.png?wh=1111x79" alt="图片"></p><p>我们选中575号报文，然后Follow -&gt; TCP Stream，得到了这个RST所在的TCP流的全部报文：</p><p><img src="https://static001.geekbang.org/resource/image/1y/38/1yyb053e46e3dcf4702bdfa2df015f38.png?wh=1235x416" alt="图片"></p><p>报文很多，上图显示的只是一部分报文。我往前翻阅了更前面的报文，也能看到很多相对长的时间间隔，有17秒、39秒、11秒的，但也没有什么规律。这时候，我们就需要结合前面刚分析到的一些信息，要不然就要在报文的海洋里迷失方向了。</p><p>我们通过前面的分析发现，心跳探测包的报文数据是01，心跳回复包的报文数据是41。所以，让我们再次借助强大的过滤器。在 <code>tcp.stream eq 0</code> 后面添加 <code>and tcp.payload eq 01</code>，即整体过滤器变为：</p><pre><code class="language-plain">tcp.stream eq 0 and tcp.payload eq 01\n</code></pre><p>这样就过滤出了这个TCP流里面，<strong>所有的心跳探测包</strong>：</p><p><img src="https://static001.geekbang.org/resource/image/6c/0c/6c86e426b76f84dc529a66d05169cd0c.png?wh=1246x408" alt="图片"></p><p>可见，这些心跳探测包也是很明显遵循了45秒间隔的规律。不过，等一下，为什么572号心跳探测包跟它的上一次心跳探测，间隔的时间是<strong>58秒</strong>，而不是45秒呢？</p><p><img src="https://static001.geekbang.org/resource/image/fe/18/fed696dbb35675c0fb989a9728bf7218.png?wh=1221x41" alt="图片"></p><p><strong>这很反常，也是第二个很重要的发现。</strong>要知道，这45秒的机制是在代码里实现的，照理说不可能出现13秒这么大的误差。现在，我们把 <code>tcp.payload eq 01</code> 这个条件去掉，回到这个TCP流的完整报文区域来综合分析：</p><p><img src="https://static001.geekbang.org/resource/image/f7/43/f760066db14b0f950d3c71329ee22043.png?wh=1243x392" alt="图片"></p><p>我来给你解读一下：</p><ul>\n<li>572号报文（客户端发出）是心跳探测包；</li>\n<li>573号报文（服务端发出）是心跳回复包；</li>\n<li>574号报文（客户端发出）是客户端对573号报文的确认；</li>\n<li>575号报文（客户端发出）是一个RST报文，它跟客户端日志报错有直接的关系。</li>\n</ul><p>以上4个报文之间几乎没有时间上的停顿。这里，你可能会有个小疑问：为什么572号报文跟上一个报文的间隔是17秒而不是45秒呢？其实，这是<strong>Time列的类型</strong>导致的，我这里用的是Delta time displayed类型，是跟前一个被显示的报文的间隔时间。</p><p>之前我们看到很多个整齐的45秒，是因为那个窗口里显示的报文，都是过滤过的心跳报文，跟这里<strong>显示的报文</strong>不同，所以显示的间隔时间也不同。Wireshark里的Time一列有多种配置可选，所以你一方面要理解这里面各种Time的区别，一方面自己做分析的时候，也可以根据实际需要，灵活选择Time类型。</p><p><img src="https://static001.geekbang.org/resource/image/60/10/6020ec054ae4fc6a79062fe025fde810.png?wh=738x259" alt="图片"></p><p>考虑到这里的问题就是跟心跳包和挥手包直接相关，排除掉无关报文，可以让我们的分析思路也变得更加清晰。所以，我们再一次调整过滤器，改为：</p><pre><code class="language-plain">tcp.stream eq 0 and (tcp.len == 1 or tcp.flags.reset == 1)\n</code></pre><p>就得到这个TCP流里面的心跳包和挥手包：</p><p><img src="https://static001.geekbang.org/resource/image/2d/ec/2df9da0f4fc2a4cfbda7752e38de98ec.png?wh=1920x523" alt="图片"></p><p>我们最后再确认一下时间戳。这个隔了58秒才发出的心跳包，发送的时间是在17:08:10：</p><p><img src="https://static001.geekbang.org/resource/image/38/c9/38ced831f878b7b94fb4d28af42e1cc9.png?wh=640x138" alt="图片"></p><p>跟日志中restart时间一致：</p><p><img src="https://static001.geekbang.org/resource/image/7c/f3/7c9bbea46yyd3641448e2d502b8acff3.png?wh=548x73" alt="图片"></p><p>现在，事实已经很清楚了：客户端在连接被断开之前的心跳探测包，并没有遵循客户声称的“每隔45秒”，而是很意外地隔了58秒。我们结合程序逻辑，已经可以推断出真实的状况了。看示意图：</p><p><img src="https://static001.geekbang.org/resource/image/f0/7e/f03e54e7cb407456f4e324b85c8d7e7e.jpg?wh=2000x1125" alt=""></p><h3>结论：排除网络，追查代码</h3><p>现在，我们已经清楚了这个报错的原因：客户端程序出了一个Bug，某一次心跳探测包发出的时候是在上一次心跳的58秒以后（也就是相对正常情况，迟了13秒）。那么，服务端虽然立即发送了心跳回复包，但也一样是在58秒以后了。程序的逻辑非常简单粗暴：一旦心跳回复包的到达时间超过了第46秒（也就是45秒+1秒超时），就认为是心跳探测失败。</p><p><strong>为什么探测包本身会比预定的时间晚了13秒才发出呢？</strong>根据这个很明确的信息，客户再次检查了应用代码，终于定位到了出问题的代码段。修复代码后，问题随之解决。</p><p>补充一下，在这个案例中，<strong>异常时的心跳包探测和回复的耗时本身是正常的</strong>。我们可以看到包号572是客户端发出的心跳探测包，包号573是服务端发出的心跳回复包。两者之间，间隔只有0.000370秒，即0.37毫秒，完全是同机房内网时延的正常水平。</p><h2>理解心跳机制的原理</h2><p>显然，这个案例是关于应用层自己实现的心跳机制的，有一定的特殊性。但在机制上，也体现了心跳包的一些特点，比如：</p><ul>\n<li>定时发送心跳探测包；</li>\n<li>对于心跳回复包有超时限制。</li>\n</ul><p>而从更普适的尺度上来看，其实TCP本身提供的Keep-alive机制更为安全易用。</p><p>一般来说，对于操作系统已经实现的特性，我们最好<strong>直接去利用，而不是自己创造一个类似的轮子</strong>。这好比你想基于UDP，在应用层实现类似TCP的种种传输保障机制，也不是不可以，但实现起来会相当复杂（参考<a href="https://devopedia.org/quic">QUIC协议</a>）。TCP心跳机制看似简单，但从上面这个案例来看，稍有不慎，还是很容易发生错误的。</p><h3>TCP Keep-alive</h3><p>那么TCP自身的Keep-alive，究竟是怎样的一个存在呢？</p><p>其实，如果不做显式的配置，默认创建出来的TCP Socket是不启用Keep-alive的，也就是都不会发送心跳包。不过，大部分应用程序已经在代码里启用了Keep-alive，所以你平时不太会遇到连接失效的问题。比如我稍后要演示的一个含心跳包的抓包文件，抓取的就是Chrome浏览器的流量，里面就有很多心跳包，因为Chrome浏览器启用了TCP心跳保活机制。</p><p>要打开这个TCP Keep-alive特性，你需要使用setsockopt()系统调用，对已经创建的Socket进行配置，启用Keep-alive。具体的调用方法，你可以参考<strong>man setsockopt</strong>。</p><p>在Linux操作系统层级，也有三个跟Keep-alive有关的全局配置项。</p><ul>\n<li><strong>间隔时间</strong>：net.ipv4.tcp_keepalive_time，其值默认为7200（秒），也就是2个小时。</li>\n<li><strong>最大探测次数</strong>：net.ipv4.tcp_keepalive_probes，在探测无响应的情况下，可以发送的最多连续探测次数，其默认值为9（次）。</li>\n<li><strong>最长间隔</strong>：net.ipv4.tcp_keepalive_intvl，在探测无响应的情况下，连续探测之间的最长间隔，其值默认为75（秒）。</li>\n</ul><blockquote>\n<p>补充：你可以在Linux系统里面，执行<strong>man tcp</strong>，查看内核对TCP协议栈的详细文档。这里我摘录一下关于Keep-alive的部分：<br>\n&nbsp;</p>\n<ul>\n<li>tcp_keepalive_intvl (integer; default: 75; since Linux 2.4)<br>\n&nbsp;<br>\nThe number of seconds between TCP keep-alive probes.<br>\n&nbsp;</li>\n<li>tcp_keepalive_probes (integer; default: 9; since Linux 2.2)<br>\n&nbsp;<br>\nThe&nbsp; maximum number of TCP keep-alive probes to send before giving up and killing the connection if no response is obtained from the other end.<br>\n&nbsp;</li>\n<li>tcp_keepalive_time (integer; default: 7200; since Linux 2.2)<br>\n&nbsp;<br>\nThe number of seconds a connection needs to be idle before TCP&nbsp; begins&nbsp; sending&nbsp; out&nbsp; keep-alive probes.&nbsp; &nbsp;Keep-alives are sent only when the SO_KEEPALIVE socket option is enabled.&nbsp; The default value is 7200 seconds (2 hours).&nbsp; An idle connection is terminated after approximately an&nbsp; additional 11 minutes (9 probes an interval of 75 seconds apart) when keep-alive is enabled.</li>\n</ul>\n</blockquote><p>如果我们连接启用了Keep-alive，但没有设定自定义的数值，那么就会使用上面这些默认值，即：当连接闲置（没有数据交互）达到7200秒（2小时）时发送心跳包，每次心跳包超时时间为75秒，最多重试9次。</p><p>这样的话，对于一个已经失效的TCP连接，最大需要7200+75*9=7875秒（约等于2小时11分钟）才能探测到。</p><p>毫无疑问，这个时间是相当长的。不过结合时代背景，这个其实也可以理解：TCP Keep-alive被设计的时候是八十年代，当时因特网还很初级，所以设计者们并不想让心跳包占据太多的网络资源。从而，就有了这么一个感知时间很长的心跳机制。关于TCP Keep-alive的一些更多信息在<a href="https://datatracker.ietf.org/doc/html/rfc1122#page-101">RFC1122</a>里，你有兴趣的话可以去研究一下。</p><p>另外一个值得注意的地方，是Keep-alive报文本身的特点。在上面的案例中，这个特定的应用层代码设定的心跳包特征是这样的：</p><ul>\n<li>心跳探测包，载荷为1个字节，其值为01。</li>\n<li>心跳回复包，载荷也为1个字节，其值为41。</li>\n</ul><p>但是TCP本身提供的Keep-alive报文特征就非常不同了。首先，它的序列号就很奇特，是上一个报文的序列号<strong>减1</strong>，载荷<strong>为0</strong>。回复的报文也同样特别，确认号为收到的序列号<strong>加1</strong>。而且，无论是探测包还是回复包，其载荷长度都<strong>为0</strong>。</p><p>文字描述不是很容易理解，我给你看一个实际的例子。这是某一次我用Chrome浏览器访问网站时做的抓包：</p><p><img src="https://static001.geekbang.org/resource/image/ed/1a/ed092d46acc5594db8ae276f927b2f1a.png?wh=1920x453" alt="图片"></p><p>上图的红色底色的多个报文，就是Wireshark识别出来的TCP心跳包。我也把需要关注的信息用红色方框标注出来了。</p><p>25号报文是离心跳包最近的一个常规报文，Wireshark告诉我们：它的下一个序列号（图中的NextSeq）是1578。也就是说，如果下一个是常规报文，那么这个常规报文的序列号就是1578。然后看同是这个客户端发出的报文27，这就是一个心跳包，它的序列号却是1577（也就是<strong>1578-1</strong>），载荷为0（Len=0）。对端对这个心跳包做了回应（包号28），确认号为1578（<strong>1577+1</strong>），载荷也为0。</p><p>我们再来看一下示意图：</p><p><img src="https://static001.geekbang.org/resource/image/5b/6c/5b04d1eee64f3ea50185a479b55fc06c.jpg?wh=2000x803" alt=""></p><p>要是你了解TCP握手和挥手阶段的确认号的话，你对这个+1机制是不是感觉很熟悉？可见，TCP认为心跳包也是十分重要的，它跟握手和挥手一样，都属于控制报文，它的确认号机制也体现了这一特点。</p><p>有趣的是，RFC1122里并没有规定心跳探测包的载荷一定是0，它也可以是1。只是从我有限的抓包经验来看，心跳包都是载荷为0的，看来这是比较常见的实现方式。</p><h3>HTTP Keep-alive</h3><p>那么我们也经常听到的HTTP Keep-alive，又是一个什么东西呢？难道是应用层实现的心跳保活机制？意味着HTTP也有心跳包这种东西吗？</p><p>其实没有那么复杂。<strong>HTTP的Keep-alive，是用Connection这样一个HTTP header来实现的。</strong>你应该知道，HTTP报文的header形式是Key: value。比如常见的header有：</p><ul>\n<li>User-Agent: curl/7.68.0  （客户端发出）</li>\n<li>Host: www.baidu.com     （客户端发出）</li>\n<li>Content-Type: text/html （服务端发出）</li>\n<li>Server: bfe/1.0.8.18        （服务端发出）</li>\n</ul><p>而Keep-alive头部，就是这个形式：<strong>Connection: Keep-alive</strong>（注意这里有一个“-”符号）。</p><p>客户端和服务端都可以发送这个保活头部。表达的意思也跟外交人员的语言一样优雅专业：“我方真诚地希望，贵方能切实履行我们的协议，按照长连接待遇来处理本次连接，谢谢配合”。</p><p>你应该也知道，HTTP的版本有0.9、1.0、1.1、2.0。HTTP/0.9现在基本不再使用了，而HTTP/1.0占的流量比例也已经很低了（在公网上小于1%）。目前占主流的是HTTP/1.1和HTTP/2，而这两者都<strong>默认使用长连接</strong>。</p><p>那既然HTTP/1.1默认是长连接了，为什么还要有这个Connection头部呢？在我看来，这有两个原因。</p><ul>\n<li><strong>协议兼容性</strong></li>\n</ul><p>在HTTP/1.0版本中，默认用的是短连接。这导致了一个明显的问题：每次HTTP请求都需要创建一个新的TCP连接。随着因特网带宽和各类资源迅速增长，每次建立TCP连接的开销变成了主要矛盾。</p><p>为了克服这个不足，Connection: Keep-alive头部被扩充进了HTTP/1.0，用来显式地声明这应该是一次长连接。当然，从HTTP/1.1开始，长连接已经是默认设置了，但为了兼容1.0版本的HTTP请求，Connection这个头部在HTTP/1.1里得到了保留。</p><ul>\n<li><strong>关闭连接的现实需求</strong></li>\n</ul><p>即使在HTTP/1.1里，也并非所有的长连接都需要永久维持。有时候任意一方想要关闭这个连接，又想用一种“优雅”（graceful）的方式，那么就可以用 <strong>Connection: Close</strong> 这个头部来达到“通知对端关闭连接”的目的，体面而有效。另外，在HTTP/2里，连接关闭是通过另外的机制实现的，与Connection头部无关。</p><p>这个Connection头部看似简单，其实有时候也能起到很大的作用。在eBay，我们每年有大量的流量迁移的工作。其中，这个Connection头部就在迁移的<strong>平滑度和收敛速度</strong>上，起到了很关键的作用。</p><p>具体来说，我们在新老两个VIP之间迁移流量，用的是名称解析（基于DNS/GSLB）的返回值的比例，来控制真实的HTTP流量比例，这就可以逐步从新老比例为1:99，到50:50，最后到100:0也就是全部到新VIP。</p><p>但是这样会有这么个问题：因为流量基本都是HTTP/1.1（即默认是长连接），客户端依然坚持使用着老的VIP，造成DNS/GSLB的比值调整没有起到应有的效果，真正观察到的流量经常跟DNS/GSLB的设置值相差甚远，或者说<strong>有很强的滞后性</strong>。</p><p><img src="https://static001.geekbang.org/resource/image/e2/fe/e2864876efec290dc58ebf62393d76fe.jpg?wh=2000x659" alt=""></p><p>从上图中可以看到，客户端在查询GSLB（相当于智能DNS）的时候，拿到的新老IP的比例为2:2，那么访问量应该也是符合这个比例。但是因为长连接的存在，这些拥有长连接的客户端连问都不会去问GSLB，而是会继续往老的连接（也就是连着老VIP的连接）上发送流量，那么我们迁移流量的工作，就受到影响了。</p><p>为了解决这个问题，我们当然可以选择等待（比如几小时甚至几天）它自然收敛，但其实我们找到了更好的办法：在新老VIP上都添加了rewrite policy，使得每一定比例的HTTP响应里面，都带上Connection: Close这个头部。</p><p>客户端收到这个头部后，按照协议规定，它必须关闭这条长连接。在下一次HTTP请求的时候，客户端就会遵循“DNS解析-&gt;发起新连接-&gt;发送HTTP请求”这样的工作路径，于是新发起的连接数跟DNS解析数量基本对齐，也就达到了我们的目的。</p><p><img src="https://static001.geekbang.org/resource/image/b6/5c/b608c91ef62f7b824cb8b3fa5935f65c.jpg?wh=2000x660" alt=""></p><p>下图是在迁移尾声时候的流量趋势图。在这个阶段，老VIP已经从DNS/GSLB里禁用了。但原先不插入这种Connection: Close头部的话，总还是有很多请求在老的VIP上。在插入了Connection: Close头部后，老VIP上的流量几乎立刻停止，可谓立竿见影。</p><p><img src="https://static001.geekbang.org/resource/image/64/52/641ebb1db456baebfcda566b35d4ff52.png?wh=1920x392" alt="图片"></p><h2>小结</h2><p>这节课，我们通过一个奇特的案例，详细探究了一种应用层保活机制的Bug引发的报错。在这个排查过程中，<strong>Wireshark过滤器</strong>的使用，很大程度上帮助了这次排查。所以，这里我推荐你要熟悉以下这些过滤器，在你以后的网络排查工作中，应该能给到不少的帮助：</p><pre><code class="language-plain">tcp.len eq 长度\ntcp.flags.fin eq 1\ntcp.flags.reset eq 1\ntcp.payload eq 数据\n</code></pre><p>抓包分析中，面对Wireshark里千奇百怪的报文，有时候也会遇到不知道从何下手的窘况，那么你可以<strong>直接查看Expert Information</strong>，从那里寻找线索也不失为一个有效的办法。</p><p>另外，在原理部分，我还给你介绍了TCP层面的Keep-alive和HTTP层面的Keep-alive的联系和区别。应该说，这确实是两个容易令人困惑的概念，不仅名称一样，作用也接近。通过这次讲解，希望能帮助你彻底理解这两个概念。</p><p>最后，我再给你梳理提炼一下这节课的关键知识点。</p><p>首先，对于TCP Keep-alive，你需要掌握：</p><ul>\n<li>默认TCP连接并不启用Keep-alive，若要打开的话要<strong>显式地调用setsockopt()</strong>，来设置保活包的发送间隔、等待时间、重试个数等配置。在全局层面，Linux还默认有3个跟Keep-alive相关的内核配置项可以调整：tcp_Keepalive_time，tcp_Keepalive_probes，还有tcp_Keepalive_intvl。</li>\n<li>TCP心跳包的特点是，它的序列号是<strong>上一个包的序列号-1</strong>，而心跳回复包的确认号是这个序列号-1+1（即还是这个序列号）。</li>\n</ul><p>然后，对于HTTP Keep-alive的知识点，你需要理解：</p><ul>\n<li>HTTP/1.0默认是短连接，HTTP/1.1和2默认是长连接。</li>\n<li><strong>Connection: Keep-alive</strong> 在HTTP/1.0里，能起到维持长连接的作用，而在HTTP/1.1里面没有这个作用（因为默认就是长连接）。</li>\n<li><strong>Connection: Close</strong> 在HTTP/1.1里，可以起到优雅关闭连接的作用。这个头部在流量调度场景下也很有用，能明显加快基于DNS/GSLB的流量调整的收敛速度。</li>\n</ul><h2>思考题</h2><p>最后再给你留两道思考题：</p><ol>\n<li><code>tcp.payload eq abc</code>，这个过滤器可以搜索到精确匹配“abc”字符串的报文。那么，如果是模糊匹配，比如只要<strong>包含“abc”</strong>的报文我都想搜到，这个过滤器又该如何写呢?</li>\n<li>在工作中，你遇到过跟TCP Keep-alive相关的问题吗？你是怎么解决的呢？</li>\n</ol><p>欢迎你把答案分享到留言区，我们一起进步成长。</p><h2>附录</h2><p>抓包示例文件：<a href="https://gitee.com/steelvictor/network-analysis/tree/master/07">https://gitee.com/steelvictor/network-analysis/tree/master/07</a></p>',
        article_title: "07 | 保活机制：心跳包异常导致应用重启？",
      },
      {
        title: "08 | 分段：MTU引发的血案",
        id: 484667,
        content:
          '<p>你好，我是胜辉。</p><p>在<a href="https://time.geekbang.org/column/article/477510">第1讲</a>里，我给你介绍过TCP segment（TCP段）作为“部分”，是从“整体”里面切分出来的。这种切分机制在网络设计里面很常见，但同时也容易引起问题。更麻烦的是，这些概念因为看起来都很像，特别容易引起混淆。比如，你可能也听说过下面这些概念：</p><ul>\n<li>TCP分段（segmentation）</li>\n<li>IP分片（fragmentation）</li>\n<li>MTU（最大传输单元）</li>\n<li>MSS（最大分段大小）</li>\n<li>TSO（TCP分段卸载）</li>\n<li>……</li>\n</ul><p>所以这节课，我就通过一个案例，来帮助你彻底搞清楚这些概念的联系和区别，这样你以后遇到跟MTU、MSS、分片、分段等相关的问题的时候，就不会再茫然失措，也不会再张冠李戴了，而是能清晰地知道问题在哪里，并能针对性地搞定它。</p><h2>案例：重传失败导致应用压测报错</h2><p>我先来给你介绍下案例背景。</p><p>在公有云服务的时候，一个客户对我们公有云的软件负载均衡（LB）进行压力测试，结果遇到了大量报错。要知道，这是一个比较大的客户，这样的压测失败，意味着可能这个大客户要流失，所以我们打起十二分的精神，投入了排查工作。</p><p>首先，我们看一下这个客户的压测环境拓扑图：</p><p><img src="https://static001.geekbang.org/resource/image/9a/e0/9ae5998eb5922295ebbdab9147eff0e0.jpg?wh=1821x692" alt=""></p><p>这里的香港和北京，都是指客户在我们平台上租赁的云计算资源。从香港的客户端机器，发起对北京LB上的VIP的压力测试，也就是短时间内有成千上万的请求会发送过来，北京LB就分发这些请求到后端的那些同时在北京的服务器上。照理说，我们的云LB的性能十分出色，承受数十万的连接没有问题。不夸张地说，就算客户端垮了，LB都能正常工作。</p><!-- [[[read_end]]] --><p>但是在这次压测中，客户发现有大量的HTTP报错。我们看了具体情况，这个压测对每个请求的超时时间设置为1秒。也就是说，如果请求不能在1秒内得到返回，就会报错。而问题症状就是出现了大量这样的超时报错。</p><p>既然通过LB做压测有问题，客户就绕过LB，从香港客户端直接对北京服务器进行测试，结果发现是正常的，压测可以顺利完成。当然，因为单台服务器的能力比不上LB加多台服务器的配置，所以压测数据不会太高，但至少没有报错了。</p><p>那这样的话，客户是用不了我们的LB了？</p><p>我们还是用<strong>抓包分析</strong>来查看这个问题。显然，我们知道了两种场景，一种是正常的，一种是异常的。那么我们就在两种场景下分别做了抓包。</p><h3>绕过LB的压测</h3><p>我们来看一下绕过LB的话，请求是如何到服务器的。香港机房的客户端发起HTTP请求，通过专线进入广州机房，然后经由广州-北京专线，直达北京机房的服务器。</p><p><img src="https://static001.geekbang.org/resource/image/70/66/70b3b3925eyy21baf5b7875b625e9a66.jpg?wh=1824x690" alt=""></p><p>我们来看一下这种场景下的抓包文件：</p><blockquote>\n<p>抓包文件已经脱敏后上传至<a href="https://gitee.com/steelvictor/network-analysis/blob/master/08/bypassLB.pcap">gitee</a>。</p>\n</blockquote><p><img src="https://static001.geekbang.org/resource/image/fc/18/fc78697920a60ff6190808d4234f9218.jpg?wh=1724x1420" alt=""></p><p>考虑到压测的问题是关于HTTP的，我们需要查看一下HTTP事务的报文。这里只需要输入过滤器http就可以了：</p><p><img src="https://static001.geekbang.org/resource/image/cd/58/cd91d720450c3c3532ac0fa4434da758.jpg?wh=999x253" alt="图片"></p><p>我们选中任意一个这样的报文，然后Follow -&gt; TCP Stream，来看看整个过程：</p><p><img src="https://static001.geekbang.org/resource/image/a4/7c/a4f5858b2f5d81f9b64367fe9aef227c.jpg?wh=1094x319" alt="图片"></p><p>从这个正常的TCP流里，我们可以获取到以下信息：</p><ul>\n<li><strong>时延（往返时间）在36.8ms</strong>，因为SYN+ACK（2号报文）和ACK（9号报文）之间，就是隔了0.036753秒，即36.8ms。</li>\n<li><strong>HTTP处理时间也很快</strong>，从服务端收到HTTP请求（在10号报文），到服务端回复HTTP响应（14到16号报文），一共只花费了大约6ms。</li>\n<li>在收到HTTP响应后，客户端在25ms（324号报文）后，发起了TCP挥手。</li>\n</ul><p>看起来，服务器确实没有问题，整个TCP交互的过程也十分正常。那么，我们再来看一下“经过LB”的失败场景又是什么样的，然后在报文中“顺藤摸瓜”，进一步就能逼近根因了。</p><h3>经过LB的压测</h3><p>我们打开绕过LB的压测的抓包文件，在<a href="https://time.geekbang.org/column/article/482610">第7讲</a>中，我提到过利用Expert Information（专家信息）来展开排查的小技巧。那么这里也是如此。</p><blockquote>\n<p>抓包文件已经脱敏后上传至<a href="https://gitee.com/steelvictor/network-analysis/blob/master/08/viaLB.pcap">gitee</a>。</p>\n</blockquote><p><img src="https://static001.geekbang.org/resource/image/52/eb/52c0fb3e5bc9eb410c3fbee0d036abeb.jpg?wh=1792x1198" alt=""></p><p>可以看到，里面有标黄色的Warning级别的信息，有50个需要我们注意的RST报文：</p><p><img src="https://static001.geekbang.org/resource/image/40/16/4040e386a4642a2da06faa61f332cb16.jpg?wh=1816x306" alt=""></p><p>我们展开Warning Connection reset (RST)，选一个报文然后找到它的TCP流来看一下：</p><p><img src="https://static001.geekbang.org/resource/image/0b/e8/0b98ebd04e4ce8dc33c6e241d8eed7e8.jpg?wh=1890x1016" alt="图片"></p><p>比如上图中，我们选中575号报文，然后Follow -&gt; TCP Stream，就来到了这条TCP流：</p><p><img src="https://static001.geekbang.org/resource/image/2a/02/2ab7de2bfc73ffc0304596f63e15bb02.jpg?wh=1920x623" alt="图片"></p><p>不过这里，我先考考你，<strong>目前这个抓包是在客户端还是服务端抓取的呢？</strong></p><p>如果你对<a href="https://time.geekbang.org/column/article/478189">第2讲</a>还有印象，应该已经知道如何根据TTL来做这个判断了。没错，因为SYN+ACK报文的TTL是64，所以这个抓包就是在发送SYN+ACK的一端，也就是服务端做的。</p><p>接下来就是重点了。显然，这个TCP流里面，有两个重复确认（54和55号报文），还有两个重传（154和410号报文），在它们之后，就是客户端（源端口53362）发起了TCP挥手，最终以客户端发出的RST结束。</p><p>这里隐含着两个疑问，我们能回答这两个疑问了，那么问题的根因也就找到了。</p><h4>第一个疑问：为什么有重复确认（DupAck）？</h4><p>重复确认在TCP里面有很重要的价值，它的出现，一般意味着传输中出现了丢包、乱序等情况。我们来看看这两个重复确认报文的细节。</p><p><img src="https://static001.geekbang.org/resource/image/9c/bc/9cb992205f77253dac247a31f3e8cdbc.jpg?wh=996x74" alt="图片"></p><p>我们很容易发现，这两个DupAck报文的确认号是1。这意味着什么呢？你现在对TCP握手已经挺熟悉了，显然应该能想到，这个1的确认号，其实就是<strong>握手阶段完成时候的确认号</strong>。也就是说，客户端其实并没有收到握手后服务端发送的第一个数据报文，所以确认号“停留”在1。</p><p>那么，为什么是两个重复确认报文呢？我们把视线从2个DupAck报文往上挪，关注到整个TCP流的情况。</p><p><img src="https://static001.geekbang.org/resource/image/cd/9e/cd0ef78f432565b472a8fcf39595d69e.jpg?wh=786x294" alt=""></p><p>握手完成后，客户端就发送了POST请求，然后服务端先回复了一个ACK，确认收到了这个请求。之后有连续3个报文作为HTTP响应，返回给客户端。</p><p>按照TCP的机制，它可以收一个报文，就发送一个确认报文；也可以收多个报文，发送一个确认报文。反过来说，一端发送几次确认报文，就意味着它收到了至少同样数量的数据报文。</p><p>在当前的例子里，因为有2个DupAck报文，那么客户端一定至少收到了2个数据报文。是哪两个呢？一定是连续3个报文的第二和第三个，也就是1388字节报文的后面两个。因为如果是收到了1388字节那个，那确认号就一定不是1，而是1389（1388+1）了。</p><p>我们再把视线从2个DupAck往下挪，这里有2个TCP重传。</p><p>我们关注一下Time列，第一个重传是隔了大约200ms，第二次重传隔了大约472毫秒。这就是<strong>TCP的超时重传机制</strong>引发的行为。关于重传这个话题，后续课程里会有大量的展开，你可以期待一下。</p><p><img src="https://static001.geekbang.org/resource/image/0f/64/0f6c4a19bca1918d75099420de2c6164.jpg?wh=1798x152" alt="图片"></p><p>那么结合上面这些信息，我们也就理解了“通过LB压测失败”的整个过程，在TCP里面具体发生了什么。我还是用示意图来展示一下：</p><p><img src="https://static001.geekbang.org/resource/image/e2/97/e2973c5380261123626b5a3754626c97.jpg?wh=2000x1125" alt=""></p><p>不过你也许会问：“每次这样画一个示意图，好像比较麻烦啊？难道Wireshark就不能提供类似的功能吗？”</p><p>Wireshark主窗口里展示的报文，确实有点类似“一维”，也就是从上到下依次排列，在解读通信双方的具体行为时，如果能添加上另外一个“维度”，比如增加向左和向右的箭头，是不是可以让我们更容易理解呢？</p><p>其实，我们能想到的，Wireshark的聪明的开发者也想到了，Wireshark里确实有一个小工具可以起到这个作用，它就是<strong>Flow Graph</strong>。</p><p>你可以这样找到它：点开Statistics菜单，在下拉菜单中找到Flow Graph，点击它，就可以看到这个抓包文件的“二维图”了。不过，因为我们要查看的是过滤出来的TCP流，而Flow Graph只会展示抓包文件里所有的报文，所以，我们需要这么做：</p><ul>\n<li>先把过滤出来的报文，保存为一个新的抓包文件；</li>\n<li>然后打开那个新文件，再查看Flow Graph。</li>\n</ul><p>比如这次我就可以看到下面这个Flow Graph：</p><p><img src="https://static001.geekbang.org/resource/image/03/5f/03ab05db4962c3b2d81e0471a083bf5f.jpg?wh=1470x1206" alt=""></p><p>上图读起来，是不是感觉信息量要比主界面要多一些？特别是有了左右方向箭头，给我们大脑形成了“第二个维度”，报文的流向可以直接看出来，而不再去看端口或者IP去推导出流向了。</p><p>好了，“为什么会有重复确认”的问题，我们搞清楚了，它就是由于三个报文中，第一个报文没有到达客户端，而后两个到达的报文触发了客户端发送两次重复确认。我们接下来看更为关键的问题。</p><h4>第二个疑问：为什么重传没有成功？</h4><p>第一个报文就算暂时丢失，后续也有两次重传，为什么这些重传都没成功呢？既然我们同时有成功情况和不成功情况下的抓包文件，那我们直接比较，也许就能找到原因了。</p><p>让我们把两个文件中的类似的TCP流对比一下：</p><p><img src="https://static001.geekbang.org/resource/image/87/74/873c5cb8372a68fe88489d9383848a74.jpg?wh=1920x454" alt="图片"></p><p>你能发现其中的不同吗？这应该还是比较容易发现的，它就是：<strong>HTTP响应报文的大小</strong>。两次测试中，虽然HTTP响应报文都分成了3个TCP报文，但最大报文大小不同：左边是1348，右边是1388，相差有40字节。既然已经提到了报文大小，那你应该会联想到我们这节课的主题，MTU了吧？</p><p><strong>MTU，中文叫最大传输单元，也就是第三层的报文大小的上限。</strong>我们知道，网络路径中，小的报文相对容易传输，而大的报文遇到路径中某个MTU限制的可能会更大。那么在这里，假如这个问题真的是MTU限制导致的，显然，1388会比1348更容易遇到这个问题！</p><p><img src="https://static001.geekbang.org/resource/image/83/1b/83b1ac2130fecb5f46d719c30184671b.jpg?wh=2000x799" alt=""></p><p>就像上面示意图展示的那样，如果路径中有一个偏小的MTU环节，那么完全有可能导致1388字节的报文无法通过，而1348字节的报文就可以通过。</p><p>而且，因为MTU是一个静态设置，在同样的路径上，一旦某个尺寸的报文一次没通过，后续的这个尺寸的报文全都不能通过。这样的话，后续重传的两次1388字节的报文也都失败这个事实，也就可以解释了。</p><p>既然问题跟MTU有关，我们就检查了客户端到服务端之间的一整条链路，发现了一个之前没注意到的情况：除了广州到北京之间有一条隧道，在北京LB到服务端之间，还有一条额外的隧道。我们在<a href="https://time.geekbang.org/column/article/481042">第5讲</a>里学习过，<strong>隧道会增加报文的大小</strong>。而正是这条额外隧道，造成了报文被封装后，超过了路径最小MTU的大小！从下面的示意图中，我们能看到两次路径上的区别所在：</p><p><img src="https://static001.geekbang.org/resource/image/61/c3/61998617c7ae9cef8c2a7f66fdb634c3.jpg?wh=2000x474" alt=""></p><p>经过LB的时候，报文需要做2次封装（Tunnel 1和Tunnel 2），而绕过LB就只要做1次封装（只有Tunnel 1）。跟生活中的例子一样，同样体型的两个人，穿两件衣服的那个看起来比穿单衣的那个要显胖一点，也是理所当然。要显瘦，穿薄点。或者实在要穿两件，那只好自己锻炼瘦身（改小自己的MTU）了！</p><p>另外，由于Tunnel 1比Tunnel 2的封装更大一些，所以服务端选择了不同的传输尺寸，一个是1388，一个是1348。</p><h4>第三个疑问：为什么重传只有两次？</h4><p>一般我们印象里TCP重传会有很多次，为什么这个案例里只有两次呢？如果你能联想到<a href="https://time.geekbang.org/column/article/479163">第3讲</a>里提到的多个内核TCP配置参数，那可能你会想到<code>net.ipv4.tcp_retries2</code>这个参数。确实，通过这个参数的调整，是可以把重传次数改小，比如改为两次的。不过在这个案例里不太可能。一方面，除非有必要，没人会特地去改动这个值；另外一个原因，是因为我们找到了更合理的解释。</p><p>这个解释就是<strong>客户端超时</strong>，这一点其实我在前面介绍案例的时候就提到过。从TCP流来看，从发送POST请求开始到FIN结束，一共耗时正好在1秒左右。我们可以把Time列从显示时间差（delta time）改为显示绝对时间（absolute time），得到下图：</p><p><img src="https://static001.geekbang.org/resource/image/b7/f8/b714802046bfd8220d580155f621dbf8.jpg?wh=1480x638" alt="图片"></p><p>可见，客户端在0.72秒发出了POST请求，在1.72秒发出了TCP挥手（第一个FIN），相差正好1秒，更多的重传还来不及发生，连接就结束了。</p><p>这种“整数值”，一般是跟某种特定的（有意的）配置有关，而不是偶然。那么显然，这个案例里，客户端压测程序配置了1秒超时，目的也容易理解：这样可以保证即使一些请求没有得到回复，客户端还是可以快速释放资源，开启下一个测试请求。</p><h2>一般对策</h2><p>其实，我估计你在日常工作中也可能遇到过这种MTU引发的问题。那一般来说，我们的对策是把两端的MTU往下调整，使得报文发出的时候的尺寸就小于路径最小MTU，这样就可以规避掉这类问题了。</p><p>举个例子，在我的测试机上，执行<strong>ip addr</strong>命令，就可以查看到各个接口的MTU，比如下面的输出里，enp0s3口的当前MTU是1500：</p><pre><code class="language-plain">$ ip addr\n1: enp0s3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000\n&nbsp; &nbsp; link/ether 08:00:27:09:92:f9 brd ff:ff:ff:ff:ff:ff\n&nbsp; &nbsp; inet 192.168.2.29/24 brd 192.168.2.255 scope global dynamic enp0s3\n&nbsp; &nbsp; &nbsp; &nbsp;valid_lft 82555sec preferred_lft 82555sec\n&nbsp; &nbsp; inet6 fe80::a00:27ff:fe09:92f9/64 scope link\n&nbsp; &nbsp; &nbsp; &nbsp;valid_lft forever preferred_lft forever\n</code></pre><p>而假如，路径上有一个比1500更小的MTU，那为了适配这个状况，我们就需要调小MTU。这么做很简单，比如执行以下命令，就可以把MTU调整为1400字节：</p><pre><code class="language-plain">$ sudo ip link set enp0s3 mtu 1400\n</code></pre><h2>“暗箱操作”</h2><p>那除了这个方法，是不是就没有别的方法了呢？其实，我喜欢网络的一个重要原因是，它有很强的“可玩性”。<strong>只要我们有可能拆解网络报文，然后遵照协议规范做事情，那还是有不少灵活的操作空间的。</strong>你可能会好奇：这听起来有点像“灰色地带”一样，难道网络还能玩“潜规则”吗？</p><p>比如这次的案例，网络环节都是软件路由和软件网关，所以“暗箱操作”也成了可能，我们不需要修改两端MTU就能解决这个问题。是不是有点神奇？不过，你理解了TCP和MTU的关系，就会明白这是如何做到的了。</p><p>MTU本身是三层的概念，而在第四层的TCP层面，有个对应的概念叫<strong>MSS，Maximum Segment Size（最大分段尺寸），也就是单纯的TCP载荷的最大尺寸</strong>。MTU是三层报文的大小，在MTU的基础上刨去IP头部20字节和TCP头部20字节，就得到了最常见的MSS 1460字节。如果你之前对MTU和MSS还分不清楚的话，现在应该能搞清楚了。</p><p><img src="https://static001.geekbang.org/resource/image/13/64/1328c6cdda41681e7a199c06dcaa6964.jpg?wh=1655x495" alt=""></p><p>MSS在TCP里是怎么体现的呢？其实我在TCP握手那一讲里提到过 <a href="https://time.geekbang.org/column/article/479163">Window Scale</a>，你很容易能联想到，MSS其实也是在握手阶段完成“通知”的。在SYN报文里，客户端向服务端通报了自己的MSS。而在SYN+ACK里，服务端也做了类似的事情。这样，两端就知道了对端的MSS，在这条连接里发送报文的时候，双方发送的TCP载荷都不会超过对方声明的MSS。</p><p>当然，如果发送端本地网口的MTU值，比对方的MSS + IP header + TCP header更低，那么会以本地MTU为准，这一点也不难理解。这里借用一下 <a href="https://datatracker.ietf.org/doc/html/rfc879">RFC879</a> 里的公式：</p><blockquote>\n<p><strong>SndMaxSegSiz = MIN((MTU - sizeof(TCPHDR) - sizeof(IPHDR)), MSS)</strong></p>\n</blockquote><p>MTU是两端的静态配置，除非我们登录机器，否则改不了它们的MTU。但是，它们的TCP报文却是在网络上传送的，而我们做“暗箱操作”的机会在于：<strong>TCP本身不加密，这就使得它可以被改变！</strong>也就是我们可以在中间环节修改TCP报文，让其中的MSS变为我们想要的值，比如把它调小。</p><p>这里立功的又是一张熟悉的面孔：<strong>iptables</strong>。在中间环节（比如某个软件路由或者软件网关）上，在iptabes的FORWARD链这个位置，我们可以添加规则，修改报文的MSS值。比如在这个案例里，我们通过下面这条命令，把经过这个网络环节的TCP握手报文里的MSS，改为1400字节：</p><pre><code class="language-plain">iptables -A FORWARD -p tcp --tcp-flags SYN SYN -j TCPMSS --set-mss 1400\n</code></pre><p>它工作起来就是下图这样，是不是很巧妙？通过这种途中的修改，两端就以修改后的MSS来工作了，这样就避免了用原先过大的MSS引发的问题。我称之为“暗箱操作”，就是因为这是通信双方都不知道的一个操作，而正是这个操作不动声色地解决了问题。</p><p><img src="https://static001.geekbang.org/resource/image/yy/c5/yy96d3041c813231222a87fc3f77d5c5.jpg?wh=2000x1125" alt=""></p><h2>什么是TSO？</h2><p>前面说的都是操作系统会做TCP分段的情况。但是，这个工作其实还是有一些CPU的开销的，毕竟需要把应用层消息切分为多个分段，然后给它们组装TCP头部等。而为了提高性能，网卡厂商们提供了一个特性，就是让这个分段的工作<strong>从内核下沉到网卡上来完成</strong>，这个特性就是<strong>TCP Segmentation Offload</strong>。</p><p>这里的offload，如果仅仅翻译成“卸载”，可能还是有点晦涩。其实，它是off + load，那什么是load呢？就是CPU的开销。如果网卡硬件芯片完成了这部分计算任务，那么CPU就减轻负担了，这就是offload一词的真正含义。</p><p>TSO启用后，发送出去的报文可能会超过MSS。同样的，在接收报文的方向，我们也可以启用GRO（Generic Receive Offload）。比如下图中，TCP载荷就有2800字节，这并不是说这些报文真的是以2800字节这个尺寸从网络上传输过来的，而是由于接收端启用了GRO，由接收端的网卡负责把几个小报文“拼接”成了2800字节。</p><p><img src="https://static001.geekbang.org/resource/image/d4/67/d45a675de246069fed22867335a54567.jpg?wh=1834x344" alt="图片"></p><p>所以，如果以后你在Wireshark里看到这种超过1460字节的TCP段长度，不要觉得奇怪了，这只是因为你启用了TSO（发送方向），或者是GRO（接收方向），而不是TCP报文真的就有这么大！</p><p>想要确认你的网卡是否启用了这些特性，可以用ethtool命令，比如下面这样：</p><pre><code class="language-plain">$ ethtool -k enp0s3 | grep offload\ntcp-segmentation-offload: on\ngeneric-segmentation-offload: on\ngeneric-receive-offload: on\nlarge-receive-offload: off [fixed]\nrx-vlan-offload: on\ntx-vlan-offload: on [fixed]\nl2-fwd-offload: off [fixed]\nhw-tc-offload: off [fixed]\nesp-hw-offload: off [fixed]\nesp-tx-csum-hw-offload: off [fixed]\nrx-udp_tunnel-port-offload: off [fixed]\ntls-hw-tx-offload: off [fixed]\ntls-hw-rx-offload: off [fixed]\n</code></pre><p>当然，在上面的输出中，你也能看到有好几种别的offload。如果你感兴趣，可以自己搜索研究下，这里就不展开了。</p><p>对了，要想启用或者关闭TSO/GRO，也是用ethtool命令，比如这样：</p><pre><code class="language-plain">$ sudo ethtool -K enp0s3 tso off\n$ sudo ethtool -k enp0s3 | grep offload\ntcp-segmentation-offload: off\n</code></pre><h2>IP分片</h2><p>IP层也有跟TCP分段类似的机制，它就是IP分片。很多人搞不清IP分片和TCP分段的区别，甚至经常混为一谈。事实上，它们是两个在不同层面的分包机制，互不影响。</p><p><strong>在TCP这一层，分段的对象是应用层发给TCP的消息体</strong>（message）。比如应用给TCP协议栈发送了3000字节的消息，那么TCP发现这个消息超过了MSS（常见值为1460），就必须要进行分段，比如可能分成1460，1460，80这三个TCP段。</p><p><img src="https://static001.geekbang.org/resource/image/e6/b4/e6b8bb5cd1721d8b0be961a566fa6db4.jpg?wh=1787x516" alt=""></p><p><strong>在IP这一层，分片的对象是IP包的载荷</strong>，它可以是TCP报文，也可以是UDP报文，还可以是IP层自己的报文比如ICMP。</p><p>为了帮助你理解segmentation和fragmentation的区别，我现在假设一个“奇葩”的场景，也就是MSS为1460字节，而MTU却只有1000字节，那么segmentation和fragmentation将按照如下示意图来工作：</p><p><img src="https://static001.geekbang.org/resource/image/ec/cf/ec7a1c58e680061cfb00e71922c135cf.jpg?wh=1830x1013" alt=""></p><blockquote>\n<p>补充：为了方便讨论，我们假设TCP头部就是没有Option扩展的20字节。但实际场景里，很可能MSS小于1460字节，而TCP头部也超过20字节。</p>\n</blockquote><p>当然，实际的操作系统不太会做这种自我矛盾的傻事，这是因为它自身会解决好MSS跟MTU的关系，比如一般来说，MSS会自动调整为MTU减去40字节。但是我们如果把视野扩大到局域网，也就是主机再加上网络设备，那么就有可能发生这样的情况：1460字节的TCP分段由这台主机完成，1000字节的IP分片由路径中某台MTU为1000的网络设备完成。</p><p>这里其实也有个隐含的条件，就是主机发出的1500字节的报文，不能设置 <strong>DF（Don’t Fragment）位</strong>，否则它既超过了1000这个路径最小MTU，又不允许分片，那么网络设备只能把它丢弃。</p><p>在Wireshark里，我们可以清楚地看到IP报文的这几个标志位：</p><p><img src="https://static001.geekbang.org/resource/image/ec/77/ec65ebe4f5cf7aa476286aa300904a77.jpg?wh=1268x464" alt="图片"></p><p>现在我们假设主机发出的报文是不带DF位的，那么在这种情况下，这台网络设备会把它切分为一个1000（也就是960+20+20）字节的报文和一个520（也就是500+20）字节的报文。1000字节的IP报文的 <strong>MF位（More Fragment）</strong>会设置为1，表示后续还有更多分片，而520字节的IP报文的MF字段为0。</p><p>这样的话，接收端收到第一个IP报文时发现MF是1，就会等第二个IP报文到达，又因为第二个报文的MF是0，那么结合第二个报文的fragment offset信息（这个报文在分片流中的位置），就把这两个报文重组为一个新的完整的IP报文，然后进入正常处理流程，也就是上报给TCP。</p><p>不过在现实场景里，<strong>IP分片是需要尽量避免的</strong>，原因有很多，主要是因为互联网是一个松散的架构，这就导致路径中的各个环节未必会完全遵照所有的约定。比如你发出了大于PMTU的报文，寄希望于MTU较小的那个网络环节为你做分片，但事实上它可能不做分片，而是直接丢弃，比如下面两种情况：</p><ul>\n<li>它考虑到开销等问题，未必做分片，所以直接丢弃。</li>\n<li>如果你的报文有DF标志位，那么也是直接丢弃。</li>\n</ul><p>即使它帮你做了分片，但因为开销比较大，增加的时延对性能也是一个不利因素。</p><p>另外一个原因是，分片后，TCP报文头部只在第一个IP分片中，后续分片不带TCP头部，那么防火墙就不知道后面这几个报文用的传输层协议是什么，可能判断为有害报文而丢弃。</p><p>总之，为了避免这些麻烦，我们还是不要开启IP分片功能。事实上，Linux默认的配置就是，发出的IP报文都设置了DF位，就是明确告诉每个三层设备：“不要对我的报文做分片，如果超出了你的MTU，那就直接丢弃，好过你慢腾腾地做分片，反而降低了网络性能”。</p><h2>小结</h2><p>这节课，我们通过拆解一个典型的MTU引发的传输问题，学习了MTU和MSS、分段和分片、各种卸载（offload）机制等概念。这里，我帮你再提炼几个要点：</p><ul>\n<li>在案例分析的过程中，我们解读了Wireshark里的信息，特别是两次DupAck和两次重传，推导出了问题的根因。这里，你需要了解 <strong>200ms超时重传这个知识点</strong>，这在平时排查重传问题时也经常用到。</li>\n<li>借助 <strong>Wireshark的Flow graph</strong>，我们可以更加清晰地看到两端报文的流动过程，这对我们推导问题提供了便利。</li>\n<li>如果能稳定重现成功和失败这两种不同场景，那就对我们排查工作提供了极大的便利。我们通过<strong>对比成功和失败两种场景下的不同的抓包文件</strong>，能比较快地定位到问题根因。</li>\n<li>如果排查中遇到<strong>有“整数值”出现</strong>，可以重点查一下，一般这跟人为的设置有关系，也有可能就是根因，或者与根因有关。</li>\n<li>如果你对网络中间环节（包括LB、网关、防火墙等）有权限，又不想改动两端机器的MTU，那么可以选择在中间环节实施“暗箱操作”，也就是<strong>用iptables规则改动双方的MSS</strong>，从而间接地达到“双方不发送超过MTU的报文”的目的。</li>\n<li>我们也学习了如何<strong>用ethtool工具查看offload相关特性</strong>，包括TSO、LRO、GRO等等。同样通过ethtool，我们还可以对这些特性进行启用或者禁用，这为我们的排查和调优工作提供了更大的余地。</li>\n</ul><h2>思考题</h2><p>最后再给你留两道思考题：</p><ul>\n<li>在LB或者网关上修改MSS，虽然可以减小MSS，从而达到让通信成功这个目的，但是这个方案有没有什么劣势或者不足，也同样需要我们认真考量呢？可以从运维和可用性的角度来思考。</li>\n<li>你有没有遇到过MTU引发的问题呢？欢迎你分享到留言区，一同交流。</li>\n</ul><h2>附录</h2><p>抓包示例文件：<a href="https://gitee.com/steelvictor/network-analysis/tree/master/08">https://gitee.com/steelvictor/network-analysis/tree/master/08</a></p>',
        article_title: "08 | 分段：MTU引发的血案",
      },
      {
        title: "09 | 长肥管道：为何文件传输速度这么慢？",
        id: 484923,
        content:
          '<p>你好，我是胜辉。</p><p>在上节课里，我们通过一个MTU引发问题的案例，学习了MTU相关的知识点，了解了MTU、MSS、TCP分段、IP分片这些概念之间的区别和联系。我们也学习了用iptables修改MSS来达到规避MTU问题的目的，是不是觉得网络虽然学起来不简单，但学会了好像也挺好玩的？</p><p>那这个感觉就对了，学习本来就是一件有意思的事情，也是很有收获感的事情。学得更多，收益更多，反过来就能推动自己继续学得更多，形成良性循环。</p><p>当然，上节课我们主要说的是如果传输失败会怎么样，而从这节课开始，我们会讨论讨论传输起来以后的效率问题，比如传输速度。</p><p><strong>传输速度</strong>是网络性能中非常重要的一部分内容，因为它直接影响了我们享受到的网络服务的品质。比如现在移动端网络越来越快，所以无论传输大文件还是实时的视频通话，都得到了很大的发展。</p><p>数据中心更是如此。服务器的网卡早就从多年前的千兆网卡（1Gbps）升级到万兆网卡（10Gbps）。而负载均衡、防火墙等网络设备，更是从10Gbps升级到40Gbps，乃至100Gbps。</p><p>在这个大背景下，照理说在数据中心之间传个文件早已不在话下。但是我们偏偏遇到了这样一个奇葩的问题：<strong>数据中心间传输文件的速度居然只有400多KB/s</strong>，是不是很奇怪？你有没有遇到过类似的传输速度方面的问题，你又是如何通过网络分析，找到根因的呢？</p><!-- [[[read_end]]] --><p>所以这节课，我们就来研究分析下这个文件传输速度的案例，一起来深入探讨下如何提升TCP传输速度的话题。</p><p>这样，当你日后也遇到传输速度方面的问题时，就可以运用这节课学到的知识，去真正解决问题。甚至，即使看起来没有速度问题，你也能主动发现提高网络性能的机会，带来额外的惊喜，同时也把你的网络维护的能力提升到一个更高的层次上来。</p><h2>案例背景</h2><p>这是2017年我处理的eBay内部的案例。当时我们有一个需求是把一个Firmware安装文件从美国数据中心拷贝到欧洲的一个POP点，这个点在荷兰的阿姆斯特丹。文件不算大，95MB。其实这是一项常规任务，因为我们每年都会做一到两次的Firmware升级，经常在美国几个数据中心之间拷贝安装文件，一般这样一个100MB左右的文件约二十多秒就能完成。</p><p>这一次，我们需要把全球POP节点的设备也进行升级。对我们来说，确实也是第一次做从北美到欧洲这么长距离的文件传输，结果发现传输速度慢了一个数量级。我们的POP点有很多个，按顺序做升级的话，每次文件传输都增加3分钟，那么20个POP就增加60分钟，显著拖累了整个的升级速度。</p><p>当然，地理上看，从美国加州到荷兰阿姆斯特丹确实极为遥远，距离数千公里，坐飞机也要14个多小时：</p><p><img src="https://static001.geekbang.org/resource/image/3d/a2/3d731a2563eb2c38aa91484ab92829a2.png?wh=992x453" alt="图片"></p><p>然后我们也来看一下美国不同的数据中心之间拷贝的速度：</p><p><img src="https://static001.geekbang.org/resource/image/c0/dc/c0713e2fb303be50437c51d732b458dc.jpg?wh=1863x349" alt=""></p><p>再看一下从美国拷贝到荷兰的速度：</p><p><img src="https://static001.geekbang.org/resource/image/f1/4f/f118bc63c49feae0337abdbcdfaa454f.jpg?wh=1871x307" alt=""></p><p>当你看到两种场景下传输同一个文件的速度有这么大的差别，第一感觉是什么呢？你觉得会是什么因素导致了速度变这么慢？会不会想到带宽？</p><p>确实，当时我们也才开始维护欧洲POP点，对美国和欧洲之间的底层传输网络情况并不熟悉，所以也怀疑：<strong>会不会是带宽本身就不大呢？</strong>我们找了负责骨干网的同事，了解到这个带宽有10Gbps，远超这个传输速度。</p><p>然后，你会想到什么呢？对了，我补充一下：这个传输是scp拷贝，所以是基于TCP的。你自然明白，TCP的传输速度也受丢包的影响。如果丢包严重，传输速度肯定跑不起来。我们觉得这个也是有可能的，就去抓包检查。结果发现丢包率并不高，不高的丢包率却导致如此低的速度，这个可能性很小。</p><p>更为重要的是，<strong>既然我们要排查传输速度的问题，那么首先要看的，是不是网络I/O的状况呢？</strong></p><blockquote>\n<p>补充：抓包示例文件已经传输至<a href="https://gitee.com/steelvictor/network-analysis/tree/master/09">Gitee</a>，建议用Wireshark打开示例文件，结合文稿学习，效果更好。</p>\n</blockquote><p>在Wireshark的众多统计工具中，有两个工具就是I/O分析而生的，一个是 <strong>I/O Graph</strong>，另一个是 <strong>TCP Stream Graph</strong>。我们先看I/O Graph。</p><h2>I/O Graph</h2><p>这个工具在Statistics下拉菜单中：</p><p><img src="https://static001.geekbang.org/resource/image/48/e2/481e6a312e60b8bb7455340719de37e2.png?wh=252x546" alt="图片"></p><p>点击I/O Graph后，Wireshark会弹出一个趋势图，其X轴是时间，Y轴是性能指标：</p><p><img src="https://static001.geekbang.org/resource/image/3b/e2/3b012aec31994547cfa0ec86fb01bee2.png?wh=845x808" alt="图片"></p><p>注意，这时的图是不准确的，所以我们需要做一些调整。既然我们关注传输速度，所以就选择All Bytes这个指标项（也是默认选中的那项）作为Y轴，然后修改它的计量单位。很可能你的Wireshark默认选中的是AVG(Y Field)，而这并不是我们要关注的<strong>字节数</strong>。我们可以双击AVG(Y Field)进入编辑模式，把它改为<strong>Bytes</strong>：</p><p><img src="https://static001.geekbang.org/resource/image/e3/c5/e355081c0c2f0f3a34bd9885902692c5.png?wh=810x225" alt="图片"></p><p>很棒！这时我们就能清晰地看到，Wireshark帮助我们计算出来的分时的速度趋势柱状图了，差不多速度在480KB/s上下：</p><p><img src="https://static001.geekbang.org/resource/image/eb/74/eb3249ec5973ffa8b92da250965db774.jpg?wh=1690x1326" alt="图片"></p><p>你可能也注意到了，在X时间轴上看，一开始几秒速度比较低，第7秒才达到400KB/s以上。为什么一开始速度这么低呢？其实，这正是TCP慢启动的一个缩影：<strong>初始阶段，速度是特别低的，但是会很快爬高</strong>。</p><p>整体来看，这个传输速度还是很“稳定”的。对，一直很“稳定”得低。</p><h2>TCP Stream Graphs</h2><p>如果说I/O Graph展示的速度值很容易理解，那么TCP Stream Graphs展示的信息就需要一点TCP的知识来辅助理解了。</p><p>还是到Statics下拉菜单，选择TCP Stream Graphs，在子菜单中选择Time sequence (Stevens)。</p><blockquote>\n<p>补充：Stevens这个名字你应该是熟悉的，他就是鼎鼎大名的TCP/IP三卷和Unix网络编程两卷等名著的作者，Richard Stevens。他把网络协议的圣火带到人间，却又早早离开了人间。这个工具以他的名字命名，也代表了大家对他的深深的怀念。</p>\n</blockquote><p><img src="https://static001.geekbang.org/resource/image/53/d2/53cff83679be9c7399136d76a2e8d2d2.jpg?wh=663x543" alt="图片"></p><p>然后就能看到时间为X轴、TCP序列号为Y轴的图了。你应该知道，序列号其实就等于字节数，那么显然，这里这条线的斜率也就是传输速率了。</p><p><img src="https://static001.geekbang.org/resource/image/c4/9e/c4e9b272905c8478c282d66026ae399e.jpg?wh=927x793" alt="图片"></p><p>我们可以自己计算这个斜率（速率） 是多少。比如可以计算10秒和40秒两处的序列号的差距，再除以(40-10)秒，就是速率了。10秒处的序列号是2800092，40秒处是16480292，那么速率就是(16480292-2800092)/(40-10)=456KB/s。</p><p><img src="https://static001.geekbang.org/resource/image/7a/eb/7af63950d3bdb4f7b0cf7d5d3d08f7eb.jpg?wh=663x404" alt="图片"></p><p><img src="https://static001.geekbang.org/resource/image/6a/c1/6a48f093cayycf2c2db93946833d22c1.jpg?wh=810x577" alt="图片"></p><p>你可能发现了，<strong>两个Graph算出来的速度怎么有点差异？</strong>一个是480KB/s左右，一个是456KB/s，相差有5%左右。</p><p>其实，这是正常的。因为I/O Graph统计的Bytes是二层帧的大小，而TCP Stream Graphs关注的是四层TCP段的大小。后者比前者少了二层到四层的头部。严格来说，TCP Stream Graphs的斜率，只是TCP payload的速率；而I/O Graph展示的，才是我们一般谈论的传输速度。当然，在定性的讨论中，这点差异是可以忽略的。</p><h2>理解与传输相关的知识点</h2><p>其实，上面用了两种方式查证速度，也无非是印证了我们在scp命令输出里，看到的速度值本身是准确的。但目前为止，我们还没有找到造成这个速度的原因。这时候，我们稍微调整一下问题描述：<strong>假如带宽足够，网络也稳定，那又是什么决定了TCP传输的速度？</strong></p><p>回想一个小学时就学过的公式：<strong>速度=距离/时间</strong>。</p><p>看上去极为简单的公式，似乎在这里没什么帮助。不过奥妙的地方在于：什么是距离，什么又是时间？搞清楚了这两个问题的答案，就搞清楚了这次传输分析的精髓。不过我这里先卖个关子，先继续把接下来的知识点讲清楚，到后面你也许自己就能悟到答案了。</p><p>假设你在开车，开出去100公里，耗时1小时。这是因为你眼观四路、耳听八方，在确保安全的前提下用比较快的速度行驶。如果你蒙上眼睛，那是1米都不敢开的，是不是？</p><p>对于网络传输来说，报文发送出去后，发送端本身就“失去视力”了，也就是看不到报文在错综复杂的网络中行走时，到底遇到了什么样的“路况”。那它是继续发送更多数据好呢，还是先等对方确认这部分数据，然后再发送下一份数据好呢？如果确认了，那下一份数据又该发多少呢？一系列问题，需要我们回答。</p><p>不过还好，睿智的TCP的设计者们早就考虑到了这些问题。这里呢，有几个相关知识点你需要了解，且听我细细道来。</p><p>首先，一个报文被视为成功发送，基于以下的路径：</p><p><strong>（开始）发送端&gt;&gt;数据报文&gt;&gt;接收端&gt;&gt; ACK报文&gt;&gt;发送端（结束）</strong></p><p><img src="https://static001.geekbang.org/resource/image/19/1e/19d0e832824c49c76e86393c7e01461e.jpg?wh=1282x460" alt=""></p><p>这样一来一回的时间，就是报文被成功传送的耗时。你可能会有疑问：数据报文到了接收端不就是已经完成传输了吗？其实，这个时候发送端还不知道接收端是否已经完成接收了，因为还没收到确认。</p><p>从<strong>操作系统的角度</strong>来看：“完成”的标志，是这部分数据可以从发送缓存中删除；从缓存中删除这部分数据的前提，就是收到ACK报文。即“确认多少，我就清除多少”。像下图这样：</p><p><img src="https://static001.geekbang.org/resource/image/f9/a1/f91abcdffbe68392e84ea36271126ba1.jpg?wh=2000x1125" alt=""></p><p>来回的时间，在英文里叫 <strong>Round Trip Time</strong>（RTT），即往返时间，也叫<strong>时延</strong>。这是你需要知道的第一个名词。</p><p>你有没有发现，报文大部分时候就像飞机在空中航行，跟两端的哪一端都碰不着，除了起飞和降落。在“空中”的时间，就取决于RTT。RTT越长，报文在“空中”的时间就越长。这个时候，这些报文就有了一个新的身份，叫做“在途数据”，它的大小，叫做<strong>在途字节数</strong>。英文是 <strong>Bytes in flight</strong>。这是你需要知道的第二个名词。</p><p>事实上，Wireshark也很周到地帮我们想到了这一点，在TCP详情页的SEQ/ACK analysis部分就有 <strong>Bytes in flight</strong> 信息。比如下面这个：</p><p><img src="https://static001.geekbang.org/resource/image/ba/09/ba46c99c183c40eaa20c77b4d47ace09.png?wh=426x76" alt="图片"></p><p>带宽，类似道路的车道数，你应该见过节假日里高速公路上停满私家车的图片。可见，车道越多，高速路越长，能容纳的车就越多。在网络世界里，带宽很大、RTT很长的网络，被冠以一个特定的名词，叫做<strong>长肥网络</strong>，英文是 <strong>Long Fat Network</strong>。在长肥网络中的TCP连接，叫做<strong>长肥管道</strong>，英文是 <strong>Long Fat Pipeline</strong>。这是你需要知道的第三个名词。</p><p>道路上最多可以容纳多少辆车呢？显然，是车道数×道路长度。那么类似地，带宽跟往返时间（RTT）相乘，就是在空中飞行的报文的最大数量，即<strong>带宽时延积</strong>。在英文里叫 <strong>Bandwidth Delay Product</strong>，缩写是BDP（Delay就是RTT）。不用我说，你也明白带宽时延积是你需要知道的第四个名词。</p><p>其实，以上这些名词都是从英文翻译过来的，所以读起来感觉不太像本土词汇，特别是“长肥管道”。你别把“长肥”记成“肥肠”就行了。</p><p>我们用图对比一下，就能明显看出，所谓的长肥管道，就是带宽时延积更大的网络，蓝色部分表示的就是带宽时延积：</p><p><img src="https://static001.geekbang.org/resource/image/76/2c/76a904603f912942afaaa5074b114c2c.jpg?wh=1768x1040" alt=""></p><p>你可能还会有疑问，在途数据不是应该RTT的一半（即单程时间）再乘以带宽吗？从飞机的比喻来看，也许是的。但是你要考虑下图这种情况。如果回程时间只是用来传输ACK，没有被用来传输实际的数据，效率就打了对折了。就像图的右侧表示的，只有一半的时间在真正传输数据，另外一半时间没有在发送数据：</p><p><img src="https://static001.geekbang.org/resource/image/6d/03/6d5fa4b8b550d11df2f5a20c830c9f03.jpg?wh=2000x1125" alt=""></p><p>事实上，发送端并不知道什么时候报文到了接收端，它唯一知道的是什么时候自己收到了ACK报文，所以它会把这些时间全部用来发送报文。我把上图中原先不传数据的时间段2和4也改为发送数据，于是形成了下面这张图：</p><p><img src="https://static001.geekbang.org/resource/image/e2/4e/e214329cb79fbea930895e2a9c20484e.jpg?wh=1654x1125" alt=""></p><p>当然，<strong>实际情况是远比示意图复杂的</strong>。比如，在途字节数一定等于<code>带宽×RTT</code>吗？要知道，接收端也不傻，也是可以源源不断地对数据进行确认的，这样也就形成了更加复杂的关系：发送端在不断发出在途数据，接收端也在不断回复ACK。这时候的在途数据，可以用下面的公式来获得：</p><blockquote>\n<p><strong>inflight_data = (latest_sequence_sent + latest_len_sent) - max_ack_received</strong></p>\n</blockquote><p>最后还有个重要的概念是“窗口”。有人开玩笑说：讨论TCP握手的时候，大家侃侃而谈；一旦说到窗口，就都沉默了，气氛尴尬。继续往下学习这一部分，你会成为那个打破沉默的人。<code>：）</code></p><p>其实，下次别人跟你切磋“窗口技术”的时候，可以先下手为强：“你说的是到底哪个窗口？”因为事实上，TCP有3个窗口：接收窗口、拥塞窗口，还有发送窗口。</p><ul>\n<li><strong>接收窗口</strong>：它代表的是接收端当前最多能接收的字节数。通过TCP报文头部的Window字段，通信双方能互相了解到对方的接收窗口。</li>\n<li><strong>拥塞窗口</strong>：发送端根据实际传输的拥塞情况计算出来的可发送字节数，但不公开在报文中。各自暗地里各维护各的，互相不知道，也不需要知道。</li>\n<li><strong>发送窗口</strong>：对方的接收窗口和自身的拥塞窗口两者中，值较小者。实际发送的在途字节数不会大于这个值。</li>\n</ul><p>接收窗口是明的，在抓包文件里就能看到；拥塞窗口和发送窗口是暗的，抓包文件里没有。我们看这张图就明白了：</p><p><img src="https://static001.geekbang.org/resource/image/68/75/68bdddb151cbd999ca3b819bfcb13375.jpg?wh=2000x526" alt=""></p><h2>解题</h2><p>好了，终于把传输相关的关键知识点介绍完了，是不是感觉信息量有点大，甚至还没完全理解？别急，下面我们把这些知识点应用到这个案例中，这些看似干涩的知识点就会逐步丰润起来。</p><h3>时延</h3><p>我们首先收集时延信息。用美国数据中心的发送端去Ping阿姆斯特丹接收端的IP，我们发现<strong>时延在134ms</strong> 左右。用Ping的原因是，它的ICMP报文比较轻量，不会引起双方很多的额外处理时间，所以适合用来获取相对纯粹的网络往返时间。</p><p>在TCP通信中，因为协议栈本身也需要做拆解包、缓冲、Socket处理等工作，所以TCP层面的RTT会比IP层面的RTT略长一点。好在，Wireshark也提供了RTT信息。我们选取一个报文，在TCP详情的SEQ/ACK analysis部分，就有iRTT信息。这里是141ms，比Ping探测到的134ms多了一点，这是因为TCP协议栈本身处理也有一些延迟：</p><p><img src="https://static001.geekbang.org/resource/image/8f/e7/8f68052f3b9e753b9ed7749c5d4a2fe7.jpg?wh=501x411" alt="图片"></p><p>iRTT是intial RTT的缩写，Wireshark就是从TCP握手阶段的报文里计算出这个值的。对于客户端来说，就是发出SYN和收到SYN+ACK的间隔。对于服务端，就是发出SYN+ACK和收到ACK的间隔。</p><h3>带宽时延积</h3><p>接下来我们算算带宽时延积是多少。时延是134ms，带宽是10Gbps，那么带宽时延积就是0.134×10Gb。转换为Byte需要再除以8，得到约168MB。这个数值的意思是，假设这条链路完全被这次的文件传输连接所占满，那么最多可以有168MB的在途数据。</p><p>我们这个Firmware文件也才95MB，如果按这个速度，那一个来回就传完了，也就是只需要134ms！当然，TCP有慢启动机制，不可能一开始就把一百多MB这么大的数字作为初始拥塞窗口，所以也不会真的一个来回就传完了。</p><p>到这一步，我们已经明白，<strong>带宽时延积并不是限制速度的因素</strong>。那么一定是别的东西，在暗地里约束着这次传输。这个东西是什么呢？我们快接近真相了。</p><h3>发送窗口</h3><p>在Wireshark里查看接收窗口的数值也是很直观的。任意一个TCP报文的头部都有Window字段，长度为2个字节，所以最大值为2的16次方，即64KB。在<a href="https://time.geekbang.org/column/article/479163">第3讲</a>我们讨论TCP握手相关案例时，提到过<strong>Window Scale</strong>。它是在<a href="https://datatracker.ietf.org/doc/html/rfc1323">RFC1323</a>中引进的，使得Window值最大能达到2的30次方，即1GB。</p><p>我们看一下当时阿姆斯特丹POP点作为接收端，它宣告给美国数据中心的TCP接收窗口值是多少。为了方便查看，你可以在Wireshark里这样做：</p><ul>\n<li><strong>添加Calculated window size为自定义列</strong>。操作方法是任选一个TCP报文，在TCP详情那里找到[Calculated window size:xxxx]，右单击，在弹出菜单中点击Apply as Column。然后主窗口里就多了这样一列Calculated window size。</li>\n<li><strong>在过滤器输入框输入tcp.srcport eq 22</strong>，这样就把POP点（监听在22端口）发出的报文过滤出来了，更加方便我们查看。</li>\n</ul><p>我得到的界面是这样的：</p><p><img src="https://static001.geekbang.org/resource/image/15/ee/154e401f906f5d43b86897c43e66c3ee.jpg?wh=1830x644" alt="图片"></p><p>左上角是过滤器，右侧第二列就是刚刚新添加的Calculated window size。显然，能从图中看出，传输起始阶段有不少重传。不过没关系，让我们集中注意力到接收窗口值上。</p><p>由图可见，这个阶段的接收窗口是64KB上下浮动，值偏小。照理来说，传输稳定阶段，接收窗口会大很多，也就应该会比64KB大不少吧？我们往下翻，滚动到整体报文的中间位置，看看此时接收窗口值是多少：</p><p><img src="https://static001.geekbang.org/resource/image/e0/23/e02fb7d982b909055d476c6c8e6a3223.jpg?wh=1920x633" alt="图片"></p><p>有点意外，不仅没有上升，反而偶尔还略微下降了，在54~64KB之间浮动。这是怎么回事？这跟速度慢是否有关系？</p><p>前面提到过，发送端实际的发送窗口是拥塞窗口和对方接收窗口这两者中的较小者。那究竟是多少呢？我们可以这么找到它：</p><ul>\n<li>选中传输中的一个报文，在TCP详情的[SEQ/ACK analysis]部分，找到[Bytes in flight]。</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/f9/80/f97f073b3f9197383c6ba5dfc842bf80.jpg?wh=1668x1172" alt="图片"></p><ul>\n<li>然后右单击，选中Apply as column。</li>\n</ul><p>主界面里有Bytes in flight这一列后，你就可以排序，下图中显示，这次传输的客户端在途字节数最大是65700，接近64KB，这就是发送窗口。</p><p><img src="https://static001.geekbang.org/resource/image/7e/50/7ef67069370fa0d67a46799308115f50.jpg?wh=1812x372" alt="图片"></p><p>64KB很小啊，这就相当于，在一个长肥网络中，存在着一个“瘦”很多的管道，完全没有把带宽利用起来。我觉得可以叫做“长瘦管道”。</p><p><img src="https://static001.geekbang.org/resource/image/3d/22/3d02b1dc03bdc5d93fa2c024b8591022.jpg?wh=1463x619" alt=""></p><p>这个长瘦管道的传输速度是多少呢？在这个案例里，发送窗口=接收窗口，那么在网络上跑的数据量最多就是64KB。那传输这64KB的耗时是多久呢？这不就是往返时间RTT吗？现在，你是否终于回想起前面我卖关子的那个小学公式呢？</p><blockquote>\n<p><strong>速度 = 距离/时间</strong></p>\n</blockquote><p>所以，这次的传输速度的上限就是window/RTT = 64KB/134ms = <strong>478KB/s</strong>！还记得前面用TCP Stream Graphs(Stevens)斜线图得到的速率吗？那个是456KB/s。可见，实际值只比上限值略低，说明虽然窗口很小，但传输过程本身还是比较充分地利用了这个有限的窗口。</p><p>整个案件得以真相大白：限制速度的最大因素，既不是带宽，也不是丢包，而是<strong>窗口</strong>，确切地说就是<strong>接收端（POP点）的接收窗口</strong>。因为这些接收端的设备比较特殊，沿用了老旧的配置，导致TCP接收窗口过小。</p><p>窗口小是因为Window Scale没被启用吗？我们检查发现，Window Scale其实启用了，你可以回头看一下窗口值的截图，有几次的值是65728，明确超过了65535。另外，在握手包里也可以看到Window Scale被启用的信息：</p><p><img src="https://static001.geekbang.org/resource/image/0c/e0/0ce22e90fecc80c194b183cb572ca6e0.jpg?wh=1764x1026" alt="图片"></p><blockquote>\n<p>红框中的 <code>TCP Option-Windows scale: 6</code> 就是表示窗口系数（Scale Factor）为6，即2的6次方（得64）。也就是说，真正的接收窗口值，是Window字段的值乘以64。</p>\n</blockquote><p><strong>那为什么启用了Window Scale，还是被限制在64KB附近呢？</strong>我们发现，这还是受限于这台设备本身的特点。在某些情况下，这里的Window Scale虽然启用了，但无法充分工作，导致实际上这台设备的接收窗口一直被压制在64KB附近。</p><p>想象一下，发送端每次“喂到”网络上的数据只有64KB，这64KB还需要经过134ms后才能到达接收端。然后就这样周而复始，直到全部的95MB发送完成。明明是很宽敞的网络，但可惜，对端的收发室太小了，你只好把大货车停车库里，改开电瓶车，一小车一小车地送货，你急也没用。这速度，哪里还起得来？</p><p>我们对它的配置做了修改，使得接收窗口大幅增加后，速度马上提上去很多倍，这才彻底解决了问题。</p><h2>最后的疑问</h2><p>不过你是否留意到了，难道当时美国本土数据中心里面的设备就没有这个问题吗？如果也是用了这个老旧的配置，难道就没有传输慢的问题吗？</p><p>我给你的答案是：其实也是老旧的配置，其实速度也是慢的。但是为什么这个问题就没有被暴露呢？这又是跟前面的知识点有关系了。你能想到是哪个因素掩盖了这个问题吗？</p><p>这次的答案不是窗口，而是<strong>往返时间</strong>。在美国本土的多个数据中心之间，RTT大约在10ms多一点，也就是只有美国到荷兰的134ms的十分之一。联系公式“<strong>速度=距离/时间</strong>”，分子（接收窗口）不变，分母（时延）变为原来的十分之一，那么速度会变成原来的十倍。十倍的速度就没有显得那么慢了，所以并没有引起我们的注意。</p><p>我用一个图表示一下这种RTT的不同引起的传输上的区别：</p><p><img src="https://static001.geekbang.org/resource/image/29/04/29f1ae041993e7181e989e04ebd3f504.jpg?wh=1900x992" alt=""></p><p>你看，传输速度的问题，可以说就是窗口和往返时间这两个大玩家在起作用。你只要<strong>抓住这两个主要矛盾，就能解决大部分传输速度的问题</strong>了。其他因素也可能有它的作用，但一般不是核心矛盾。我们要学会抓重点，这对于工作，乃至对于人生，都是很有意义的。</p><h2>小结</h2><p>这节课，我用了一个典型的文件传输的案例，帮你回顾了跟TCP传输有关的方方面面的知识点，包括：</p><ul>\n<li><strong>时延</strong>：也叫往返时间，是通信两端之间的一来一回的时间之和。</li>\n<li><strong>在途报文</strong>：发送端已经发出但还未被确认的报文，时延越长，发送窗口越大，在途报文可能越多，这两者是正比关系。</li>\n<li><strong>带宽时延积</strong>：带宽和时延的乘积，表示这个网络能承载的最多的在途数据量。</li>\n<li><strong>发送窗口</strong>：发送窗口是拥塞窗口和对方接收窗口两者中的较小值。</li>\n</ul><p>基于以上的知识，我们得以推导出最终的<strong>核心公式：速度上限=发送窗口/往返时间</strong>。用英文可以表示为：velocity = window/RTT。</p><p>以后你在处理TCP传输速度问题的时候，一样可以应用上面这些知识：先获取时延，再定位发送窗口，最后用这个公式去得到速度的上限值。</p><p>除了这些TCP的知识点，我也提到了使用Wireshark的一些技巧，包括：</p><ul>\n<li><strong>查看I/O Graph的方法</strong>，这个可以直观地展示数据传输的速度。</li>\n<li><strong>查看TCP Stream Graphs的方法</strong>，这个能看到TCP序列号随着时间的变化趋势，同时也可以经过简单计算推导出TCP载荷的传输速度（因为没有计入各种报文头部，这个值比#1的速度值要略低一些）。</li>\n<li><strong>查看TCP Window Scale是否启用</strong>，以及启用的话，Scale Factor的值的查找方法。</li>\n</ul><p>总而言之，你在自己处理类似的传输速度、延迟等问题的时候，一方面可以充分利用我分享的协议方面的知识，获得坚实的理论支撑；一方面也可以使用这节课提到的这些Wireshark分析技巧，获得数据上的支持。当你把理论和数据这两条“腿”都锻炼健壮后，你一定能在网络排查的世界里，更加坚定有力地前行。</p><h2>思考题</h2><p>你有没有在工作中遇到过TCP传输速度相关的问题呢？通过这节课的学习，你已经掌握了传输速度相关的不少知识，你准备怎么运用这些知识，来解决这个传输问题呢？</p><p>欢迎在留言区分享出你的答案和思考过程，也欢迎你把今天的内容分享给更多的朋友，我们一起成长。</p><h2>附录</h2><p>抓包示例文件：<a href="https://gitee.com/steelvictor/network-analysis/tree/master/09">https://gitee.com/steelvictor/network-analysis/tree/master/09</a></p>',
        article_title: "09 | 长肥管道：为何文件传输速度这么慢？",
      },
      {
        title: "10 | 窗口：TCP Window Full会影响传输效率吗？",
        id: 485689,
        content:
          '<p>你好，我是胜辉。</p><p>有时候，不少知识点在过段时间重新回看的时候，又会有新的体会和发现。比如在<a href="https://time.geekbang.org/column/article/484667">第8讲</a>里，我们回顾了一个MTU造成传输失败的案例，虽然整个排查过程的步骤不算很多，但也算是TCP传输问题的一个缩影了。尤其是其中那个失败的TCP流中的一些现象，比如客户端发出的重复确认（DupAck），还有服务端启动的超时重传，都值得我们继续深挖，所以我会在后续的课程里继续这个话题。</p><p>然后在上节课里，我们还探讨了传输速度的相关知识，也初步学习了窗口的概念。最后，我们终于推导出了TCP传输的核心公式：速度=窗口/往返时间。这个公式，对于我们理解传输本质和排查传输问题，都有很强的指导意义。</p><p>然而，如果你足够细心的话，其实可能会对上节课里的细节有一些疑问，比如：既然接收窗口满了，那为什么当时没有看到TCP Window Full这种提示呢？</p><p>其实，我这边也有不少内容按住没有展开，包括核心公式的理解，我们在这节课里将有一个新的认识。另外，我也将带你继续挖掘窗口这个细分领域，这样你以后遇到跟窗口相关的问题，就知道如何破解了。</p><h2>案例：TCP Window Full是导致异地拷贝速度低的原因吗？</h2><p>也是在公有云服务的时候，有个客户有这么一个需求，就是要把文件从北京机房拷贝到上海机房。但是他们发现传输速度比较慢，就做了抓包。在查看抓包文件的时候，发现Wireshark有很多<strong>TCP Window Full</strong>这样的提示，不明白这些是否跟速度慢有关系，于是找我们来协助分析。</p><!-- [[[read_end]]] --><h3>解读Expert Information</h3><p>我们先要了解一下抓包文件的整体状况。怎么看呢？当然是看Expert Information了：</p><p><img src="https://static001.geekbang.org/resource/image/72/8c/7285f181c54e6f464836e8c44f43c88c.jpg?wh=1818x234" alt="图片"></p><p>它确实提醒我们，有69个Warning级别报文，它们的问题是TCP Window specified by the receiver is now completely Full。在展开进一步排查之前，我们先对这个信息做一下解读。</p><ul>\n<li>TCP Window：在上节课里，我介绍了TCP的三种窗口，分别是发送窗口、接收窗口、拥塞窗口。那么这里说的是哪个窗口呢？一般说到TCP Window，<strong>如果没有特别指明，就是指接收窗口</strong>。</li>\n<li>specified by the receiver：这也很明确，这个窗口是接收方的，其实就是佐证了这个窗口就是接收窗口。</li>\n<li>is now completely Full：窗口满了，这又怎么理解呢？你还记得在上节课里学过的在途数据，也就是Bytes in flight吗？<strong>当在途数据的大小等于接收窗口的大小时，这个窗口就是“满了”</strong>。</li>\n</ul><p>好了，这个信息解读完毕，一句话说就是：发生了69次在途数据等于接收窗口的情况。</p><p>接下来我们看看TCP Window Full具体是个什么样子。比如我们选中224号报文，主界面也自动定位到了这个报文：</p><p><img src="https://static001.geekbang.org/resource/image/49/4f/4957d1a108c78d3f73eee9b85213864f.jpg?wh=1920x960" alt="图片"></p><p>我们可以看到，除了224报文，也确实有很多其他报文也报了TCP Window Full的警告信息。</p><h3>解读TCP Window Full</h3><p>TCP Window Full这个信息非常直接明了，就是说“接收窗口满了”。不过，你可别以为这个信息是TCP报文里的某个字段。其实，它只是Wireshark通过分析得出的信息。你有没有注意到，TCP Window Full前后是有方括号的。一般来说，<strong>Wireshark自己分析得到的信息，都会用方括号括起来</strong>，而TCP报文本身的字段，是不会带这种方括号的。我们来看一个截图：</p><p><img src="https://static001.geekbang.org/resource/image/b7/a5/b76369d4ef7fe6f9dd49816247351fa5.jpg?wh=1630x1062" alt="图片"></p><p>上面是224号报文的TCP详情，里面有不少信息也带上了方括号，比如其中的 [Bytes in flight: 112000]，这也是解读出来的，而且它跟Window Full关系很大。</p><p>前面提到过，在途数据（或者叫在途字节数，Bytes in flight）等于接收窗口大小的时候，Wireshark就会解读为TCP Window Full了。不过，如果你在上图中找一下Window size，会发现它是19200，而不是Bytes in flight的112000，这又是为什么呢？</p><p><img src="https://static001.geekbang.org/resource/image/82/98/829ee4eda487862ed761cd0bafff8d98.jpg?wh=674x396" alt="图片"></p><p>这是因为，我们把发送方和接收方的接收窗口搞混啦。这里你需要搞清楚：如果说在途数据的发送方是A，接收方是B，那么这里Window Full的窗口，<strong>是B的接收窗口</strong>，而不是A的接收窗口。上图是A的报文，自然没有我们要找的B的接收窗口信息了，那怎么找到B的接收窗口呢？</p><p>因为这次通信是SCP文件传输，那么A就是客户端，它的端口是38979；B就是服务端，它的端口是22。我们的具体做法是：在抓包文件里，找到B（也就是源端口为22）的报文，而且应该是这个TCP Window Full报文之前的最近的一个，在这个报文里就有B的最近的接收窗口值。</p><p>单纯用文字，可能未必容易理解，我给你画了一张示意图：</p><p><img src="https://static001.geekbang.org/resource/image/2e/95/2e594ee968a7a62b307e5f22c08c4095.jpg?wh=2000x1125" alt=""></p><p>上图中，我还是用A指代发送端，用B指代接收端。当A的在途数据跟B的接收窗口大小相等时，Wireshark就会判断出，这个接收窗口满了，这意味着：A无法再从自己的发送缓冲区把数据发送出来了。只有当B回复ACK，确认了n字节的数据后，A才有可能发送最多n字节的数据（如果缓冲区有足够多的待发数据的话）。</p><p>让我们回到Wireshark窗口，找到离这个TCP Window Full最近的，从源端口22发送来的报文。我们发现，它就是下图这个222号报文：</p><p><img src="https://static001.geekbang.org/resource/image/30/60/30d13a6c4e4730742fe1bed79b6b0a60.jpg?wh=1836x1404" alt="图片"></p><p>可以看到，这个报文的接收窗口就是112000，正好等于前面224号报文的Bytes in flight的112000字节。所以，我把前面的示意图改进一下供你参考。这里面的信息比较多，建议你耐心多花几分钟时间来充分理解其中的机制：</p><p><img src="https://static001.geekbang.org/resource/image/84/0f/84453f815b643f6ac29062a3b9ccf90f.jpg?wh=2000x1125" alt=""></p><p>整个过程是这样的：</p><ul>\n<li>B发送了报文222给A，其中带有B自己的接收窗口112000字节。由于这是一个纯的确认报文，所以没有TCP载荷，也没有在途数据。</li>\n<li>报文抵达A端，进入A的接收缓冲区。</li>\n<li>A从222号报文中得知，B现在的接收窗口是112000字节，由于发送缓冲区有足够多的待发的数据，A选择用满这个接收窗口，也就是连续发送112000字节。</li>\n<li>A把这112000字节的数据发送出来，成为报文224，其中还带有A自己的接收窗口值19200字节，不过，由于这次主要是A向B传送数据，所以B发给A的基本都是纯确认报文，这些报文的载荷都是0。极端情况下，即使A的接收窗口为0，只要B回复的报文没有载荷，它们也是可以持续通信的。</li>\n<li>224报文抵达B端，正好填满B的接收窗口112000字节。Wireshark分别从222报文中读取到B的接收窗口值，从224报文中读取到在途字节数，由于两者相等，所以Wiresahrk提示TCP Window Full。而这个信息是被Wiresahrk展示在224报文中的。</li>\n</ul><h2>自己验证TCP Window Full</h2><p>对于在途数据，既然Wireshark可以解读出来，那只要理解了TCP的原理，我们同样可以自己来计算，这不仅可以考查我们对TCP知识的掌握程度，同时对日常排查也有帮助。有这么多好处，你是不是跃跃欲试了呢？不过在开始之前，我们要先学习一个新的概念。</p><h3>下个序列号</h3><p>下个序列号，也就是<strong>Next Sequence Number</strong>，缩写是<strong>NextSeq</strong>。它是指当前TCP段的结尾字节的位置，但不包含这个结尾字节本身。很显然，下个序列号的值就是当前序列号加上当前TCP段的长度，也就是<strong>NextSeq = Seq + Len</strong>。</p><p>这也不难理解，因为TCP字节流是连续的，那么既然Seq + Len是这个报文的数据截止点，自然也是下一个报文的起始点，你可以参考这个示意图：</p><p><img src="https://static001.geekbang.org/resource/image/f7/bf/f7864509fd7ff569ebbfa54e14a9e6bf.jpg?wh=2000x910" alt=""></p><p>在Wireshark里，我们也可以找到NextSeq这个解读值，比如下图这样：</p><p><img src="https://static001.geekbang.org/resource/image/bd/7e/bd1093a136119ebf4166989e64b3637e.jpg?wh=756x201" alt="图片"></p><p>明白了NextSeq，我们来看如何手工验证TCP Window Full。比如，还是分析224号报文的这次TCP Window Full，我们可以这么做，来验证一下在途数据是否真的是112000字节。</p><p><img src="https://static001.geekbang.org/resource/image/0e/28/0ec8c879acb7526bbff8eb9650cyy928.jpg?wh=1558x550" alt="图片"></p><p>首先，跟上面的步骤类似，我们要找到222号报文。在这个报文里，服务端（源端口22）告诉客户端：“我确认你发送来的198854字节的数据”。我们先把这个数字记为X。</p><p>然后，我们查看224号报文里，客户端发送的数据到了哪个位置：</p><p><img src="https://static001.geekbang.org/resource/image/22/2e/22f94b3933fb841705b28d11a3106b2e.jpg?wh=1552x984" alt="图片"></p><p>我们可以在224号报文的TCP详情页，看到Next sequence number: 310854，而这个数字，就是客户端发送的数据的最新的位置。我们把这个数字记为Y。当然，你也可以像前面说的那样，把Seq和Len加起来，也就是308054 + 2800，得到的自然也是310854。</p><p>最后，我们做一个最简单的减法：Y - X = 310854 - 198854 = 112000！这正是前面说的在途数据的大小。</p><p>恭喜你，你已经学会了如何手工计算在途数据的方法，这也意味着你对TCP的了解又更深入了一点。你可以这么来总结计算在途数据的方法：</p><blockquote>\n<p><strong>Bytes_in_flight = latest_nextSeq - latest_ack_from_receiver</strong></p>\n</blockquote><p>不过，你会不会觉得，虽然这个计算方法对理解窗口有帮助，但是既然Wireshark会给我们提示，那这种计算也主要是自我练习而已，应该不会真的用得上吧？</p><p>这还真不好说。因为，Wireshark在不少场景下并不会给你提示。比如，在接收窗口接近满但又不是完全满的时候，哪怕是离窗口满只差1个字节，Wireshark也不会提示TCP Window Full了。但是，在途数据都已经逼近接收窗口的99.9%了，你还觉得这个肯定没有问题，或者一定没有隐患吗？</p><p>要知道，这种临界状况也很可能跟问题根因有关。那么你掌握了这个方法，就可以把排查做得更彻底了。或者，如果你想预防性能瓶颈，那么提前找到这种窗口临界满的状况，也是有益的。</p><p>到这里，我们可以回答开始时候的问题了：为什么上节课里，没有看到TCP Window Full这种提示呢？我们看一下当时的报文状况吧。</p><p>在22:30:39.067477，接收端的确认号为7105632：</p><p><img src="https://static001.geekbang.org/resource/image/95/15/95c7e978090178984b6cb27355021e15.jpg?wh=896x61" alt="图片"></p><p>然后在22:30:39.209712，接收端的确认号为7169872：</p><p><img src="https://static001.geekbang.org/resource/image/4a/77/4ae7928be673fd6d2c43c7289aed0077.jpg?wh=901x59" alt="图片"></p><p>这两个报文的时间跨度正好是141ms左右，也就是这次传输里面的<strong>往返时间</strong>。在这个往返时间里，接收端确认了多少数据呢？是7169872-7105632=64240，也就是64KB。这个就是99.9%逼近TCP Window Full了，但是因为还差小几十个字节，所以Wireshark并没有提示TCP Window Full！</p><p>你可能还想追问：那为什么不把这剩余的0.1%的窗口“榨干”，非要留一点呢？我们看一下当时的接收窗口和在途数据的具体情况，就以上面选择的5864报文附近为例：</p><p><img src="https://static001.geekbang.org/resource/image/14/d7/1439256885a585cebc96a139e41495d7.jpg?wh=900x91" alt="图片"></p><p>接收端（源端口为22）的接收窗口为65728，发送端（源端口为59159）的在途数据为65700，两者相差只有28字节。对于发送端来说，没有必要为了这区区28字节再发送一个小报文了，等接收窗口空余出多一点的空间后再动身不迟。</p><blockquote>\n<p>如果你还没学习过上节课的内容，可能会对这些信息感到疑惑，建议先去把上节课学完，再来学习这一讲，效果更好。</p>\n</blockquote><h2>TCP Window Full对传输的影响</h2><p>好了，现在我们已经对TCP Window Full做了充分的分析，而且也明白了：这就是<strong>接收端的接收窗口小于发送端的发送能力而出现的状况</strong>。我们也很容易得出推论：瓶颈在接收端，<strong>TCP Window Full也确实会影响传输速度</strong>。</p><p>春节刚过，你可能对高速公路上的状况也感受深刻吧！很多路段出现了堵车，这就相当于TCP Window Full，更多的车辆上不了高速了，只好堵在外面。如果高速公路的路更宽、车速更快，那么就相当于接收窗口变得更大，车辆就能进更多，也就相当于Bytes in flight更大了。这么说来，TCP流量控制和高速车流控制这两个领域也有不少共通之处，说不定双方都互有借鉴呢。</p><p>回到客户这次的案例。我们看看，这次的传输速度是多少呢？在上节课里，我介绍了在Wireshark里查看TCP传输速度的两种方法。比如，我们现在用I/O Graph来看一下：</p><p><img src="https://static001.geekbang.org/resource/image/50/5f/50bb15443878c7458035155f5a19425f.jpg?wh=1920x1287" alt="图片"></p><blockquote>\n<p>补充：如果你的I/O Graph显示的不是这种图，那需要像图中这样：</p>\n<ul>\n<li>选中All Bytes指标；</li>\n<li>Y轴的单位选为Bytes。</li>\n</ul>\n</blockquote><p>这个图不能说不对，但柱子比较粗，看起来不是很精确。这是因为，它默认是以1秒为间隔而计算的速度。但是TCP传输中途，很可能每过几毫秒都有所变化，所以，如果我们要看更加精细的图，可以调整一下粒度，把Interval从1sec改为100ms，看看会怎么样：</p><p><img src="https://static001.geekbang.org/resource/image/91/e1/91b410903c0b1f35fa0b88666ae012e1.jpg?wh=308x328" alt="图片"></p><p><img src="https://static001.geekbang.org/resource/image/55/5b/5575a289a1aba0d303a0be0db968175b.jpg?wh=1920x1284" alt="图片"></p><p>这样看起来精确了很多。我已经把这个抓包文件上传到<a href="https://gitee.com/steelvictor/network-analysis/tree/master/10">Gitee</a>，你可以下载后，依照这里的步骤，把我做过的排查工作也演练一遍，这对你掌握课程知识是很有帮助的。</p><blockquote>\n<p>补充：如果你用Wireshark查看到的I/O Graph跟我这里的不同，那可以对比一下Interval和SMA period这两个配置是否跟图中的一致。</p>\n</blockquote><p>这里有个小的注意点：因为我们选择的间隔是100ms，所以Y轴的数字就是Bytes/100ms，换算成Bytes/s的话，要把Y轴的数字再<strong>乘以10</strong>。从图上看，在一开始的8秒几乎没有数据传输发生，从第8秒开始速度上到了400KB/s（就是图上的40K*10）左右，一直到结束都大致维持在300~400KB/s这个区间里。</p><h2>继续深挖窗口</h2><p>一般来说，接收窗口、拥塞窗口、发送窗口，这些都不是一上来就是一个很大的值的，而是跟汽车起步阶段类似，逐步跑起来的。那么这就产生了一个很有意思的话题：这些窗口之间都是怎么协调的呢？直观上感觉，无论哪个更快了，另外两个就要受影响。</p><p>我们很容易理解，假设起始值相同，如果接收窗口增长的速度小于拥塞窗口的增长速度，那么接收窗口就成了瓶颈；反过来说，拥塞窗口增速更小，那么它就成了瓶颈。</p><p>当接收窗口成为瓶颈的时候，很容易就出现这里的TCP Window Full的现象。不过，我们这么多讨论都是基于文字，如果有更加直观的方式，让我们理解这个现象就更好了。这里，我们就可以再学一个Wireshark的小工具：TCP Stream Graphs里面的 <strong>Window Scaling</strong>。</p><p>我们还是打开Wireshark的Statistics下拉菜单，找到TCP Stream Graphs，在二级菜单中，选择Window Scaling：</p><p><img src="https://static001.geekbang.org/resource/image/cf/e4/cfa2f41e1e0a52f9yyd4d226c441a4e4.jpg?wh=462x552" alt="图片"></p><p>这时候就能看到Windows Scaling的趋势图了：</p><p><img src="https://static001.geekbang.org/resource/image/0c/82/0cda89fe0232b8515007fe1fc5e7aa82.jpg?wh=922x817" alt="图片"></p><p>我就直接给你把关键信息标注出来了。这里主要是两个关键点。</p><ul>\n<li><strong>数据流的方向要找对</strong>：比如这次传输是从客户端向SSH服务端发送数据，所以要确认这是从一个高端口向22端口发送数据的流向。如果搞反了，那图就变成了SSH服务端回复的ACK报文了，不是你要分析的传输速度了。</li>\n<li><strong>定位TCP Window Full</strong>：在这里，Receive Window是“阶梯”式的，每次变化后会保持在一个“平台”一小段时间，那么这时候Bytes Out（发送的数据，也就是Bytes in flight）就有可能触及这个“平台”，每次真的碰上的时候，就是一次TCP Window Full。</li>\n</ul><p>我们可以看一个例子。图中的蓝线代表Bytes Out，绿线代表Receive Window。你可以像我这样，在这几个“平台”区域，找到蓝线和绿线的汇合点，然后在这些点上点击鼠标左键，就能定位到TCP Window Full事件了。</p><p><img src="https://static001.geekbang.org/resource/image/37/3e/37cb41aa86cce8a437c13db7374d2a3e.jpg?wh=832x573" alt="图片"></p><p>上图中，我用鼠标放大了一个“平台”，然后选中了一个Receive Window和Bytes Out重合的点，它是1200号报文，主窗口也自动定位到了这个报文，果然它也是一次TCP Window Full。</p><h2>验证传输公式</h2><p>在上节课里，我们推导出了TCP传输的核心公式：<strong>速度=窗口/往返时间</strong>。既然当前案例里TCP Window Full的时候，Bytes in flight跟接收窗口相等，那么在这个公式里的窗口，是否就是Bytes in flight呢？我们来验证一下。</p><p>还是在Wireshark窗口里，我们要添加这么几个自定义列，以便进行数据比对：</p><ul>\n<li>Acknowledgement Number：确认号</li>\n<li>Next Sequence Number：下个序列号</li>\n<li>Caculated Window Size：计算后的接收窗口</li>\n<li>Bytes in flight：在途字节数</li>\n</ul><blockquote>\n<p>补充：如果对怎么添加自定义列还不清楚，可以复习<a href="https://time.geekbang.org/column/article/481042">第5讲</a>中的添加TTL自定义列的部分。</p>\n</blockquote><p>另外，因为我们要集中检查发送端的Bytes in flight，就需要把源端口38979的报文过滤出来，这样就不会被另一个方向的报文给干扰了。</p><p><img src="https://static001.geekbang.org/resource/image/eb/52/ebf76c39e874a4350f8423485f0acd52.jpg?wh=1231x315" alt="图片"></p><p>在这里，我们看到的Bytes in flight是112000字节左右。从右边滚动条的位置来看，这是在传输过程的初期。让我们滚动到中间和后期，看看这些在途字节数是多少：</p><p><img src="https://static001.geekbang.org/resource/image/56/eb/5633292b85320c4a309d67f06c94b0eb.jpg?wh=1152x315" alt="图片"></p><p>中期这里的Calculated Window Size明显增大了，到了445312字节。再看看后半程：</p><p><img src="https://static001.geekbang.org/resource/image/55/d7/55cecd4faa33088df661d5058df379d7.jpg?wh=1158x314" alt="图片"></p><p>最后阶段已经达到863800字节。综合这三个阶段来看，折中值差不多在400KB左右，我们把它除以RTT 0.029秒，得到的是400KB/0.029s=13790KB/s。显然，这个数值远超过前面I/O Graph里看到的300~400KB/s。这是怎么回事呢？难道我们的核心公式是错的吗？</p><p>不知道你有没有考虑到这个问题：Bytes in flight是指真的一直在网络上两头不着吗？一般来说，数据到了接收端，接收端就发送ACK确认这部分数据，然后TCP Window就往下降了。比如ACK 300字节，那么TCP Window就又空出来300字节，也就是发送端又可以新发送300字节了。</p><p><img src="https://static001.geekbang.org/resource/image/ba/96/ba911b5a75b84f691d7c61821e759b96.jpg?wh=2000x871" alt=""></p><p>像图上这种情况：</p><ul>\n<li>B通知A：“我的接收窗口是1000”；</li>\n<li>A向B发送了1000字节，此时B的接收窗口满；</li>\n<li>B向A确认了300字节的数据，自身的接收窗口也扩大为300字节；</li>\n<li>A的在途字节数也从1000字节变成700字节，因为刚刚有300字节被B确认了。</li>\n</ul><p>图中的t1到t4表示时间点。t2到t4就是一次往返的时间，在这个往返时间内，被传输的数据是1000字节吗？不是。因为被确认的只有300字节，所以传输完的也只有这300字节，速度也就不是1000/RTT，而是300/RTT！我们可以把核心公式做一下改进，变成下面这个：</p><blockquote>\n<p><strong>velocity = acked_data/RTT</strong></p>\n</blockquote><p>我们再用改进后的公式来计算这次的速度。我们可以选择传输中间偏后面一点的报文来做分析。比如下图中，我们选择1337号和1357号报文为起始和截止点，计算NextSeq的差值，还有时间的差值，然后两个差值相除。</p><p><img src="https://static001.geekbang.org/resource/image/d7/ec/d7174698a1e5c56357f6bd559bbb83ec.jpg?wh=978x289" alt="图片"></p><p>33600/0.094 = <strong>357KB/s</strong>。是不是很接近I/O Graph的值了？看来这样计算才是正确的！</p><p>那为什么在上节课里我们就可以用 <strong>窗口/往返时间</strong> 来计算速度，而且数值也很准呢？而这种方法用到这里就完全不对呢？</p><p>这是因为上节课的案例，在途数据一旦到了接收端，都被及时确认了。而当前这个案例里面并没有这样。也就是说，这次的案例，出现了“滞留”现象。</p><p><img src="https://static001.geekbang.org/resource/image/4e/e0/4e02af20ef67d67490ba5b2c00bb7de0.jpg?wh=1000x512" alt="图片"></p><p>还是1337到1357号报文，我们去掉了过滤器 <code>tcp.srcport eq 38979</code>，这样就展示了双向报文。可以看到，服务端（B端）在这段时间内，只确认了22400字节（1495254 - 1472854），而同样时刻的在途数据，却一直维持在一个比较高的数字，在660KB上下。所以，<strong>真正完成了传输的数据量，是前者22400B，而不是“虚浮”的660KB</strong>。</p><p>那你可能又要问了：既然已经确认了22400字节，为什么客户端的在途字节数还是没有变化呢？</p><p>这是因为，客户端被确认了22400字节的数据，马上又把这个尺寸的数据发送出去了，事实上就<strong>维持了这个在途字节数的尺寸</strong>。</p><p>我可以再做一个比喻帮助你理解这个现象。我们如果去银行的一个窗口（这可不是TCP窗口）排队办业务，现在排队人数为10人，相当于Bytes in flight为10。每分钟都有一个人能完成业务办理，原以为队列会减小为9人，结果每当有一个人出来，保安就喊：“下一个！”于是就立刻又补进来一个人，所以队伍还是维持在10人这么长。</p><p>那么，窗口的业务员的办理速度是多少呢？显然不是10人/分钟，而是1人/分钟了。这样是不是理解起来容易多了？而上节课的情况，相当于这里的“每次就处理一个人”，所以处理速度就是1人/分钟，也就可以用“速度=窗口/往返时间”来计算了。</p><h2>小结</h2><p>这节课，我们集中讨论了TCP传输中的窗口相关的知识，特别是围绕TCP Window Full这个Wiresahrk中比较常见的信息，展开了深入的讨论。你需要重点掌握以下这些知识点：</p><ul>\n<li>Wireshark报告TCP Window Full是因为，一端的在途数据跟另一端的接收窗口相等。</li>\n<li>TCP的下个序列号（Next Sequence Number）等于序列号和段长度之和，即<strong>NextSeq = Seq + Len</strong>。</li>\n<li>我们可以自己人工计算出在途字节数，公式是<strong>Bytes_in_flight = latest_nextSeq - latest_ack_from_receiver</strong>。</li>\n<li>人工计算在途字节数的方法是：\n<ul>\n<li>找到最近一次发送出去的报文的NextSeq，记为X；</li>\n<li>找到在这次发送之前收到的最近的ACK，记录它的ACK，记为Y；</li>\n<li>X－Y，得到在途字节数。</li>\n</ul>\n</li>\n<li>另外我们也知道了，TCP Window Full确实会影响到传输速度。</li>\n</ul><p>除了技术知识点，我也带你学习了下面这些Wireshark工具使用技巧：</p><ul>\n<li>在Statistics下拉菜单下的I/O Graph工具，可以直观地展示传输速度图。</li>\n<li>同是Statistics菜单下的TCP Stream Graphs的Window Scaling工具，可以直观的展示TCP Window Full历史曲线图。</li>\n</ul><p>最后，我们还发现这个案例里的接收端有“数据滞留”的现象，这就导致“速度=窗口/往返时间”的公式遇到了挑战，而我们可以进一步优化为新的公式：<strong>速度=确认数据/往返时间</strong>。这个改进后的公式，可以兼容这种有“数据滞留”现象的传输场景。</p><h2>思考题</h2><p>给你留两道思考题：</p><ul>\n<li>TCP的序列号和确认号，最大可以到多少？</li>\n<li>接收端只确认部分数据，导致了“数据滞留”现象，这个现象背后的原因可能是什么呢？</li>\n</ul><p>欢迎你把答案分享到留言区，我们一同交流和进步。</p><h2>附录</h2><p>抓包示例文件：<a href="https://gitee.com/steelvictor/network-analysis/tree/master/10">https://gitee.com/steelvictor/network-analysis/tree/master/10</a></p>',
        article_title: "10 | 窗口：TCP Window Full会影响传输效率吗？",
      },
      {
        title: "答疑（二）| 第6~10讲思考题答案",
        id: 495213,
        content:
          '<p>你好，我是胜辉。</p><p>在上一节答疑课里，我们回顾了第1到第5讲的思考题，分析了每道题目的解题思路，当然更重要的是对这五节课的知识点做了一次复习和补充。你看完了这些解答后的感觉如何呢？跟之前你自己的思考过程和结论相比，又有哪些相同和不同之处呢？相信通过再一次的思考，你对知识的掌握将会更加深刻。</p><p>那么这节课，我们就继续讲解这些课后的思考题。先从第6讲开始。</p><h2>06讲的答疑</h2><h3>思考题</h3><p>这节课，我给你介绍了利用防火墙的TTL跟原先的正常报文的TTL不一致的特点，从而识别出防火墙的方法。那么，假设有一天，防火墙公司把这个特性也完善了，我们再也不能仅凭TTL的突变而发现防火墙了，你觉得还有什么办法可以识别出防火墙吗？</p><h3>答案</h3><p>这是一个开放式的问题，其实并没有标准答案。不过，通过对这个问题的思考，我们可以对网络的理解更进一步。我们先参考下面这张图，复习一下用TTL判断防火墙的原理：</p><p><img src="https://static001.geekbang.org/resource/image/f6/56/f6a1206ea604d7f3dd88981e39366256.jpg?wh=1694x562" alt=""></p><blockquote>\n<p>补充：上图是一个例子，假设防火墙是路径上的第5跳，那么防火墙自己发出的报文的TTL就比真正的发送端的TTL多了5。</p>\n</blockquote><p>然后想一想，我们之所以能通过TTL定位防火墙，本质原因是什么呢？实际上，本质原因就是这个IP报文是防火墙自己构造的，而<strong>在构造时TTL被设置为了初始值</strong>。</p><!-- [[[read_end]]] --><p>那么，还有哪些元信息也是在这个构造好的报文里呢？其实这些信息也可以帮我们辅助判断防火墙。</p><p>另外在IP层，除了TTL，还有一个字段也值得我们注意，它就是<strong>IP ID</strong>。这个字段占用了2个字节，所以它的最大值是2^16-1，也就是65535。IP ID一般是IP报文的发起端设置的，每次发包递增1，然后在0到65535之间循环使用。</p><p>它最主要的用途，是<strong>接收端会根据这个ID来组装（assemble）同一组IP分片（fragments），</strong>而中间设备（交换机、路由器）不会改动IP ID。并且在防火墙构造RST报文的时候，IP包的TTL和ID值也是防火墙自己设置的，也就有可能被我们用来发现防火墙。所以，如果SYN+ACK和RST的IP ID不连续，我们就可以判断出防火墙，也就是下图这样：</p><p><img src="https://static001.geekbang.org/resource/image/51/52/51df7643ff2127011ce9e6fd124e9652.jpg?wh=1846x884" alt=""></p><p>不过，我在探究这个可能性的时候，又遇到了新的问题。要知道，Linux系统发送SYN+ACK的时候，IP ID一定是0，但是我既在Wireshark里看到了这个现象，也在内核代码里证实了这一点。而这就给我们利用IP ID带来了障碍：<strong>因为即使RST真的是对端发出的，两个报文的IP ID也会不同。</strong></p><p>只有当我们收到至少三个报文，也就是除了SYN+ACK和RST之外，我们还需要在中间收到一些报文，才可以对比RST跟中间报文的IP ID的差别，然后进行判断。</p><p>所以，利用IP ID的方法，对于TCP握手后就发生RST的情况并不适用，但对于交互过程中发生RST的情况应该是适用的。你有机会也可以在遇到RST时，来抓包验证一下。</p><p><img src="https://static001.geekbang.org/resource/image/16/d5/168e95c43c17fc318d72df93205231d5.jpg?wh=1821x943" alt=""></p><h2>07讲的答疑</h2><h3>思考题</h3><ol>\n<li>tcp.payload eq abc，这个过滤器可以搜索到精确匹配“abc”字符串的报文。那么，如果是模糊匹配，比如只要包含“abc”的报文我都想搜到，这个过滤器又该如何写呢？</li>\n<li>在工作中，你遇到过跟TCP Keep-alive相关的问题吗？你是怎么解决的呢？</li>\n</ol><h3>答案</h3><p>第一个问题，其实在上节答疑课里我也提到过，就是利用各层的 <strong>contains</strong> 动词来写过滤器。比如：</p><ul>\n<li>应用层可以是 <code>http contains "abc"</code>；</li>\n<li>传输层可以是 <code>tcp contains "abc"</code>；</li>\n<li>网络层可以是 <code>ip contains "abc"</code>；</li>\n<li>数据链路层可以是 <code>frame contains "abc"</code>。</li>\n</ul><p>这类过滤器的使用场景一般是这样的：我们知道应用层的某一个信息，比如HTTP报文里面的某个uuid，然后再通过这个信息，去找到对应的报文。</p><p>而在这里，又可以细分为两种不同的场景。</p><p><strong>第一种，某一个TCP连接在通信两端的报文的对应。</strong>比如，客户端发出的HTTP报文里含有某个uuid，那么我们需要在服务端的抓包文件里，把这个TCP流过滤出来，就可以用上面说的那些过滤器。</p><p><img src="https://static001.geekbang.org/resource/image/61/e6/6180fd2be0492305d612eb6e2f2fbce6.jpg?wh=2000x954" alt=""></p><p><strong>第二种，LB前后方两个不同TCP连接的报文的对应。</strong>比如，LB会转发请求给后端多台服务器中的某一个，而这个选择一般是动态的，这就给我们的排查工作带来了不小的障碍。</p><p>那么这里可以参考的方法就是：在“客户端&lt;-&gt;LB”这一侧的连接中，找到HTTP报文的某个uuid，然后在“LB&lt;-&gt;多个后端IP”这一侧的抓包中，搜索这个uuid，就能得到同样是这个请求的相关报文了。</p><p>我们可以看看下面这个示意图：</p><p><img src="https://static001.geekbang.org/resource/image/28/6d/282b9b474a7b96998c657bfdbebae46d.jpg?wh=2000x1125" alt=""></p><p>也就是说，TCP流1和TCP流1<code>\'</code>，还有TCP流2和TCP流2<code>\'</code>，分别是同一个TCP流在连接两侧的不同体现。而<strong>由于流1和流2确实是完全不同的连接，无法用任何TCP报文头部的信息来进行匹配，所以只能用应用层的信息来找到对应关系</strong>。</p><p>第二个问题也是一个开放式的问题。我看到不少同学都分享了他们的经验，这也增加了我自己的见识，真是一件多赢的事。比如，<strong>@江山如画</strong>同学对这个问题的回复是这样的：</p><blockquote>\n<p>在工作中，我目前还没有碰到过和TCP Keep-alive有关的问题，不过我之前看一些建立网络隧道的软件里，有Keep-alive这个参数，查询说它和SNAT有关，就让我一直很迷惑。<br>\n&nbsp;<br>\n今天学习了老师的课，我又去深究了下。原来Virtual Tunnel会建立虚拟网卡，它和物理网卡之间需要做流量的桥接，进而就需要做SNAT。然后SNAT会维护一个端口映射表，因为链接太多可能会占满本地端口，如果没有设置Keep-alive，那么在一段时间内都没有传输数据的话，就会把端口转换的记录给删掉，这时候通信双方想再通信就不行了。所以需要每隔一段时间发送心跳包，保证SNAT端口映射表中的记录不被删掉，从而保证连接存活。</p>\n</blockquote><p>这实际上就是一个很典型的场景，也就是NAT和TCP Keep-alive之间的关系。NAT是维护连接跟踪表的，里面的表项有一定的有效期，过了有效期的表项就会被清空。而TCP Keep-alive正好可以定期去刷新这个计时器，让它“永葆青春”，表项不被删除，这个连接就可以一直愉快地工作下去了。</p><p>类似地，<strong>@Geek_535c45</strong>同学也对TCP Keep-alive问题发出了灵魂的拷问：</p><blockquote>\n<p>Chrome每隔45秒发起TCP Keep-alive包的意义是什么呢？</p>\n</blockquote><p>其实答案也跟上面的解释差不多，Chrome每隔45秒的TCP Keep-alive，主要是起到下面这两个作用：</p><ul>\n<li><strong>避免连接被中间环节给撤销掉</strong>。一般客户端是在内网里的，出公网的时候NAT会转换为公网IP跟对端通信。而跟上面的例子类似，为了保持NAT表项不被回收，所以Chrome要发送保活报文。</li>\n<li><strong>避免被服务端认为是无效连接给撤销掉</strong>。服务端一般也有idle timeout时间，也就是如果一个客户端连接超过一定时间没有报文传送，服务端就会取消这个连接，而且这时很可能不发送FIN或者RST，导致客户端对连接失效这个事情并不知情。因为服务端要面向公网上成千上万的访问者，所以它的逻辑是定时清除无效连接，以保证服务端的资源得到合理的使用。</li>\n</ul><h2>08讲的答疑</h2><h3>思考题</h3><ol>\n<li>在LB或者网关上修改MSS，虽然可以减小MSS，从而达到让通信成功这个目的，但是这个方案有没有什么劣势或者不足，也同样需要我们认真考量呢？你可以从运维和可用性的角度来思考。</li>\n<li>你有没有遇到过MTU引发的问题呢？</li>\n</ol><h3>答案</h3><p>对于第一个问题，在LB或者网关上修改MSS，这当然起到了作用。从短期来看，也是很不错的临时方案。但是长期来看，一旦客户和自己发生人员流动，又缺乏文档记录，就很可能会引起问题。比如说，这个配置可能会被后来的人当作无用配置而清除，而到时候如果没有人了解这个上下文，显然就要花很多时间去排查，给双方带去的业务影响也不会小。</p><p>尽管这可能算是一个非技术问题，但是也会影响到自己团队和兄弟团队的工作体验。所以我们做运维工作，还有一个重要的原则：<strong>优先选择简洁又容易维护的方案</strong>，而不是复杂且不易维护的方案，避免给以后的运维工作埋下隐患。</p><p>所以，后来我们选择的长期方案还是在两端修改MTU，这样的话客户团队就很清楚自己机器上的配置，就有责任也有条件做好维护。</p><p>第二个问题，还是<strong>@江山如画</strong>同学，他分享了一个典型案例：</p><blockquote>\n<p>之前我们手动创建了虚拟网卡，和物理网卡之间做了流量的桥接，发现有些报文在这二者之间转发时会被丢掉。然后通过分析，我们发现虚拟网卡的MTU设置过大，并且报文DF位设置为了1，后来我们通过ifconfig命令把虚拟网卡的MTU改小，报文就可以正常转发了。</p>\n</blockquote><p>不用我做更多解释，你应该都很清楚这个场景里的技术细节了。这也是比较常见的情况，你掌握好以后应该也有机会运用到实际工作中，也就是说，<strong>如果遇到丢包的问题，你可以看看MTU的值是否超标了</strong>。这是一个比较合算的“投资”，就花几分钟查一下，也许真能解决一个原本要查几个小时的大问题。</p><h2>09讲的答疑</h2><h3>思考题</h3><p>你有没有在工作中遇到过TCP传输速度相关的问题呢？通过这节课的学习，你已经掌握了传输速度相关的不少知识，你准备怎么运用这些知识，来解决这个传输问题呢？</p><h3>答案</h3><p>我看到留言中有一个同学的回答是这样的：</p><blockquote>\n<p>我们公司之前有一个场景是多个客户端连接同一个服务端，如果某个客户端下载文件或上传文件，占用了大量的带宽，就会导致新的客户端连不上服务端。后来我们就在服务端使用tc工具做了流控，根据客户端个数来均分带宽，还有限制单个IP的最大使用带宽。</p>\n</blockquote><p>其实， tc的原理是调整qdisc发送缓存队列，超出队列的数据就丢弃，这样的话发送端就探测到了“拥塞”，进而会主动降速，从而也就达到了限制速度的效果。而“拥塞”这个知识点，又是我们在<a href="https://time.geekbang.org/column/article/486281">第11讲</a>里深入探讨过的内容。其实从第9到13讲的内容都是关于传输的，所以我建议你可以把第9到13讲的内容结合起来学习，应该就能更加深入和全面地理解TCP传输这个重要话题了。</p><h2>10讲的答疑</h2><h3>思考题</h3><ol>\n<li>TCP的序列号和确认号，最大可以到多少？</li>\n<li>接收端只确认部分数据，导致了“数据滞留”现象，这个现象背后的原因可能是什么呢？</li>\n</ol><h3>答案</h3><p>第一个问题呢，其实是一个简单的<strong>协议规范的问题</strong>。这个知识点，你很容易就可以在<a href="https://www.ietf.org/archive/id/draft-ietf-tcpm-rfc793bis-25.html#section-3.10.7.4-2.1.2.1.9.7.2.1">RFC793</a>里找到。TCP的序列号和确认号的长度都是4个字节，4个字节也就是32位，所以最大值是2^32-1，也就是4G。我也建议你记住这些小的知识点，因为它是在工作中高频使用的知识点，你能记住的话就不用去搜索了，可以提升工作的效率。</p><p>第二个问题就比较复杂了，而且因为当时的现场没有了，所以也没有标准答案。但是这个思考的过程对我们掌握这一讲的知识是很有帮助的，而且，我们其实还是可以大致推导出当时的情景。</p><p>我直接借用<a href="https://time.geekbang.org/column/article/485689">第10讲</a>里面的一张示意图：</p><p><img src="https://static001.geekbang.org/resource/image/5d/e1/5daef13e8f38a14a86c45yyd28513ee1.jpg?wh=2000x872" alt=""></p><p>我也来简单描述一下这个过程。</p><ul>\n<li>在t1这个时间点，B通知A：“我的接收窗口是1000字节”。</li>\n<li>在t2这个时间点，A收到了B的通知，然后发送了1000字节，此时这些都属于在途字节数。</li>\n<li>在t3这个时间点，B收到了1000字节，此时接收窗口（缓冲区）全部用完，Wireshark据此判断出“Window Full”。然后B向A发送确认包，确认了300个字节。</li>\n<li>在t4这个时间点，A收到B确认300个字节的确认包，于是<strong>判断出在途字节数是700字节，所以A能发的最多只有300字节。</strong></li>\n</ul><p>我们再来看一下接收端的报文处理流程。如果我们把缓冲区（包括Ring Buffer和Recieve Buffer）比作水池，那么报文进入缓冲区，就相当于有水龙头往里注水；应用程序到缓冲区取走数据，又相当于有水龙头放出水。这两个水龙头的速度的不同，就造成了缓冲区的动态调整。</p><p><img src="https://static001.geekbang.org/resource/image/a4/f5/a483db4d1ffe854e660d4c4611cde0f5.jpg?wh=2000x778" alt=""></p><p>那么，发生数据“滞留”，其实就相当于水池中一直囤着水，也就是“放水速度不够”，应用程序没有把数据都及时读取走导致的。</p><p>显然，比较理想的情况是放水的速度大于等于注水的速度，这是传输速度最快的方式。</p><p>但是，我们也要认识到，操作系统设立缓冲区的一大原因，就是考虑到“注水”和“放水”的速度本来就是经常不相等的，所以用缓冲区这个“空间”，来抵消速度不同导致的“时间”上的矛盾。简单来说就是“<strong>用空间换时间</strong>”。而且，不仅是网络接收和发送的缓冲区如此，大部分缓冲区也都是“用空间换时间”原则的体现。</p><p>所以通过这个思考，你对TCP的缓冲区等概念，是否也有了新的认识了呢？</p><p>好了，今天的答疑就到这里。如果你还有什么疑问，同样可以回复到留言区，我们一同进步、成长。</p>',
        article_title: "答疑（二）| 第6~10讲思考题答案",
      },
      {
        title: "11 | 拥塞：TCP是如何探测到拥塞的？",
        id: 486281,
        content:
          '<p>你好，我是胜辉。</p><p>前面两节课，我们通过真实的案例，一起学习了TCP传输方面的知识，比如其中的核心概念：往返时间、接收窗口和发送窗口、在途字节数，还有推导出来的核心公式。</p><p>当然，在实际场景里，我们可以直接利用Wireshark的I/O Graph查看速度趋势图，这样最方便，并且有不同时段的速度，方便我们对整体状况做全面的评估。</p><p>不过，不知道你有没有发现，TCP传输的起始阶段，速度都是从低到高升上来的，很少有一上来就直接以最终速度运行的情形。其实，这个行为跟 <strong>TCP拥塞控制</strong>有着密切的关系。所以这节课，我会带你了解什么是拥塞窗口、TCP是如何检测和避开拥塞的。这样呢，以后你处理TCP拥塞相关的问题时候，就能有的放矢，做到有针对性的分析了。</p><p>好，让我们来看一个具体的案例吧。</p><h2>案例</h2><p>在公有云服务的时候，我们有个银行的客户，他们有一次需要跨机房拷贝一个大文件，也就是用SCP命令，把文件从公有云机房拷贝到他们的自建机房。但是客户发现速度比较慢，让我们看一下原因。</p><p><img src="https://static001.geekbang.org/resource/image/c8/5b/c88359c7b0889fe31436bd1424b0015b.jpg?wh=1686x539" alt=""></p><p>架构上说，客户自己的机房在上海，公有云上的资源则在北京。地理位置相差很远，而且机房所属的性质也完全不同，那两者如何通信呢？走公网的话不太安全，而且速度和质量没有保障。</p><!-- [[[read_end]]] --><p>如果你熟悉网络的话，可能知道可以在两个机房之间搭建VPN。而从可靠性角度考虑，客户选择了使用公有云的专线产品，也就是在上海自有机房和北京公有云之间，打通了专线。这样，客户在上海的自有机房，就可以直接以10.x.x.x这种内网地址，直达他们在北京公有云的资源，就好像真的在同一个内网一样。是不是挺酷的？</p><blockquote>\n<p>补充：上传的抓包文件我做了脱敏修改，所以IP也是随机值而不是10.x.x.x。</p>\n</blockquote><p>可是，他们在传输文件的时候，发现速度只有200KB/s左右，达不到购买的专线带宽值。正好他们也在传输过程做了tcpdump抓包，我们来看看这个抓包文件的具体情况。</p><blockquote>\n<p>抓包示例文件已上传至 <a href="https://gitee.com/steelvictor/network-analysis/tree/master/11">Gitee</a>，你可以用Wireshark打开这个文件，跟随我的分析步骤来同步学习。</p>\n</blockquote><p>跟我们在<a href="https://time.geekbang.org/column/article/484923">第9讲</a>和<a href="https://time.geekbang.org/column/article/485689">第10讲</a>学过的一样，我们可以用 <strong>I/O Graph</strong> 这个小工具来直观地看一下传输速度：</p><p><img src="https://static001.geekbang.org/resource/image/1c/cd/1c9eb4698d2fa4cf0fccac8ac2413ccd.jpg?wh=896x624" alt="图片"></p><p>从图上看，速度大体上在100~200KB/s之间浮动，有少数几个时段的速度在230KB/s左右。这是传输速度方面的整体概览。</p><p>另外就是要查看TCP传输过程中的一些行为了，这些行为没办法在I/O Graph上体现出来，但是它们很可能就是“因”，正是因为它们，才有了传输速度这个“果”。</p><p>现在课程快学到一半了，你应该对Expert Information很熟悉了吧？这次也不例外，我们来看一下Expert Information：</p><p><img src="https://static001.geekbang.org/resource/image/12/60/125d10e8d65429639yy87875a4e91360.jpg?wh=1818x296" alt="图片"></p><p>可见信息量还挺大的，包括了多种行为。</p><ul>\n<li>Warning级别有两种，分别是乱序（Out-of-Order）和前面报文未抓取的情形。这两者本质上都是乱序引起的现象。</li>\n<li>Note级别，一共有四种，分别是：\n<ul>\n<li><strong>Spurious重传</strong>：这是已经被确认过的数据再一次被重传。</li>\n<li><strong>快速重传</strong>：收到3次及以上次数的重复确认后，不等超时就做出的重传。</li>\n<li><strong>重传</strong>：超时计时器到点而触发的重传，这就是有名的超时重传。</li>\n<li><strong>重复确认</strong>：确认号重复的多个报文，重复确认是引发快速重传的原因。</li>\n</ul>\n</li>\n<li>Chat级别的 <strong>TCP Window update</strong>，这里主要是客户端（上海）向SSH服务端（北京）通告自己的接收窗口的变化，是比较正常的行为。</li>\n</ul><p>既然还是跟传输速度相关的话题，你应该还记得上节课刚深入讨论过的TCP Window Full了吧？但是这里为什么一条这样的信息都没有呢？</p><p>对于TCP传输来说，其速度大体上是窗口/往返时间。而这里的窗口，在不同情境下就有着不同的含义。我们用CW（Congestion Window）来指代自身的拥塞窗口，而用RW（Receive Window）指代对端的接收窗，那么：</p><ul>\n<li>当RW&lt;CW时，这里的“窗口”就是RW；</li>\n<li>当RW&gt;CW时，这里的“窗口”就是CW。</li>\n</ul><p>对于情况1，也就是对端的接收窗口小于自身拥塞窗口的情况，一般意味着传输过程中没有或者很少有“拥塞”发生，因而拥塞窗口能增长到较高的值。所以，传输速度的上限就是对端接收窗口值决定的。这样呢，也就容易在Wireshark里观察到TCP Window Full这样的现象。当然，也不是每次都一定有TCP Window Full，像<a href="https://time.geekbang.org/column/article/484923">第9讲</a>的案例就没有。</p><p>对于情况2，也就是对端的接收窗口大于自身拥塞窗口的情况，这一般意味着传输过程中遇到了“拥塞”，因而拥塞窗口进行了适配，也就是往下调整，这往往会使得拥塞窗口变得比较小。</p><p>事实上，我们在第9讲也已经初步介绍了上面这些知识。我做一下搬运工，同时也做一下美工，对第9讲的图补充了Wireshark可能解读出来的信息，供你参考。下次你在Wireshark看到TCP Window Full、Out-of-Order、或者retransmission时，就可以跟这里的图对应起来，协助你排查传输速度方面的问题了。</p><p><img src="https://static001.geekbang.org/resource/image/7e/a3/7edf726e8a4a678996ce79067e9700a3.jpg?wh=2000x695" alt=""></p><p>在当前这个案例里，因为乱序、重传等都有大几千个，非常多，所以我们初步判断，应该就是这些事件导致传输遇到了拥塞，也进而限制了传输速度。那么到这里，我们也就正式进入拥塞机制的学习了。我会给你概括其关键部分，也会结合案例里的抓包文件，来带你获得一个更加感性的认识。</p><h2>TCP拥塞控制</h2><p>为了应对错综复杂的互联网网络环境，TCP使用了<strong>拥塞控制机制</strong>来确保传输速度和稳定性。这里你也要注意，总的来说，拥塞控制主要是通信两端自己需要实现的功能，而途中的网络设备，比如交换机、路由器等等，除了可能会发出拥塞通知报文以外，其他时候它们只管转发报文，都是不会担负更多的拥塞控制的责任的。</p><p>TCP拥塞控制主要有四个重要阶段：</p><ul>\n<li>慢启动；</li>\n<li>拥塞避免；</li>\n<li>快速重传；</li>\n<li>快速恢复。</li>\n</ul><p>其中还包括拥塞窗口这个概念，接下来我给你逐一介绍一下。</p><h3>慢启动</h3><p>Slow Start，是指TCP传输的开始阶段是从一个相对低的速度（“慢”一词的由来）开始的。事实上，在这个阶段，拥塞窗口会以翻倍的方式增长，所以从增长过程来看，叫“快启动”也未尝不可。</p><p>具体来说，在这个阶段，每次TCP收到一个确认了数据的ACK，拥塞窗口就增加一个MSS。比如下面这样：</p><p><img src="https://static001.geekbang.org/resource/image/10/8c/107e4ef0ec736a9b71dbe5968efa768c.jpg?wh=1728x951" alt=""></p><p>不过这里的“确认了数据的ACK”怎么理解呢？</p><p>它说的是有确认数据的ACK报文，而不是重复的ACK报文。比如收到2个ACK但确认号一样，那第二个ACK就不是“确认了数据的ACK”了，拥塞窗口不会增加2个MSS，而是只增加1个MSS。</p><p>那么，这个过程什么时候终止呢？是下面两件事中有一件发生时：</p><ul>\n<li>遇到了拥塞；</li>\n<li>拥塞窗口增长到慢启动阈值。</li>\n</ul><h4>慢启动阈值</h4><p>慢启动阈值（也有人称之为慢启动门限），英文简称ssthresh。过了这个阈值，拥塞窗口的增长速度立刻就放缓了，变成了每过一个RTT，拥塞窗口就只增长一个MSS（此前是每个确认数据的ACK，增长一个MSS）。</p><p><img src="https://static001.geekbang.org/resource/image/b4/c9/b4a0028f02c61b1c8c1afeyya2cf44c9.jpg?wh=715x599" alt="图片"></p><p>比如上图的例子中，假设ICW是4个MSS，ssthresh是32个MSS。在慢启动阶段，经过一个RTT后，CW扩大为8个MSS，然后是16个MSS，32个MSS，以指数级上升。</p><p>那么到了这个阈值后，TCP就进入了<strong>拥塞避免阶段</strong>，每过一个RTT，拥塞窗口只增加一个MSS，于是在图上看，就又变成了一条平直的斜率比较低的直线了。</p><p><img src="https://static001.geekbang.org/resource/image/b2/f2/b2ebefaeb3070dfdffd7fe024b0dc6f2.jpg?wh=1663x963" alt=""></p><p>那么，如果拥塞窗口正好等于慢启动阈值，发送方应该选择继续慢启动过程（指数性增长），还是拥塞避免过程（线性增长）呢？ <a href="https://datatracker.ietf.org/doc/html/rfc5681">RFC5681</a> 的规定是“没有规定”，两种都可以。</p><h4>间隔确认</h4><p>这里有个情况必须要提一下。很多TCP实现里（比如Windows系统），确认报文是这样工作的：如果收到连续多个报文，确认报文是一个隔一个回复。也就是：</p><ul>\n<li>收到1、2，对2进行确认；</li>\n<li>收到3、4，对4进行确认。</li>\n</ul><p>比如就在这个案例里，我们很容易就发现有这种隔一个报文再确认的现象：</p><p><img src="https://static001.geekbang.org/resource/image/09/4d/090550397c591a01a4afeb7b1965354d.jpg?wh=920x184" alt="图片"></p><p>上图中，我们选中了17号报文，Wireshark自动找到了被它确认的16号报文，也在它的左边打上了一个小小的勾。当然，我们也可以通过对比NextSeq和ACK来找到这种关系。我在图中就用红框和箭头找到了这里的3对TCP确认关系。</p><p>这样的间隔ACK，可能会使得拥塞窗口的增长速度，比每次都ACK要更低一些。</p><h3>拥塞窗口</h3><p>Congestion Window，缩写是CWND，或者CW。拥塞窗口是不是操作系统全局统一的配置呢？其实这是比较常见的误解。拥塞窗口是每个连接分开维护的，比如同一个主机有两个TCP连接在传输数据的话，那么这两个连接就各自维护自己的拥塞窗口，比如一个很大而一个很小，都没有关系。</p><p>下图中，我用CW指代拥塞窗口，图中CW1到CW8都是各自不同、独立维护的拥塞窗口：</p><p><img src="https://static001.geekbang.org/resource/image/26/b1/267cfac4b58d69ef8a4e8568ca8d34b1.jpg?wh=2000x836" alt=""></p><p>这里还有一个子概念很重要，叫<strong>初始拥塞窗口</strong>，英文是Initial Congestion Window（或者Initial Window），缩写为ICW（或者IW）。</p><p>在Linux内核3.0以前，初始拥塞窗口的大小比较小，在2到4个MSS。2010年，谷歌<a href="https://datatracker.ietf.org/doc/html/draft-hkchu-tcpm-initcwnd-01">提出</a>，为了充分利用现代互联网的传输能力，Linux应该把ICW从2~4个MSS提升到10个MSS。这也被应用到了Linux内核3.0版本及以后的版本中，比如在include/net/tcp.h中，就定义了TCP_INIT_CWND的值为10。</p><pre><code class="language-plain">/* TCP initial congestion window as per rfc6928 */\n#define TCP_INIT_CWND       10\n</code></pre><p>前面刚介绍过，在慢启动阶段，每过一个RTT，拥塞窗口就翻倍。那么不同的ICW就会造成不同的传输速度，比如：</p><p><img src="https://static001.geekbang.org/resource/image/31/a2/3114816492a2e9d6da4074f9fd7541a2.jpg?wh=2000x591" alt=""></p><p>看着ICW的变迁，我真的就觉得很多知识是相通的。还记得我们在<a href="https://time.geekbang.org/column/article/479163">第3讲</a>学习TCP握手的时候，介绍的Window Scale概念吗？为什么已经有Window字段，设计者们还要创造Window Scale呢？本质原因还是互联网发展很快，原先设计的Window不够用了。</p><p>那么现在ICW的增加也是如此：既然互联网条件好了那么多，咱就不要过于谨慎了吧？天地大得很，上来就迈大点的步子，后面跑起来就快上加快了！</p><h3>拥塞避免</h3><p>前面我说过，传输过了慢启动阈值（ssthresh）之后，就进入了<strong>拥塞避免</strong>阶段。这个阶段的特征是“<strong>和性增长乘性降低</strong>”，英文是Addictive increase/mutiplicative decrease，缩写为AIMD。它也翻自英文，怪不得这中文念起来略有不顺，特别是“和性增长”。其实说是“佛系增长”也许更容易理解吧，因为增长很慢，挺佛系的。</p><p>那么，因为AIMD的关系，每一个RTT里，拥塞窗口只增长一个MSS，所以这个阶段的拥塞窗口的增长是线性的。直到探测到拥塞，然后拥塞窗口就要往下降。这个下降是直接减半的，所以叫<strong>乘性降低</strong>。我画了一个示意图，给你做参考：</p><p><img src="https://static001.geekbang.org/resource/image/1b/fc/1b8d403aace12f2742f6054078e273fc.jpg?wh=2000x1125" alt=""></p><p>当然，图中的第二个拥塞点比第一个低只是一种可能的情况，现实场景里什么情况都可能有，因为网络状况本身就是动态变化的。</p><h3>窗口和MSS的关系</h3><p>本来这也属于拥塞算法相关的知识点，但因为确实是比较常见的误区，所以我在这里单独拎出来介绍一下。首先，窗口一般比MSS大，而且大很多，可能会有个别同学以为“MSS就是窗口最大值”。其实这是反过来的：窗口值一般比MSS大很多，相当于就是MSS的某个倍数，比如2倍、10倍、50倍等等。</p><p>MSS是有确定上限的，我在前面课程里都多次提到过，MSS一般为1460，当然根据实际情况，也经常会有更低的值。比如，开启TCP timestamp等Option的话，肯定要相应地从1460字节里扣去这部分字节数，这样的话MSS就会低于1460，比如可能是1440字节。</p><p>你可以理解为：<strong>窗口就是n个MSS</strong>。</p><blockquote>\n<p>补充：确切来说，窗口的单位是字节数，所以也经常不是MSS的整数倍，这也都是正常的。</p>\n</blockquote><h3>快速重传</h3><p>TCP每发送一个报文，就启动一个超时计时器。如果在限定时间内没收到这个报文的确认，那么发送方就会认为，这个报文已经在网络上丢失了，于是需要重传这个报文，这种形式叫做<strong>超时重传</strong>。</p><p>一般来说，TCP的最小超时重传时间为200ms。这样的超时重传的机制虽然解决了丢包的问题，但也带来了一个新的问题：如果每次丢包都要等200ms或者更长时间，那应用不是就不能及时处理了吗？特别是对于有些时间敏感型的应用来说，影响更为严重。</p><p>所以，TCP会用另外一种方式来解决超时重传带来的时间空耗的问题，就是用<strong>快速重传</strong>。在这个机制里，一旦发送方收到3次重复确认（加上第一次确认就一共是4次），就不用等超时计时器了，直接重传这个报文。</p><h3>快速恢复</h3><p>这是TCP Reno算法引入的一个阶段，它是跟随快速重传一起工作的。跟之前的“慢启动-&gt;拥塞避免-&gt;慢启动-&gt;拥塞避免”这种做法不同的是，在遇到拥塞点之后，通过快速重传，就不再进入慢启动，<strong>而是从这个减半的拥塞窗口开始</strong>，保持跟拥塞避免一样的线性增长，直到遇到下一个拥塞点。你可以参考下面的图片来理解，橙色线就是快速恢复阶段：</p><p><img src="https://static001.geekbang.org/resource/image/34/54/34cf2de4yy170ayyc8cd107efeacbb54.jpg?wh=2000x1125" alt=""></p><p>那么，是不是有了快速重传和快速恢复，传输过程中都不用反复进入慢启动了呢？其实并不是这样。如果遇到超时，也一样要回到慢启动阶段，重新开始。</p><h2>回到案例</h2><p>好了，拥塞控制相关的知识点告一段落。这里，正好我们结合实际案例，来学习一下这部分知识。</p><p>在这次从北京传输文件到上海的过程中，有没有慢启动呢？要查看这个信息，用Wireshark的TCP Stream Graphs再合适不过了。我们打开Statistics菜单下的TCP Stream Graphs -&gt; Time Sequence (Stevens)，得到下图：</p><p><img src="https://static001.geekbang.org/resource/image/ec/9f/ece79da2cf0f158bb5d7f5a322732e9f.jpg?wh=849x767" alt="图片"></p><p>这里的斜率还是比较平稳的，说明虽然有很多乱序和重传，但整体速度还算稳定。当然，如果我们放大这条线，就能看到不同的景象了。比如放大到前10秒，明显曲线的波动幅度加大了不少：</p><p><img src="https://static001.geekbang.org/resource/image/f9/be/f99c0dd7032fda26942946d028e19abe.jpg?wh=853x765" alt="图片"></p><p>我们可以看到，在起点的时候，这条曲线的斜率是很高的，过了0.5秒以后开始平缓，然后到了2.5秒开始又陡峭了，然后大体上在循环着这个过程。这样看来，你可能会想：“陡的部分都是在慢启动吗？”</p><p>整体来说，确实可以这么理解。陡的部分是慢启动过程，所以斜率比较高；缓的部分是拥塞避免阶段，斜率比较低一些。</p><p>所以我们也发现，<strong>慢启动不是只在TCP连接启动时候发生，而是可能在传输过程中发生多次</strong>。前面的示意图里就显示，一旦拥塞避免阶段探测到了拥塞，TCP还是会回到慢启动过程，只是这次的慢启动阈值跟之前的不同。然后如果多次遇到拥塞，就会重复这个过程，直到传输结束。</p><p>而要查看最初的慢启动的过程（最初的慢启动也称为冷启动，Cold Start），还得继续放大。比如下图中，我们在0~0.4秒区间内，才终于明显找到它了。</p><p><img src="https://static001.geekbang.org/resource/image/9c/30/9cf144995d235700b3546a31f3341930.jpg?wh=791x798" alt="图片"></p><p>这里我用红色方框标注出了每一次往返时间（RTT），那么在每个RTT内，22端口都连续发送了很多报文给到客户端，每次报文的序列号都是图上的一个点。显然，每个RTT后，发送的报文数量，都比前一轮RTT里发送的更多。</p><p>那你可能会有疑问：为什么这里的慢启动，并没有严格按照“翻倍”这样的原则来进行，比如说第二个红框的高度应该是第一个红框高度的两倍，但为什么图上并不是这样呢？</p><p>我个人的理解是这样的：在慢启动阶段，并不一定是“每次RTT就翻倍”，也可能会比翻倍低一些。这是因为，<strong>在慢启动阶段，TCP每收到一个ACK，拥塞窗口就增加一个MSS</strong>。假设初始拥塞窗口是2 MSS，发送2个数据报文后：</p><ul>\n<li>收到1个ACK（也就是间隔确认），那么在这次RTT内，拥塞窗口就变成了3 MSS，这是“不翻倍”的。</li>\n<li>收到2个ACK，那么在这次RTT内，拥塞窗口就变成了4 MSS，这是“翻倍”的。</li>\n</ul><p>然后我们再看一下拥塞。在这个Graph中，也明显有几个点出现了异常。因为Y轴代表序列号，那么这些低处的点位，<strong>它们的序列号自然就是比前一个更低（也就是更小）的值，说明这些是老的报文再次发了出去，所以可以判断为是重传</strong>。</p><p><img src="https://static001.geekbang.org/resource/image/b1/ea/b1f5314f1616ffb8b04497f9e0933eea.jpg?wh=789x658" alt="图片"></p><p>比如我们选中最下面这个点。这里的点都比较小，定位起来稍麻烦一些。你如果没有一点外科医生的手艺，那还是多一些耐心吧，慢慢移动鼠标才能定位到它。</p><p><img src="https://static001.geekbang.org/resource/image/2d/b2/2dbce0ed33a895dd6b327938a43d50b2.jpg?wh=1153x889" alt="图片"></p><p>定位到这个点之后，发现它是201号报文。在主窗口里，我们可以看到，201号报文是一个TCP Fast Retransmission，也就是快速重传报文。那么通过这个重传，TCP拥塞控制机制就感知到了拥塞，然后进入了拥塞避免阶段。</p><p>好了，拥塞的知识点介绍得差不多了，案例也快结束了。那么案例的结论又是什么呢？</p><p>其实就是<strong>专线上的限速设置失误</strong>造成的。这个限速应该是通过网络硬件设备完成的，它单位时间内只允许一定字节数的报文通过，如果超过限制，就会丢弃这些报文。</p><p>现在我们学习了TCP拥塞控制机制，应该已经明白了：丢包对于发送端来说就是“拥塞”，然后它就根据拥塞控制机制，主动进入拥塞避免阶段，以确保传输速度，不至于大面积丢包。事实上就自动降低了速率，达到了我们想要的“限速”的效果。</p><p>我们的同事把这个限速设置值给搞错了，并不是客户购买的那个带宽值。修正之后就解决了问题。</p><p>虽然说这个排查很简单，不过通过这些抓包文件的拆解分析，还是可以对我们理解TCP拥塞控制的机制，有很大的帮助。</p><h2>实验一下</h2><p>拥塞控制机制对TCP传输十分重要，技术细节也很复杂，所以是由内核实现的。这也是内核实现TCP栈的巨大优势：应用程序可以集中于业务逻辑，而不需要操心传输和拥塞这种底层细节了。</p><p>那么，这是不是意味着，TCP拥塞这个东西有点“高冷”，咱们也就看看，评论一下，好像啥都做不了？</p><p>并不是，接下来的实验，就是可以改变一些拥塞控制行为的。</p><h3>实验1：修改初始拥塞窗口</h3><p>有些时候，我们也有修改拥塞行为的需求。比如，修改初始拥塞窗口（ICW）。这样，当你认为某条链路网络状况比较糟糕，用更低的ICW更合理时，就可以这么做。</p><p>在Linux操作系统上，修改初始拥塞窗口的方法是这样的：</p><ul>\n<li>运行<code>ip route</code>命令，找到当前的路由条目，把整行都进行复制，记为item：</li>\n</ul><pre><code class="language-plain">$ ip route\ndefault via 10.0.2.2 dev enp0s3 proto dhcp src 10.0.2.15 metric 100\n</code></pre><ul>\n<li>运行<code>ip route change item initcwnd n</code>，把路由项item的初始拥塞窗口修改为n，比如改为2：</li>\n</ul><pre><code class="language-plain">$sudo ip route change default via 10.0.2.2 dev enp0s3 proto dhcp src 10.0.2.15 metric 100 initcwnd 2\n</code></pre><h3>实验2：改变TCP拥塞控制算法</h3><p>有时候，我们还需要调整TCP拥塞控制算法。在Linux里，我们可以通过 <strong>sysctl命令</strong>，查看或者修改这个算法。比如，Linux默认是用cubic算法，那你可以运行下面这条命令，看看是不是这样？</p><pre><code class="language-plain">$ sysctl net.ipv4.tcp_congestion_control\nnet.ipv4.tcp_congestion_control = cubic\n</code></pre><p>那如果你想用最新的BBR算法，该怎么做呢？如果内核大于4.9，Linux里就已经默认带有BBR了。执行下面的命令即可：</p><pre><code class="language-plain">$ sudo sysctl net.ipv4.tcp_congestion_control=bbr #配置拥塞算法为BBR\nnet.ipv4.tcp_congestion_control = bbr\n$ sudo sysctl net.core.default_qdisc=fq           #调整缓存队列算法\nnet.core.default_qdisc = fq\n</code></pre><blockquote>\n<p>补充：把这些配置写入到/etc/sysctl.conf，即使机器重启，配置也会保持不变。</p>\n</blockquote><p>TCP BBR拥塞控制算法是Google于2016年提出的新的算法。它的开发背景是，当今网络设备的缓存越来越大，导致丢包这个行为不像以前缓存小的时代那么频繁，但是报文延迟的问题比以前严重了。所以，要更加准确地探测拥塞，我们应该更多地关注延迟，并基于延迟的变化作出拥塞窗口的调整。</p><h2>小结</h2><p>这节课，我们学习了TCP传输中非常核心的一块内容：拥塞控制。拥塞控制的实现，主要依靠这几个环节。</p><ul>\n<li><strong>慢启动</strong>：每收到一个ACK，拥塞窗口（CW）增加一个MSS。</li>\n<li><strong>拥塞避免</strong>：策略是“和性增长乘性降低”，每一个RTT，CW增加一个MSS。</li>\n<li><strong>快速重传</strong>：接收到3次或者以上的重复确认后，直接重传这个丢失的报文。</li>\n<li><strong>快速恢复</strong>：结合快速重传，在遇到拥塞点后，跳过慢启动阶段，进入线性增长。</li>\n</ul><p>另外，我们也复习了拥塞窗口（CW）和接收窗口（RW）是如何决定了传输速度上限的，简单来说：</p><ul>\n<li>当RW&lt;CW时，速度由RW决定；</li>\n<li>当RW&gt;CW时，速度由CW决定。</li>\n</ul><p>然后，我们也知道了拥塞窗口（CW）是每条连接分开各自维护的，以及初始拥塞窗口（ICW）的概念，并且知道，从Linux 3.0内核开始，ICW已经提升到10个MSS。</p><p>在Wireshark使用技巧上，你也要清楚如何用 <strong>TCP Stream Graphs</strong> 的Time sequence (Stevens)小工具，来观察慢启动和拥塞避免等现象，包括其中发生的快速重传等行为，都可以在图上看到，这非常有利于你的排查工作。</p><p>除此之外，我们也可以对拥塞控制做一些调整：</p><ul>\n<li>用<code>ip route change</code>命令，调整某个网卡接口的初始拥塞窗口。</li>\n<li>用<code>sysctl net.ipv4.tcp_congestion_control</code>命令，查看和修改内核使用的拥塞控制算法。</li>\n</ul><h2>思考题</h2><p>你在工作中有没有遇到拥塞引起的问题，或者有没有在抓包分析过程中，观察到过拥塞现象呢？欢迎在留言区分享你的经验，我们一同成长、进步。</p><h2>附件</h2><p>抓包示例文件：<a href="https://gitee.com/steelvictor/network-analysis/tree/master/11">https://gitee.com/steelvictor/network-analysis/tree/master/11</a></p>',
        article_title: "11 | 拥塞：TCP是如何探测到拥塞的？",
      },
      {
        title: "12 | 重传的认识：重传到底是怎么回事？",
        id: 487082,
        content:
          '<p>你好，我是胜辉。</p><p>在前面的<a href="https://time.geekbang.org/column/article/484667">第8讲</a>和<a href="https://time.geekbang.org/column/article/484923">第9讲</a>，我们先后介绍了两个TCP传输方面的案例。在刚过去的<a href="https://time.geekbang.org/column/article/486281">第11讲</a>，我们更是全面了解了TCP的拥塞控制机制。其中有一个词经常被提到，就是“重传”。</p><p>在我看来，TCP最核心的价值，如果说只有一个的话，那就是<strong>对可靠传输的保证</strong>。而要实现可靠的传输，可能需要这样做：如果我的报文丢了，应该在一定次数内持续尝试，直到传输完成；而如果这些重传都失败了，那就及时放弃传输，避免陷入死循环。</p><p>所以，为了应对不同的情况，TCP又发展出了两种不同的重传类型：<strong>超时重传</strong>和<strong>快速重传</strong>。它们在各自的场景下都有不可替代的作用。不过，它们本身也只是外在的表现，触发它们的条件又分别是什么呢？</p><p>另外，你可能在Wireshark里也见过Spurious retransmission，这个又是什么意思，会对传输有什么影响吗？</p><p>这节课，我就通过对几个案例中的抓包文件的解读，带你学习这些重传家族的成员，了解它们的性格脾气，以后你在日常网络排查中看到重传，也就能顺利搞定了。</p><h2>超时重传</h2><p>我们先来学习下超时重传，Timeout Retransmission。在TCP传输中，以下两种情况，都可能会导致发送方收不到确认：</p><ul>\n<li>报文在发送途中丢失，没有到达接收方，那接收方也不会回复确认包。</li>\n<li>报文到达接收方，接收方也回复了确认，但确认包在途中丢失。</li>\n</ul><!-- [[[read_end]]] --><p><img src="https://static001.geekbang.org/resource/image/71/6f/71e453a2e8690941b9198bcb192e8d6f.jpg?wh=2000x979" alt=""></p><p>没有收到确认怎么办？发送方为了避免自己陷入“尬等”的境地，选择在等待某段时间后重新发送同样这份报文，这个等待的时间就是<strong>重传超时</strong>，Retransmission Timeout，简称RTO。这个Timeout其实是基于一个计时器，在报文发送出去后就开始计时，在时限内对方回复ACK的话，计时器就清零；而如果达到时限对方还没回复ACK的话，重传操作就被触发。</p><p>当然，超时重传也还是可能会丢包，此时发送方一般会以RTO为基数的2倍、4倍、8倍等时间倍数去尝试多次。</p><p>我们来看一个例子，熟悉一下这种重传。</p><h3>超时重传案例</h3><p>有一次，我们一个客户访问HTTPS站点的服务，时常报错失败，我们就做了抓包。这个问题的原因已经不重要了，但是这个抓包文件倒是很适合用来给我们学习TCP重传。</p><p>我们直接选一个典型的失败事务的TCP流来看一下：</p><p><img src="https://static001.geekbang.org/resource/image/e7/c8/e785dd65d5649f3aa7c0320900be5dc8.jpg?wh=1212x470" alt="图片"></p><blockquote>\n<p>示例文件已经上传至<a href="https://gitee.com/steelvictor/network-analysis/tree/master/12">Gitee</a>，建议你结合示例文件和文稿来学习，效果更好。</p>\n</blockquote><p>显然，图中黑底红字的报文就是一系列的重传，而且都是超时重传。你如果仔细看了这个文件，可能会指出：“老师不对，这里的12号报文是Spurious重传，不是超时重传”。但是听完我后面的分析，你应该会同意我的观点：这个本质上也是超时重传。</p><p>好，我们开始分析。</p><ul>\n<li>第一阶段：1~3号报文是TCP握手。</li>\n<li>第二阶段：4~9号报文是TLS握手。</li>\n<li>第三阶段：11~20号报文是连续重传，以及夹杂的DupAck和Spurious重传。</li>\n<li>第四阶段：连接关闭。它的触发点是21号报文这个TLS Alert消息，它的类型是21。要知道它的具体报错信息，需要解密才能知道（在第20讲我会介绍TLS解密的细节）。不过通常来说，在这种TLS Alert消息之后，就是TCP挥手了。</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/33/c4/3352ae9d3ca7ddc638b04dcea61491c4.jpg?wh=1920x747" alt="图片"></p><p>我们重点关注下第三阶段，也就是11~20号报文这些重传。那么问题来了：这些重传都是重传了谁呢？也就是如何找到原始报文呢？</p><p>方法就是：<strong>先找到重传报文的序列号，然后到前面找到同样这个序列号的报文</strong>。那个就是原始报文。</p><p>比如，11号报文是Wireshark提示我们的第一个重传报文，我们看到它的序列号是272。在11号报文前面序列号同为272的，是10号报文。那么显然，11就是10的重传，后面的14到20号报文也是如此。</p><p><img src="https://static001.geekbang.org/resource/image/1a/17/1ab83ea403c9325652688f287349e617.jpg?wh=1920x403" alt="图片"></p><p>这里还有一个我们熟悉的现象。因为这一系列的重传是对同一个原始报文的重传，所以它们的发送时间也遵循了“<a href="https://en.wikipedia.org/wiki/Exponential_backoff">指数退避</a>”的原则，比如：</p><ul>\n<li>#11和#10隔了233ms</li>\n<li>#14和#11隔了467ms</li>\n<li>#15和#14隔了936ms</li>\n<li>……</li>\n</ul><p>到20号报文的时候，已经重传了第8次，也是最后一次。那么为什么没有第9次呢？有下面这两种可能：</p><ul>\n<li>客户端本来就最多只重传8次，所以后续也不再重传。</li>\n<li>服务端的TLS Alert报文过来，并发起了挥手，这样后续也没机会重传了。</li>\n</ul><p>然后我们再来理解一下整个的传输过程。不过因为抓包文件是单侧的，所以你要注意，<strong>Wireshark里的信息，是需要跟你“抓包发生在哪一侧”这个信息，结合起来解读的</strong>。</p><p>我画了下面这张图，供你参考这个过程。请注意，这是从抓包视角（也就是客户端）来解读的。如果服务端有抓包，很可能是不同的景象。比如，有可能服务端确实收到了这些重传报文，但确认报文一直没能成功传过来，这个可能性也是存在的。那样的话，这张图就会非常不同了。</p><p><img src="https://static001.geekbang.org/resource/image/01/56/01fe829f2b84f4a1f0167452c4a77f56.jpg?wh=1971x1125" alt=""></p><p>12号报文的序列号是1200，跟9号报文相同，而且由于客户端9号报文已经收到并且回复了确认，所以Wireshark认为它是<strong>Spurious重传</strong>。这又是什么重传呢？</p><h3>Spurious重传</h3><p>Wireshark如果识别到某个报文已经被确认过，但又再次发送，那么这次就是Spurious重传。简单来说，就是已经成功了，不需要再传。</p><p>这可以发生在两个方向上。具体来说，假设两端分别是A和B，我们在A端抓包，发现下面任何一种情况，Wireshark就会标记X为Spurious重传：</p><ul>\n<li>A发送了报文X，B回复了确认，A再次发送X。</li>\n<li>A收到了B发过来的报文X，A也回复了确认，但B再次发送X。</li>\n</ul><p>那么我们在Wireshark里看到这种Spurious重传该怎么办呢？我觉得一般不用特别处理，集中关注超时重传和快速重传就好了。</p><blockquote>\n<p>补充：这个建议也是基于概率。Spurious重传大部分时候不是问题，只有极少数情况下是问题，所以不去重点关注它是“划算的”。相关的案例，会在专栏的后半程里介绍。</p>\n</blockquote><p>不过我们仔细观察报文，还是发现了一些问题。10号报文的确认号是1251，也就是9号报文的下个序列号（1251=序列号1200+载荷长度51），所以10号报文就是客户端对9号的确认报文。那么，9号既然已经被确认了，为什么还要12号报文这个重传呢？</p><p>只有一种解释：<strong>这个抓包是在客户端做的，所以看不到服务端的情况</strong>。这个10号报文一定是没有到达服务端，所以后者认为9号未被客户端收到，于是在234ms后重传了9号的副本：12号报文。这里的234ms，就是11号报文的233ms加上12号报文的1ms。</p><p>从发送端（客户端）的抓包来看，是收到了Spurious重传：</p><p><img src="https://static001.geekbang.org/resource/image/df/10/df3d94a038aaf30ba643ac33b3db1910.jpg?wh=1920x406" alt="图片"></p><p>但从接收端（服务端）来看，我推测是认为9号丢失，所以200多ms后进行超时重传：</p><p><img src="https://static001.geekbang.org/resource/image/96/80/96fef7102d6cc81e0f969c1ddd31d880.jpg?wh=2000x720" alt=""></p><p>另外，你会发现10号报文本身也携带数据，它对9号报文的确认信息是跟着10号报文自己的数据一起过来的。反正确认信息只是一份元数据，不占用额外的空间，那么跟随数据报文一起发送是最高效的。</p><p>实际的重传的例子我们解读完了，接下来了解一下我们可能最关心的问题。</p><h3>重传超时究竟是多长呢？</h3><p><a href="https://datatracker.ietf.org/doc/html/rfc6298">RFC6298</a>规定：<strong>在一条TCP连接刚刚开始，还没有收到任何回复的时候，这时的超时RTO为1秒</strong>。在更早以前的规范里，这个值是3秒。你可以参考RFC6298的<a href="https://datatracker.ietf.org/doc/html/rfc6298#appendix-A">这个部分</a>，了解这个改变的来龙去脉。</p><p><strong>在连接成功建立后，Linux会根据RTT的实际情况，动态计算出RTO。</strong>实际场景中，RTO为200ms出头最为常见。</p><p>而且，<strong>RTO有上限值和下限值</strong>（仿佛有语音：“我们不是没有下限的~”）。一般情况下，<strong>Linux的这两个值分别是120秒和200毫秒</strong>。那么这个能否修改呢？</p><p>你可能想起了sysctl命令。但是很可惜，这两个值不能像sysctl那样调整，好像不太方便？其实，这也是一种“幸运”，操作系统把一些比较敏感、改错后影响比较大的参数，没有做成可以灵活调整的方式，也可以避免我们随便调整引发问题。</p><h2>快速重传</h2><p>上面的超时重传虽然避免了“干等”的尴尬局面，但不可避免地带来了另外的问题：“干等”的时间还是不短的，这段时间被白白浪费了。快速重传的出现就是为了解决这个问题。</p><p>它的思路是这样的：<strong>如果对端回复连续3个DupAck即重复确认，我就把序列号等于这个ACK号的包重传。</strong></p><h3>快速重传案例</h3><p>我在公有云工作的时候，有个客户对我们机房的网络可用性进行测试，结果发现测试情况不容乐观，很多HTTP请求没有得到及时回复。因为是相对简单的HTTP请求，本来期望在几个RTT之内就得到HTTP响应的，但实际上很多次都是超过了1秒。</p><p>于是我们做了抓包，然后过滤出了有问题的TCP流。我们看一下这条流的专家信息（Expert Information）：</p><p><img src="https://static001.geekbang.org/resource/image/fb/ff/fb8eb4b226bfa64d17667289d139bbff.jpg?wh=1820x342" alt="图片"></p><p>这里我选几个值得关注的信息，做一下解读。</p><p>Error级别的一条信息，是<strong>New fragment overlaps old data (retransmission?)</strong>。这是说，这个报文跟前面的报文有重合。这里的“重合”如何理解呢？比如，前面的报文是字节100到200，新的报文是字节150到250，那么两者在150到200字节之间就是重合的。这也不是很大的问题。</p><p>Note级别的3类信息，分别是：</p><ul>\n<li><strong>This frame is a (suspected) fast retransmission</strong>：这是快速重传报文，那为什么还要加个suspected字样呢？我的理解是，Wireshark是根据一些条件来综合判断这个报文属于什么类型的，但这仅仅是一种参考的信息。由于TCP报文本身没有表示重传的字段，所以Wireshark对它的解读只能作为参考，所以是suspected。其实，这个信息一般都比较准确，很少有错的时候，用suspected这个词，更多地体现了Wireshark开发人员的谦虚和严谨。</li>\n<li><strong>This frame is a (suspected) retransmission</strong>：这里就是超时重传了，Wireshark发现抓包文件中没有相关的DupAck，就推断出这个是超时重传。</li>\n<li><strong>Duplicate ACK (#1)</strong>：这里有28个重复确认报文，而且，如果我们点开的话，会发现这28个DupAck指向的都是同一个报文：56号报文。</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/a1/ea/a1c4452a6541b632ee4162e6be316bea.jpg?wh=1272x1060" alt="图片"></p><p><strong>记住：DupAck经常跟快速重传相关。因为有3个或以上数量的DupAck，就可以触发快速重传</strong>。</p><p>那么问题来了：为什么这里会有28个DupAck呢，不是有3个就足够触发了吗？</p><p>我们回到主界面来分析，你可以参考下图：</p><p><img src="https://static001.geekbang.org/resource/image/63/d5/63dd2ca0a046487c059d54020d9a05d5.png?wh=1066x755" alt="图片"></p><p>报文58~60是3个DupAck，显然也直接触发了61号快速重传报文。这样不是已经重传了吗？为什么还会有后面连续25个之多的DupAck呢？</p><p>答案可能不在这片区域。我们把视线往上挪，看一下57号及之前的报文情况。</p><p><img src="https://static001.geekbang.org/resource/image/bc/8d/bc1b274905bd14f582f3b59dd345318d.png?wh=910x750" alt="图片"></p><p>我们可以看到，57号报文之前，整个传输就像一片宁静的草原。然而，在风平浪静的表面之下，能找到我们要的答案吗？</p><p>61号快速重传报文的原始报文是哪个呢？不难找到，它就是32号报文，因为<strong>它的序列号就是42061，也就是连续DupAck的确认号</strong>。</p><p>正因为抓包在服务端抓取，所以一定能抓取到由这个服务端发出的报文，但是对端有没有收到，我们是看不到的。只有当3个的DupAck到达的时候，我们才知道这个报文一定是丢失了，然后会快速重传。</p><p>我直接给你揭晓答案：就是因为从32号报文之后，服务端还继续发送了14个数据报文，远不止3个，所以触发的DupAck也远不止是3个。你可以直接看下图来理解这里的逻辑：</p><p><img src="https://static001.geekbang.org/resource/image/43/0d/4343074e10dc89634c8f3540ac72ba0d.jpg?wh=1843x1035" alt=""></p><p>不过，你仔细看这个抓包文件的话，可能还是发现了一个漏洞：服务端从32号报文之后，发送的数据报文一共是14个，为啥客户端要回复的DupAck有28个呢？这数量对不上，老师你是不是搞错了？</p><p>我在解读这个抓包的时候，其实也在这里卡住过，确实是个有点烧脑的问题：逻辑都对，就是数量不对，到底哪里出了问题？</p><p><img src="https://static001.geekbang.org/resource/image/f8/f6/f81e98de0b231a5f6c1d9f2d0561cef6.jpg?wh=1528x965" alt=""></p><p>这种时候，你会怎么做呢？</p><ul>\n<li>宽慰自己：“应该有什么别的原因吧，就不追究了，这个小问题不影响大方向。”</li>\n<li>鼓励自己：“真相只有一个！再查一下。”</li>\n</ul><p>其实，你如果是连续学习课程过来的，应该会对TSO有印象。这是我们在<a href="https://time.geekbang.org/column/article/484667">第8讲</a>提到的概念。有了TSO，操作系统就可以把大于MSS的TCP段，比如2个MSS或更大尺寸的段，交给网卡驱动来处理，后者会利用其硬件芯片做分段工作，重新组装成新的符合MTU的报文后发送出去。</p><p>那么，“为什么14个报文触发了28个DupAck”的答案就在这里了：</p><p><img src="https://static001.geekbang.org/resource/image/c2/d4/c2a683b06747d39f92e6f9639e6663d4.jpg?wh=910x511" alt="图片"></p><p>这14个报文，每个都是2804字节大小，这就显然是TSO在起作用了。服务端把2804字节发给网卡后，网卡拆分为2个报文后发出，所以14个数据报文，实际到达接收端的时候就是28个报文！</p><p><img src="https://static001.geekbang.org/resource/image/0c/6f/0c41957d3144dcb5a24c941dee64be6f.jpg?wh=1709x489" alt=""></p><blockquote>\n<p>补充：实际TSO工作起来比这个图要复杂。为了突出重点，这个图里并没有展示TCP头部和IP头部的封装工作。</p>\n</blockquote><p><img src="https://static001.geekbang.org/resource/image/f3/cf/f397d9e75c93facc2fyy4ae6d59bd7cf.jpg?wh=1481x950" alt=""></p><p>因为tcpdump在内核里靠近网卡这一侧，所以tcpdump抓取到的还是TSO处理之前的大报文，只有到达了网卡并且被TSO机制做了分段处理后，才变成小报文。我们抓包文件里看到14个报文，实际发送出去的是28个报文，也就触发了28次DupAck。</p><p>遇到难题，努力一下，往往就有新的发现。让我们每天进步一点点。</p><h2>SACK跟重传的关系</h2><p>其实在第8讲“MTU引发的血案”里，我们就发现了SACK现象。这次我们研究重传，那就有必要回顾一下这个SACK部分，把它的含义和作用，都搞清楚。</p><p>在那次案例里，服务端向客户端发送了3个HTTP响应报文，但是因为MTU的问题，其中一个大的报文在路径上丢失了，只有后面2个报文到达了客户端，从而引发了客户端发送了两次DupAck。</p><blockquote>\n<p>示例文件已经上传至<a href="https://gitee.com/steelvictor/network-analysis/blob/master/12/SACK.pcap">Gitee</a>，建议结合示例文件和文稿来深入学习。</p>\n</blockquote><p>我们来看看这两个DupAck的详情，先看第一个：</p><p><img src="https://static001.geekbang.org/resource/image/d3/56/d37ef038ac854b213495d345a71e8556.jpg?wh=1920x1213" alt="图片"></p><p>你可以直接进入关键部分，也就是这里框出来的TCP Option SACK 1389-2077。<strong>SACK全称是Selective Acknowlegement，中文叫“选择性确认”</strong>。但是在中文里，貌似带“选择性”这个前缀的词都不是很正面，比如像“选择性忽视”“选择性失忆”什么的，我们好像都不想摊上这种事。</p><p>不过，在TCP的世界里，“选择性确认”这个概念就很不一样了，它还真的是我们实实在在需要的一个特性，能帮助TCP运作得更好。</p><p>我们先说说没有SACK会怎么样。在前面的五个报文的例子里，接收端在TCP里，我们只能对收到的连续报文进行确认。我们可以看个例子：</p><p><img src="https://static001.geekbang.org/resource/image/af/c9/af653ea11041d94c418d24a98c4f2cc9.jpg?wh=2000x951" alt=""><br>\n<img src="https://static001.geekbang.org/resource/image/27/31/272b1714ab31190eaa6f2da1c9fa6331.jpg?wh=1622x1051" alt=""></p><p>接收端回复的报文的确认号，只能是连续字节数的最后一个位置。因为发送端的序列号为201、长度为100的TCP报文丢失了，那么服务端收到的连续字节数的最后一个位置，就是第201字节。这还不是最糟糕的，后续发送过来的序列号为301和401这两个报文，服务端回复的确认报文的确认号也<strong>仍然只能是201</strong>。</p><p>现在，有3个确认报文的确认号为201。对于早期TCP实现来说，这个时候发送端只能把序列号从201开始的报文，也就是序列号分别为201、301、401的这3个报文全部重传。但是，301和401报文实际已经到达接收端，却也要跟着201一起被重传，这未免太浪费了，301和401号报文表示“201真是我们的猪队友”。</p><p>那有没有办法，只重传序列号为201的报文，而避免重传301和401呢？</p><p>于是，TCP增加了SACK这个特性。SACK机制可以告诉发送端：“虽然我的确认号是201，但是我的TCP Option里面还有更详细的信息，在那里我会告诉你，在断点后我又收到了哪些数据”。这段话比较晦涩，我们直接看这次的这个报文：</p><p><img src="https://static001.geekbang.org/resource/image/49/a9/49bcae74a56d2db4aed257cc4bd1e9a9.jpg?wh=758x222" alt="图片"></p><p>SACK最关键的部分就是 <strong>left edge和 right edge</strong>，也就是左右边界。上图告诉我们：我收到了从第1389字节开始直到2076字节（也就是不含2077）的数据。这样，我们可以把SACK和确认号结合起来，知道了通过这个报文，接收端（这里是客户端）明白了什么样的信息。我用示意图把这个信息表示了出来：</p><p><img src="https://static001.geekbang.org/resource/image/47/e4/47f91621fa7aa6de9fdb9be953ddd2e4.jpg?wh=2000x471" alt=""></p><p>类似的，我们看一下第二个重复确认报文（<a href="https://gitee.com/steelvictor/network-analysis/blob/master/12/SACK.pcap">示例文件</a>的10号报文）的SACK详情：</p><p><img src="https://static001.geekbang.org/resource/image/fe/fb/fe6eda6c07f74efafef0bb14a06d6bfb.jpg?wh=770x220" alt="图片"></p><p>可见，第二个重复确认报文的SACK，把实际收到的报文边界又往右“推”了一些，到了第2134字节（之前是第2077字节）。这个差额是2134-2077=57。</p><p><img src="https://static001.geekbang.org/resource/image/9c/7f/9ccffc7bf2bd185299633b694857d77f.jpg?wh=2000x545" alt=""></p><p>你有没有发现，这57个字节，其实正是前面抓包文件截图里的8号报文的大小，也就是说这57字节的报文确实被服务端收到了，并在随后回复的SACK报文中得以体现。这里我们再看一眼这57字节的报文在抓包文件中的位置：</p><p><img src="https://static001.geekbang.org/resource/image/19/71/191ab7df2b07697956a3f486165dd471.jpg?wh=934x290" alt="图片"></p><p>既然SACK这么好，是不是TCP传输都能用上它呢？其实，<strong>SACK要能工作，还需要SACK permitted这个TCP扩展属性的支持</strong>。这个字段只有在TCP握手的SYN和SYN+ACK报文中出现，表示自己是否支持SACK特性。比如下图：</p><p><img src="https://static001.geekbang.org/resource/image/4b/d2/4bba1dedd65f20966a820a1dcab8f9d2.jpg?wh=1174x324" alt="图片"></p><p>知道对方支持SACK，那我们就可以在后续报文里带上SACK，也就是上面的含有left edge和right edge，来告诉对方我实际收到的TCP段了，就可以避免这部分报文也被连带重传。201报文表示：还好有了SACK，队友们你们继续往前冲吧，我虽然这次掉队了，但我还会回来的。</p><p>那么有了SACK，是不是所有的这种零星到达的报文，都不用重传了呢？答案是没有那么乐观。受限于TCP Option长度，<strong>SACK部分最多只能容纳4个块</strong>。当然，这比没有SACK的情况还是好多了。</p><p><img src="https://static001.geekbang.org/resource/image/67/29/67c5546e23639f2dfa276d0289c84129.jpg?wh=2000x711" alt=""></p><h2>小结</h2><p>这节课，我们学习了TCP重传的两种类型：超时重传和快速重传。然后也通过实际案例，看到了这两种重传在实际情况中的特点。这里我再给你小结一下这些知识点，你可要好好掌握。</p><p>对于超时重传：</p><ul>\n<li>TCP对于每条连接都维护了一个超时计时器，当数据发送出去后一定时限内还没有收到确认，就认为是发生了超时，然后重传这部分数据。</li>\n<li>RTO的初始值是1秒（在发送SYN但未收到SYN+ACK阶段）。</li>\n<li>在连接建立后，TCP会动态计算出RTO。</li>\n<li>RTO有上限值和下限值，常见值分别为2分钟和200ms。</li>\n<li>实际场景中，RTO为200ms出头最为常见。</li>\n</ul><p>对于快速重传：</p><ul>\n<li>快速重传的触发条件是：收到3个或者3个以上的重复确认报文（DupAck）。</li>\n<li>在快速重传中，SACK（选择性确认）也起到了避免一部分已经到达的数据被重传。不过，也由于TCP头部长度的限制，SACK只能放置4个块，再多也不行了。</li>\n<li>快速重传只要3个DupAck就可以触发，实际上我们还可能观察到远多于3个DupAck的情况，这也是正常现象。</li>\n<li><strong>Spurious重传</strong>对TCP传输的影响比快速重传和超时重传小很多，总体来说是一种影响不大的重传。</li>\n</ul><p>另外，在案例拆解的过程中，我们也进一步学习了Wireshark的使用技巧，包括：</p><ul>\n<li><strong>Wireshark里的信息，是需要跟你“抓包发生在哪一侧”这个信息，结合起来解读的。</strong>这对你的排查会起到很关键的作用。</li>\n<li>如何定位到被重传的原始报文的方法：<strong>先找到重传报文的序列号，然后到前面找到同样这个序列号的报文。</strong></li>\n<li>如果在专家信息里看到 <strong>New fragment overlaps old data (retransmission?)</strong>，这意味着多个报文之间的数据有重叠，但一般不是严重的问题。</li>\n<li>Wireshark提示的 <strong>(suspected) fast retransmission</strong> 就是快速重传报文。</li>\n<li>Wireshark提示的 <strong>(suspected) retransmission</strong> 就是超时重传报文。</li>\n<li>如果发现有数据报文和DupAck数量不对等的情况，可以<strong>看一下是否有TSO的存在</strong>。</li>\n</ul><h2>思考题</h2><p>最后再给你留两个思考题：</p><ul>\n<li>TCP的确认报文如果丢失了，发送端还会不会重传呢？为什么？</li>\n<li>你有没有遇到过重传引发的问题，你是怎么处理的呢？</li>\n</ul><p>欢迎你把答案分享到留言区，我们一起进步、成长。</p>',
        article_title: "12 | 重传的认识：重传到底是怎么回事？",
      },
      {
        title: "13 | 重传的再认识：没有任何丢包却也一直重传？",
        id: 487915,
        content:
          '<p>你好，我是胜辉。</p><p>在上节课，我带你深入探讨了TCP重传的知识点，包括超时重传和快速重传。想必你对于重传的现象和背后的原理，也已经有了不少的了解。那么现在，你可以来思考这样一种情况：用Wireshark打开一个抓包文件，你看到了满屏的TCP Retransmission，第一感觉会是什么？</p><p>你应该会认为是掉包了，所以客户端重传了对吧？可能是网络路径上出了状况。</p><p>但实际上，网络状况是重传的一个重要因素，却不是唯一。另外一个因素也同样重要：<strong>操作系统对TCP协议栈的实现</strong>。</p><p>这是因为，TCP等传输协议不是无根之木，它们必须依托于操作系统而存在，包括各种客户端、服务端、网络设备等等。就以重传为例，表面上看是由于网络状况而引发的，但其实真正操控重传行为自身的，还是操作系统，确切地说，是TCP通信两端的操作系统。</p><p>所以，在这节课里，我会给你再介绍一个十分特殊的案例，带你用一种全新的视角来审视TCP重传。通过这节课的学习，你将会对TCP的基本设计，特别是其中最复杂的知识点之一的重传部分，有更加深刻的理解。这样即使以后你在工作中遇到各种奇怪的TCP问题的时候，也不会再轻易被它们的表面所迷惑，而是能有更加准确的判断了。</p><!-- [[[read_end]]] --><h2>eBay的HTTP请求慢的问题是怎么解决的？</h2><p>开头我们假设的那个场景，是一上来就直接分析TCP的重传问题，好像这个问题刚冒头，就是以“TCP重传”的形式出现的一样。但在真实的生产环境当中，问题出现的时候就不会那么直接了，而是以应用层的某种形式，比如以“事务处理慢”这种形式出现的。</p><p>下面，我们看一个实际的案例。</p><h3>应用层分析：应用为什么变慢了？</h3><p>eBay的应用大部分都是基于微服务进行设计和开发的。有一天，一个业务开发团队向我们基础架构团队报告了一个情况：从他们客户端集群向服务端（LB上的VIP）发送的请求，遇到了大量的Response Timeout（返回超时）的报错。这里的Timeout是一个应用层的超时设置，如果客户端无法在2秒钟（2000ms）之内收到返回信息，就会抛出超时报错。</p><p>在我们内网，同数据中心的时延基本在1ms以内，跨数据中心的时延在10ms上下，都很快。这里设置的2000ms的超时，事实上大部分都是预留给了应用程序。这个应用的超时机制，跟TCP超时重传机制类似，应用也不想“干等”。</p><p>所以，我们首先采用了挨个测试的方法。因为整个路径是：</p><blockquote>\n<p><strong>客户端 -&gt; LB -&gt; 服务器（就是LB后面的机器）</strong></p>\n</blockquote><p>而报错是在客户端观察到的，那么我们可以对比两种路径：</p><blockquote>\n<ul>\n<li><strong>客户端 -&gt; LB -&gt; 服务器</strong></li>\n<li><strong>客户端  -&gt; 服务器（绕过LB）</strong></li>\n</ul>\n</blockquote><p>看看在这两种情况下传输的请求都有什么不同。</p><ul>\n<li><strong>客户端直接访问服务器的情况</strong></li>\n</ul><p>我们发现，如果客户端绕过LB VIP去直接访问服务器，是正常的，没有超时的报错。服务器上的日志显示，很快收到了客户端发出的请求，花了三百多毫秒就处理完并回复了。比如这个服务器日志：</p><p><img src="https://static001.geekbang.org/resource/image/a1/49/a15b98f0232e30c60522863b21ab2249.png?wh=1920x41" alt="图片"></p><ul>\n<li><strong>客户端访问LB VIP的情况</strong></li>\n</ul><p>这种情况下，客户端会等待很长的时间才能拿到HTTP响应。日志也印证了这一点：服务器上的日志显示，这个请求的处理耗费了1703毫秒。如下图：</p><p><img src="https://static001.geekbang.org/resource/image/bb/30/bbaeb4c8e9f2c01c35c718c53df49b30.png?wh=1920x39" alt="图片"></p><p>并且，客户端上的日志显示，同样是这个请求，在它看来，从发出请求给LB的VIP，到收到LB的VIP返回的响应，一共消耗了2002毫秒。如下图：</p><p><img src="https://static001.geekbang.org/resource/image/bc/7a/bc3623317b15d122fbd1414b69e3c47a.png?wh=1920x40" alt="图片"></p><p>我画了一张示意图，帮助你理解得更加清楚一些：</p><p><img src="https://static001.geekbang.org/resource/image/a3/f4/a32407c628f7105702a0469269f66ff4.jpg?wh=1928x910" alt=""></p><p>你也许会问：看起来不是服务器那头本身处理的耗时很长吗？为什么不查查服务器上应用代码的Bug？</p><p>我们也一度有这个怀疑，服务器上运行的是Java代码，会不会是GC造成的影响呢？所以我们也去查了当时JVM的运行情况，发现这段时间内并没有GC事件。因此，这个可能性也被排除。并且我们定位到，服务器的耗时主要是花费在了read()调用上，即读取网络I/O上面，所以还是需要回到网络排查的方向上来。</p><blockquote>\n<p>补充：Java有相关的分析工具来定位耗时所在，或者用strace也可以定位系统调用的性能情况。</p>\n</blockquote><p>我们用一个示意图来概括这两种场景下的区别：</p><p><img src="https://static001.geekbang.org/resource/image/e6/e8/e6bde04778244ccdabd66b6339e703e8.jpg?wh=2000x1125" alt=""></p><p>由此，我们可以初步判定：问题出在LB，或者LB前后的网络环节。</p><p>这里也是我想分享给你的一个小的排查原则：<strong>针对客户端看到超时或者响应慢的这类问题，最好也检查下服务器本身花费的时间，两者对比，就能找到问题的方向了。</strong></p><p>我给你把整个思路用伪代码的形式组织如下：</p><pre><code class="language-plain">if (服务器耗时约等于客户端耗时) {\n  检查服务器耗时分布\n  if (服务器耗时在网络I/O) {\n      检查中间网络或者LB\n  } else {\n      检查服务器应用程序或操作系统\n  }\n} else if (服务器耗时远小于客户端耗时) {\n  检查中间网络或者LB\n}\n</code></pre><p>我也画了一个示意图，供你参考：</p><p><img src="https://static001.geekbang.org/resource/image/08/c1/08ff1feb493dc5e314f0d723b5de45c1.jpg?wh=2000x1125" alt=""></p><p>那么下面，我们就可以把排查重点，转到 <strong>LB和网络层面</strong>上来。</p><h2>抓住排查重点：<strong>LB和网络层</strong></h2><p>首先，我们在LB进行了抓包，用Wireshark打开抓包文件，一开始看到的是一帆风顺，全绿：</p><p><img src="https://static001.geekbang.org/resource/image/b7/79/b7eebffbbfc9b574b7daf4eefb2a3a79.png?wh=1920x265" alt="图片"></p><p>翻了几页，突然画风一变，全红：</p><p><img src="https://static001.geekbang.org/resource/image/33/22/339c5120eb663f3yyd3a5ee6ac6f9122.png?wh=1920x431" alt="图片"></p><blockquote>\n<p>补充：抓包示例文件已经上传至<a href="https://gitee.com/steelvictor/network-analysis/tree/master/13">Gitee</a>，建议你结合专栏内容和抓包示例文件一起学习，效果更好。</p>\n</blockquote><p>看到这个景象，你是否也会这样判断：“这肯定是有丢包了，所以接收方一直在回复DupAck！赶快去查网络设备的问题。”</p><p>DupAck多半是丢包引起的，但这次不是。为什么呢？我们先来看一下抓包文件展现出来的全貌。</p><ul>\n<li>第一阶段：连接建立，正常。</li>\n<li>第二阶段：客户端开始发送数据包给LB，正常。</li>\n<li>第三阶段：客户端连续发送了约70KB数据（其中大部分数据还未被确认）。</li>\n<li>第四阶段：LB发送ACK包确认收到了前面约27KB的数据。</li>\n<li>第五阶段：客户端继续发送70~91KB的数据，目前为止也是正常的。</li>\n<li>第六阶段：在27KB之后，LB连续发送数十个DupAck，然后客户端发送一次TCP fast retransmission；这样的情况持续到客户端把所有应该发的数据包都发完。</li>\n</ul><p>那么，我们可以在Wireshark里打开Expert Information看一下汇总信息。</p><blockquote>\n<p>补充：关于Expert Information的解读，我在<a href="https://time.geekbang.org/column/article/481042">第5讲</a>有介绍过，如果你感觉现在印象有点模糊的话，可以回头去温习一下。</p>\n</blockquote><p><img src="https://static001.geekbang.org/resource/image/c3/52/c394008ef274737yyc16024yy23ed852.png?wh=1858x296" alt="图片"></p><p>从图上看，快速重传有8个，DupAck有567个，平均每个快速重传对应了约70个DupAck。这里，我们来看看其中一段连续的DupAck和随后的一个快速重传：</p><p><img src="https://static001.geekbang.org/resource/image/0f/55/0f384babccf2fa56cf49f81fb2430555.png?wh=1920x482" alt="图片"></p><p>我给你解读一下：在LB给客户端发送了数十个DupAck之后（比如截图里包号93到104），客户端发送了一次快速重传（包号105），然后LB回复ACK（包号106，针对包号105），接着LB继续新一轮的数十个DupAck（从包号107开始），循环往复。</p><p>其实这里已经说明了这个案例的特殊性，也就是有“<strong>规律性现象</strong>”。如果是网络设备问题导致丢包，那么丢包会是随机现象，不太可能像这样有规律（70个DupAck加一个快速重传，不断循环）。而<strong>往往规律的背后，一般潜藏着某种未知的机制</strong>。</p><p>当然，大量的DupAck和重传，确实跟应用层我们看到的严重的延迟现象对上了。也至少能回答“<strong>应用为什么变慢</strong>”这个问题了。当然，探究根因的话，接下来就是要回答“<strong>为什么有重传</strong>”这个问题。</p><h2>网络排查推进：为什么客户端出现了重传？</h2><p>在上节课，我们学习过两种重传方式，分别是快速重传和超时重传，而这次的重传呢，Wireshark已经提醒我们，属于快速重传。那为什么会有这种快速重传呢？</p><p>这里我们先来看一下相关数据包的具体细节。</p><p>首先，我们看一下在LB回复大量DupAck之前，客户端的发送情况。</p><p><img src="https://static001.geekbang.org/resource/image/7e/y0/7e2c5707bd693f37361eecc3c87e4yy0.png?wh=1920x153" alt="图片"></p><p>可以看到，在第一个LB DupAck包之前，客户端发送的最后一个数据包是包号67，它的信息是：</p><ul>\n<li>序列号为91305，表明从握手开始有91304（减去握手阶段的1）字节的TCP载荷，从客户端发出了；</li>\n<li>确认号为1，因为LB还没回复HTTP响应，也就是没有握手以外的更多数据，所以客户端还是保持握手阶段的确认号1。</li>\n</ul><p>然后我们再来看一下第一个LB DupAck包，其包号为68：</p><ul>\n<li>序列号为1，因为LB还没回复应用层的HTTP响应，所以还是保持握手阶段的序列号1；</li>\n<li>确认号为27741，表示LB收到了27740（减去握手阶段的1）字节的数据，而第27741字节之后的数据并没有收到。</li>\n</ul><p>要知道，TCP协议规定：<strong>接收方回复的ACK包的确认号=发送方数据包的序列号+TCP载荷字节数</strong>。如果接收方回复了DupAck，假设这个DupAck的确认号为n，那么其含义是：我只收到发送方给我的序列号为n之前的数据包，而序列号为n（及其之后）的数据包，我都没有确认。</p><p>所以，LB就通过DupAck包向客户端宣告：“我这边只确认收到序列号27741之前的数据包。”</p><p>而这里出现几十次DupAck的原因是，一旦LB认为某个数据包我没有收到（此处是序列号为27741的数据包），那么数据就“断档”了，之后客户端送过来的每个数据包，LB都无法ACK这些数据包的序列号+TCP载荷字节数。所以虽然ACK包还是要发，但确认号却只能“停留”在丢包处的确认号，并且这样重复的ACK会有很多个。</p><p>为了便于理解，我把这个过程换一种方式给你展示一遍。除了27741这个号以外，其他序列号是为方便举例而编出来的：</p><p><img src="https://static001.geekbang.org/resource/image/4b/39/4ba01c8e51bd63d115883127493a5639.jpg?wh=2000x922" alt=""></p><p>上面是客户端来一个报文，LB回复一次DupAck。当然也可能像下面这样，连续来多个报文，LB回复连续的多个DupAck：</p><p><img src="https://static001.geekbang.org/resource/image/98/52/9870bdfb5fbcf9f0c8a681dc01f30952.jpg?wh=2000x914" alt=""></p><p>好了，现在情况就比较清楚了，虽然“丢包”的根因还没找到，但整个排查工作的脉络已经相对清晰了，即：<strong>某处丢包-&gt; TCP重传-&gt; TCP传输速度下降-&gt;应用层超时报错</strong>。</p><p>看起来，我们离成功只剩一步了，也就是，在抓包文件中<strong>找到那个丢失的序列号为27741的数据包</strong>！</p><p>只要证明这个数据包确实是在网络上丢失的，那么我们就去修复网络。TCP不丢包了不重传了，速度就上来了，应用就不超时了。逻辑圆满自洽，感觉胜利已经在向我们招手了，是不是？</p><p>可是峰回路转。我们去翻前面数据包的时候，发现根本就<strong>没有那个“序列号为27741”的数据包</strong>！</p><p><img src="https://static001.geekbang.org/resource/image/de/a2/de903d7a576b5e680b2e195385c048a2.png?wh=1662x80" alt="图片"></p><p>上图是最接近27741序列号的附近的数据包，有序列号27229的包，也有序列号28689的包，但就是没有位于这两个数中间的27741的包。</p><p>这个时候，恍惚中有点感觉在看悬疑小说：一桩案件的元凶被查出是某某某，结果发现某某某这个人压根不存在。</p><p>你如果有跑步的爱好，应该会知道：跑步过程中会有一个极限区，此时我们的心肺会遇到很大的压力，这种难受的感觉很容易让人放弃。但是，如果继续坚持挺过这个极限区，身体就能提升到一个新的平台上继续平衡运转。进而，我们就可以继续快乐地跑下去了。</p><p>显然，我们的排查进入了“极限区”了。止步还是进步，就在一念之间。</p><h4>TCP的本质：再次思考什么是确认号？</h4><p>那么，这个序列号27741的包是消失了吗？还是说它就在那里，只是我们忽视了它的存在？</p><p>TCP序列号、Payload（载荷）、TCP确认号，一般情况下就是一个A+B=C的关系。但是，<strong>确认号必须是序列号+全部载荷吗？它可以是序列号+部分载荷吗？</strong></p><p><img src="https://static001.geekbang.org/resource/image/24/5f/242d107b8deb994649774d5200yy525f.jpg?wh=2000x727" alt=""></p><p>举个例子，如果我从网上购买了一套衣服（上衣+裤子），我也收到了全套。但我觉得裤子尺码不对，上衣还挺合身，我可以只确认我收到了上衣（当然裤子还是要退回的），让卖家重新发裤子给我吗？这是可以的。</p><p>如果TCP也可以这样呢？那么，<strong>“寻找序列号为27741的包”也许就是个伪命题，这个“包”实际上并不是独立的一个包，它只是某一个TCP包的一部分（前半部分）。</strong>我们来看一下包号20（也就是前面找27741的时候，关注的两个报文之一）的详情：</p><p><img src="https://static001.geekbang.org/resource/image/ff/f3/ff25bc8e4e3fa37a1c9f97f945b459f3.jpg?wh=572x295" alt="图片"></p><p>这个包是从客户端发给LB的，它的序列号为27229，载荷为1460字节。Wireshark也告诉我们，客户端将要发的下一个包的序列号会是28689。然而，LB回复的ACK（后续同样的ACK就是DupAck）却是这样：</p><p><img src="https://static001.geekbang.org/resource/image/f7/5f/f73936085eb6edd61ffae08e5731c95f.jpg?wh=572x267" alt="图片"></p><p>我们再看那个最为可疑的数字27741。显然，27741=27229+512。到这里看清楚了吗？这次LB确认的是一件“上衣”（512字节），而余下的“裤子”（另外的1460-512=948字节），LB并没有确认。没有被确认的数据，在客户端看来，是需要重新发送的。</p><p>我们先看看正常情况（即每次确认1460字节 ）下的数据包交换过程：</p><p><img src="https://static001.geekbang.org/resource/image/de/7a/de421d69585d066372a67936af0e387a.png?wh=1048x708" alt="图片"></p><p>然后再来看一下这次异常交换的过程。</p><ul>\n<li>客户端：我发给你从27229开始的1460字节，下一次你懂的，我将要发的是28689开始的数据。</li>\n<li>LB： 我也不知道最近怎么搞的，记性不好，你这次给我这些数据，我好像只认得前512字节，其他的我认不出来了，先确认这512字节吧。</li>\n<li>客户端：怎么回事？只确认前512字节？麻烦了，我为了保证这次发送的TCP载荷依然能用足一个MSS即1460字节，必须<strong>把前一个包的后948字节和下一个包的前512字节，组合在一起，变成一个新的1460字节的包</strong>，再发送给你。不过还好，所有未被确认的数据还都在我的发送缓存（send buffer）里面，没有丢失。不过原先计算好的安排都要改掉了，我的CPU开销很大啊！</li>\n</ul><p>这次异常通信的过程如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/bd/07/bdd85530dc112bc9283322c0cda3e407.png?wh=1256x680" alt="图片"></p><p>这里，我再提醒你一个关键点：<strong>确认号本身代表字节数，所以它是字节级别的，而不是报文级别</strong>。也就是说，确认号是精确到某个字节的，而不是某个报文。</p><p>说到这里，我们再回顾下前面提到过的现象：平均每个快速重传对应了约70个DupAck，而每次重传都需要客户端把发送缓冲区里面打包好的数据包，挨个拆开，重组成LB想要的样子（512字节的位移的关系）。</p><p>想必，现在你已经清楚为什么应用会超时（超过2s）了：因为时间都花费在了各种包的分拆、重组上面了。光是客户端想成功发完一个POST请求，都花费了远远超出预期的时长​。就如同一辆车频繁熄火，还怎么可能高速行驶呢？</p><p>我画了一张更详细的图，来帮助你理解这个复杂的过程：</p><p><img src="https://static001.geekbang.org/resource/image/48/5f/48cc15a770219761ae21dc293c0a895f.png?wh=1726x1350" alt="图片"></p><ul>\n<li>客户端发送HTTP POST请求，在TCP层面体现为一系列数据包（30KB以内）给LB，LB转发给服务器，在30KB之前一切正常。客户端应用程序的Timeout计时器，也从POST请求发送的那一刻开始计时。</li>\n<li>大约在客户端发送数据到30KB左右时，LB回复的ACK包确认的数据，不再是数据包分界点的字节数，而是<strong>位于中间的某个字节数</strong>（这个行为比较罕见）。</li>\n<li>客户端累积收到3个这样的DupAck（在这个案例里达到70多个），认为该包丢失。加上这个包的特殊性（是之前某个完整包的一部分），客户端会从缓冲区找出对应的字节数，拼凑上后续包的数据，组装成一个新的MSS大小（1460字节）的数据包，并且在此处消耗了可观的时间。</li>\n<li>LB继续发送类似的“中间确认”包，客户端继续进行“拆包、重组”的操作，此处<strong>持续消耗客户端的时间</strong>。</li>\n<li>服务器的时间都花在read()调用上（因为数据还在重传和重组过程中），无法及时收取完整的POST请求并计算处理，此时已经无法在客户端预定的Timeout时限内完成任务，于是客户端报错。</li>\n</ul><p>为了更加方便你的理解，我把这个过程概括成了下面这张图：</p><p><img src="https://static001.geekbang.org/resource/image/cc/e0/cc6c3ae1df5f11edc486c6973a18efe0.jpg?wh=2000x1125" alt=""></p><p>真相大白了！凭借这些详细的分析和充足的证据，我们说服了LB厂商并确认这是一个Bug，它容易在请求尺寸比较大的情况下被触发。比如在这个案例里，一个POST请求平均大小在100KB，也就触发了这个Bug。</p><p>实际上，在Bug修复之前，我们通过扩大TCP receive buffer size，使得缓冲区足够大（你HTTP POST请求大，我缓冲区更大），也做到了对Bug的规避。</p><h2>小结</h2><p>今天介绍的确实是一个比较罕见的案例，也是我处理过的众多网络排查案例中，遇到的仅有的一次“确认号在中间位置”的情况。那么从协议规范来说，这样“确认中间位置”的做法，到底是否违规呢？</p><p>我们看一下TCP协议的第一版规范<a href="https://www.ietf.org/archive/id/draft-ietf-tcpm-rfc793bis-25.html#section-3.10.7.4-2.1.2.1.9.7.2.1">RFC793</a>，看看它对确认号的要求是什么：</p><pre><code class="language-plain">if the ACK bit is on\n......\n        ESTABLISHED STATE\n\n          If SND.UNA &lt; SEG.ACK =&lt; SND.NXT then, set SND.UNA &lt;- SEG.ACK.\n</code></pre><p>其中几个缩写的含义如下：</p><pre><code class="language-plain">SND.UNA - send unacknowledged #这是指已发送的但未被确认的TCP段的位置\nSND.NXT - send next           #这是已经发送的TCP段的下个序列号\n</code></pre><blockquote>\n<p>“下个序列号”这个知识点在<a href="https://time.geekbang.org/column/article/485689">第10讲</a>介绍过，你可以回头复习一下。</p>\n</blockquote><p>TCP应该接受 <strong>SND.UNA &lt; SEG.ACK =&lt; SND.NXT</strong> 这样的情形，也就是收到的报文的确认号，应该大于已经被确认的数据的位置，并且小于等于（要发送的）下个序列号。</p><p>一般来说，我们看到的大部分是“确认号等于下个序列号”的情况，如下图：</p><p><img src="https://static001.geekbang.org/resource/image/c5/82/c5b77c0bb57cbb062d34108f1f2eba82.jpg?wh=2000x499" alt=""></p><p>但是从这个案例的情况来看，就是<strong>确认号是在中间位置</strong>。这虽然很少见，但也不违规，也可以被操作系统接纳并处理。只不过，引起的开销有点过大了。</p><p><img src="https://static001.geekbang.org/resource/image/1d/32/1da50dd84147b3739d7fee5be312d232.jpg?wh=2000x502" alt=""></p><p>除了上面的知识点以外，我也建议你务必关注整个排查过程带来的启发：</p><ul>\n<li><strong>网络排查过程中要仔细核对各种事实和数据，避免仅根据表面现象轻易下结论。</strong>比如，在这个案例里，很多的TCP重传很容易让我们把关注点错引到网络状况上面去。所以只有仔细核对这些数据，发现其中的问题，才不会被自己的思维惯性所误导。</li>\n<li><strong>对于各种重传的现象和成因应该有充分的了解，这样对排查方向的确定有很大的帮助。</strong>特别是重传的两个大类，即快速重传和超时重传，它们的特征和应对策略，你最好熟记于心，这样等你遇到类似情况时，很快可以对症下药，提高解决问题的效率。</li>\n<li><strong>基于前面两点做细致踏实的分析，即使得出的结论比较意外，也应该保持实事求是的态度去看待和验证。</strong>在这次案例中，TCP确认号没有像常规的那样ACK=RCV.NXT，这也是出乎意料的事情。正是因为我们充分尊重这样的事实，并进行推理，才能突破既有的思维，找到了真正的原因。</li>\n<li><strong>对于“超时、处理慢”这类问题，建议你对比客户端和服务端的耗时，这有利于你找到正确的排查方向。</strong>在这个案例中，我们比较了两端的耗时，发现两者接近；然后，通过在服务器上做系统排查，发现时间主要花费在read()上，这就说明，问题很可能出在网络或者LB上。课程中我给你整理了一段伪代码，梳理了这种排查思路，你可以拿来参考。</li>\n<li>最后，<strong>对于排查期间发现的规律性的现象，可以重点关注</strong>。规律性的背后藏着的东西，跟问题的根因，多半有着密切的联系。所以这种规律性问题，也许正是我们排查的突破口。</li>\n</ul><h2>思考题</h2><p>最后还是给你留两道思考题：</p><ul>\n<li>如果接收端收到一个确认包，其确认号为200，而当前的未被确认的位置在500，那么接收端会怎么处理这个看起来“迟到并且重复”的确认包呢？</li>\n<li>你有没有遇到过这种“确认号在中间位置”的情况？当时有没有引起什么问题呢？</li>\n</ul><p>欢迎在留言区分享你的答案，也欢迎你把今天的内容分享给更多的朋友。</p><h2>附录</h2><p>抓包示例文件：<a href="https://gitee.com/steelvictor/network-analysis/tree/master/13">https://gitee.com/steelvictor/network-analysis/tree/master/13</a></p>',
        article_title: "13 | 重传的再认识：没有任何丢包却也一直重传？",
      },
      {
        title: "14 | 安全：用Wireshark把DDoS攻击照出原形",
        id: 488565,
        content:
          '<p>你好，我是胜辉。</p><p>在过去几节课里，我们集中学习了TCP传输相关的知识，无论是<a href="https://time.geekbang.org/column/article/484667">第8讲</a>的MTU、<a href="https://time.geekbang.org/column/article/484923">第9讲</a>和<a href="https://time.geekbang.org/column/article/485689">第10讲</a>的传输速度、<a href="https://time.geekbang.org/column/article/486281">第11讲</a>的拥塞控制，还是第12和13讲的各种重传，我们可以说是把TCP传输相关的核心概念全部过了一遍。一方面学习了RFC规范和具体的Linux实现，一方面也通过案例，把这些知识灵活运用了起来。想必现在的你，再去处理TCP传输问题的时候，已经强大很多了。</p><p>不过，上面的种种，其实还是在协议规范这个大框架内的讨论和学习，默认前提就是通信两端是遵照了TCP规定而展开工作的，都可谓是谦谦君子，道德楷模。</p><p>但是，有明就有暗。不遵照TCP规范，甚至寻找漏洞、发起攻击，这种“小人”乃至“强盗”的行为，也并非少见，比如我们熟知的 <strong>DDoS攻击</strong>。</p><p>那么这节课，我们就不来学习怎么做君子了，当然，也不是教你做“小人”，而是我们要<strong>了解“小人”可能有的各种伎俩，通过Wireshark把这种种攻击行为照个彻底，认个清楚</strong>。这样，以后你如果遇到这类情况，就心里有数，也有对策了。</p><h2>NTP反射攻击案例</h2><p>我在公有云做技术工作的时候，发现游戏类业务遭到DDoS的情况比较多见。有一次，一个游戏客户就发现，他们的游戏服务器无法登录，被玩家投诉，可以说是十万火急。客户的工程师做了tcpdump抓包，然后赶紧把抓包文件传给我们一起分析。</p><!-- [[[read_end]]] --><blockquote>\n<p>补充：抓包示例文件已经上传至<a href="https://gitee.com/steelvictor/network-analysis/tree/master/14">Gitee</a>，建议Wireshark打开抓包文件，结合文稿一起学习。</p>\n</blockquote><p>从抓包文件概览来看，确实是不正常。按惯例，我们看一下Expert Information：</p><p><img src="https://static001.geekbang.org/resource/image/fd/5a/fd308647a1b1fe2b1bb813113e857e5a.jpg?wh=1820x216" alt="图片"></p><p>游戏业务一般也是基于TCP，但这里并没多少TCP相关的信息。那我们看下具体报文呢？</p><p><img src="https://static001.geekbang.org/resource/image/5b/0c/5beeb84467b2a31dd97a2e563f277c0c.jpg?wh=1920x980" alt="图片"></p><p>全部都是这种NTP Version 2, Private, Response, MON_GETLIST_1的报文。客户根本没有对外提供什么NTP服务，这些报文是怎么来的呢，这是攻击吗？</p><p>确实，这就是DDoS攻击的一种类型，叫<strong>NTP反射放大攻击</strong>。NTP全称是Network Time Protocol，它的作用是通过网络服务来同步时间。其中有一项功能叫Monlist，它在一些比较老的设备上是默认启用的，会返回与NTP服务器进行过时间同步的最后600个客户端的IP。</p><p>响应包按照每6个IP进行分割，最多有100个响应包。这样的话，一个简单的NTP Monlist响应，就可能是请求的200多倍。想象一下，如果请求是1Gbps，那这次反射攻击就可以达到200Gbps以上，实在惊人！</p><p>我们还是用Wireshark，展开UDP部分，看一看这种Monlist响应报文的细节：</p><p><img src="https://static001.geekbang.org/resource/image/26/f3/26cefaeyy5484c497d760719538301f3.jpg?wh=1660x680" alt="图片"></p><p>每个Monlist item占用72字节：</p><p><img src="https://static001.geekbang.org/resource/image/4b/4a/4bbc3de9562104968a680e7bb9947a4a.jpg?wh=1264x1104" alt="图片"></p><p>选中一个Monlist item后，我们可以通过3种不同的方式找到它的长度：</p><ul>\n<li>它有个字段Size of data item的值是72，这是协议本身提供的元数据；</li>\n<li>在下方的字节码部分，数一下有底色的字节数，也是72个；</li>\n<li>窗口底部对我们选中的字段有提示字节数，这里也是72 bytes。</li>\n</ul><p>这些方法，对于你平时用Wireshark解读抓包文件，特别是需要核对字段的具体信息时，是挺有用的。</p><p>不过，现在Wireshark窗口里解读UDP报文长度不是很直观，这是因为<strong>Wireshark默认没有显示UDP长度的列，但我们可以自己添加</strong>。在UDP详情里选中Length，然后右单击，选中Aplly as Column：</p><p><img src="https://static001.geekbang.org/resource/image/b4/f1/b4cb43df336152e5d937de7ff63c19f1.jpg?wh=1920x985" alt="图片"></p><p>然后就能在主窗口里看到UDP报文长度了，这个列的默认名称是Length。当然你也可以把它改为UDP Length等你觉得更合适的名称。</p><p><img src="https://static001.geekbang.org/resource/image/3b/8f/3b1e05cbab2fce92fd552af1f782ec8f.jpg?wh=1920x508" alt="图片"></p><p>从图上看，这些UDP报文的长度都不大，只有448字节，这是为什么呢？我们知道MTU一般是1500字节，去掉IP头20字节和UDP头8字节，最多还有1500-20-8=1472字节，远大于448字节，为什么不用足这1472字节呢？</p><p>其实，这里就涉及UDP的一个概念：<strong>UDP报文的载荷最好不要大于512字节</strong>。</p><p>这个限制来自于IPv4协议。在IPv4的协议规范<a href="https://www.rfc-editor.org/rfc/rfc791#section-3.1">RFC791</a>里建议，虽然IP报文的长度字段是2个字节，最大可以到65535，但是由于网络不允许传输这么大的报文，所以IPv4规范建议，报文长度应该控制在相对小的范围内，这个范围是576字节，相应的UDP载荷在512字节以内：</p><pre><code class="language-plain">The number 576 is selected to allow a reasonable sized data block to\n    be transmitted in addition to the required header information.  For\n    example, this size allows a data block of 512 octets plus 64 header\n    octets to fit in a datagram.  The maximal internet header is 60\n    octets, and a typical internet header is 20 octets, allowing a\n    margin for headers of higher level protocols.\n</code></pre><p>很多应用程序都做了这部分逻辑的处理，也就是控制UDP载荷在512字节以内，比如这次的NTP Monlist的长度就是NTP协议实现，而不是内核UDP实现的。另外像DNS解析，如果数据量超过512字节，也是会自动切换为TCP模式的，根本原因也是这个很早以前的规定。</p><p>那么检查了载荷，我们很快会发现新的问题：“这里怎么只有NTP回复，没有NTP请求呢？”</p><p>其实，这正是前面说的1Gbps能放大为200多Gbps的原因。它的背后，就是反射攻击的核心技巧：<strong>它利用IP协议“不对源IP做验证”的不足，构造一个IP报文，其源IP为被攻击站点的IP，使得NTP服务器回复的报文也被发往被攻击站点</strong>。大量的响应报文就被引到了被攻击站点这里。而且这个过程中，NTP服务器被利用了还不知道。我们看个示意图：</p><p><img src="https://static001.geekbang.org/resource/image/cb/8e/cb817f657b773c832eceb3a0f9b52f8e.jpg?wh=2000x857" alt=""></p><p>如果我们面临这种攻击，该怎么办呢？</p><ul>\n<li>假如这些被利用的NTP服务器是我们的，那么需要升级版本，避免自己成为“帮凶”。</li>\n<li>如果我们是单纯的被攻击者，那就需要上一些手段了，我会在这次课程的后半段讲到。这次的游戏客户，就是上了高防后，扛住了这次攻击。</li>\n</ul><p>我们再来看一个例子。</p><h2>SSDP反射型攻击案例</h2><p>你可能听说过“肉机”这个词，这也是国内技术圈发明的一个有意思的词汇，它指的是被黑客掌握了系统权限的主机。作为傀儡，这些成千上万的“肉机”可以被黑客集中调动起来发起攻击行为。假如一个黑客组织掌握了1万台“肉机”，那么，只要每台“肉机”发起哪怕只有1Mbps的攻击流量，乘以1万，就是10个Gbps的流量，不可小视。</p><p>但是，要拿到这么多“肉机”却并非易事。于是，聪明的黑客又想到了另外一种思路：借力打力。</p><p>借什么力呢？借协议的“<strong>响应是请求的很多倍</strong>”这个力。显然，前面介绍的NTP反射攻击就是这样的。所以这种攻击，英文里叫reflection attack with amplification。amplication就是放大的意思。在这种模式下，只需要量很少的“肉机”，就可以发起巨大的攻击流量。</p><p>下面是另外一个客户的案例。当时他们遭受了一波攻击，也正好做了抓包。我们看一下抓包文件的Expert Information：</p><p><img src="https://static001.geekbang.org/resource/image/be/de/be8932473a55aeffef2557e0c5261fde.jpg?wh=1806x172" alt="图片"></p><blockquote>\n<p>补充：抓包示例文件已经上传至<a href="https://gitee.com/steelvictor/network-analysis/tree/master/14">Gitee</a>，建议Wireshark打开抓包文件，结合文稿一起学习。</p>\n</blockquote><p>你是不是也觉得比较奇怪？这里没有TCP握手报文，上来就是HTTP/1.1 200 OK，这HTTP有点“自来熟”啊。既然这里有51257个HTTP 200响应报文，那按常理也至少有几十个TCP连接，而这里连一个SYN和FIN都没有。</p><p>那就让我们直接看看这些HTTP 200具体是怎样的：</p><p><img src="https://static001.geekbang.org/resource/image/a8/02/a86588af9b086c7fc91b108cb5152602.jpg?wh=859x724" alt="图片"></p><p>奇怪，这里srcPort和dstPort居然都是空白的？不过视线移到下方，很快就找到答案了：原来是用了UDP协议。我们上面的srcPort和dstPort列是指TCP的端口号，难怪是空白。</p><p>但更奇怪的问题来了：这里的HTTP竟然用了UDP作为传输协议？HTTP一般是用TCP协议的，那这里用UDP又是怎么回事呢？是不是感觉网络协议到处都可能有意想不到的情况？</p><p>其实，这就是有名的 <strong>SSDP反射放大攻击</strong>。SSDP是在UDP这个传输层协议上，用HTTP协议格式传送信息。2014年，人们发现SSDP可以被攻击者利用。启用了SSDP协议的主要是一些家用路由器，在它们的UPnP软件中有一个漏洞，这个漏洞被攻击者利用后，这些路由器会从端口1900返回响应报文。那么显然，这些响应报文的目的地址，是被攻击站点的IP，而不是攻击发起者自己的IP了。</p><p>具体的攻击过程，跟前面NTP反射攻击里面的图差不多，这里就不重复了。当时的应对方法也是上了高防系统，顶住了这次攻击。</p><blockquote>\n<p>补充：有趣的是，著名的网络服务公司Cloudflare把SSDP戏称为<strong>S</strong>tupidly <strong>S</strong>imple <strong>DDoS</strong> <strong>P</strong>rotocol。你可以在<a href="https://www.cloudflare.com/learning/ddos/ssdp-ddos-attack">这里</a>看到Cloudflare对SSDP攻击的更多解释。</p>\n</blockquote><p>如果你也担心自己家的路由器也中招了的话，可以自测一下。访问<a href="https://badupnp.benjojo.co.uk/">https://badupnp.benjojo.co.uk/</a>这个站点，它会对你的出口IP（家用路由器的出口IP）进行探测，看看是否有1900端口可以被利用。如果没有漏洞，网页会提醒你“All good! It looks like you are not listening on UPnP on WAN”。</p><p>了解了这次攻击的类型，接下来我们再来看一下这次抓包的概览。这里我们要学习一个新的命令：<strong>capinfos</strong>。</p><p>capinfos这个命令，是Wireshark自带的工具集中的一个小工具，也就是你安装完Wireshark就有它了。我一般在命令行里用它来查看抓包文件的时长、总包量等信息。我们直接运行 <strong>capinfos文件名</strong>，输出如下：</p><pre><code class="language-plain">$ capinfos SSDP_attack_example.pcap\nFile name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;SSDP_attack_example.pcap\nFile type:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Wireshark/tcpdump/... - pcap\nFile encapsulation:&nbsp; Ethernet\nFile timestamp precision:&nbsp; microseconds (6)\nPacket size limit:&nbsp; &nbsp;file hdr: 65535 bytes\nNumber of packets:&nbsp; &nbsp;100 k\nFile size:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;36 MB\nData size:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;34 MB\nCapture duration:&nbsp; &nbsp; 1.916902 seconds\nFirst packet time:&nbsp; &nbsp;2016-05-08 10:25:02.721642\nLast packet time:&nbsp; &nbsp; 2016-05-08 10:25:04.638544\nData byte rate:&nbsp; &nbsp; &nbsp; 18 MBps\nData bit rate:&nbsp; &nbsp; &nbsp; &nbsp;145 Mbps\nAverage packet size: 348.38 bytes\nAverage packet rate: 52 kpackets/s\nSHA256:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 8fa365f01c62023576623116410a6ca289915db4717e4805120009a575fdfb57\nRIPEMD160:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;5ab83442a5178b5f34bee1009a58ddb351bd292b\nSHA1:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 52a2c52644bc2753f9a11b7bf71eb1144f2c9a30\nStrict time order:&nbsp; &nbsp;True\nNumber of interfaces in file: 1\nInterface #0 info:\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Encapsulation = Ethernet (1 - ether)\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Capture length = 65535\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Time precision = microseconds (6)\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Time ticks per second = 1000000\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Number of stat entries = 0\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Number of packets = 100000\n</code></pre><p>上面的信息比较多，我们可以重点关注下面这几个信息：</p><pre><code class="language-plain">Number of packets:&nbsp; &nbsp;100 k\nCapture duration:&nbsp; &nbsp; 1.916902 seconds\nAverage packet rate: 52 kpackets/s\n</code></pre><p>可以看到，这次抓包一共抓取了10万个报文，耗时只有1.9秒，平均包率为52kpackets/s，也就是每秒5万2千个包。而且都是SSDP协议响应报文，所以是DDoS没错。</p><p>在平时，你想了解一个抓包文件的包率、时长、平均报文大小等信息的时候，都可以用capinfos命令来快速获得。</p><p>回顾完两个具体的案例，\b想必你对于DDoS有了感性的认识了，接下来我们就来系统地认识一下DDoS。</p><h2>到底什么是DDoS攻击？</h2><p>DDoS跟DOS有着密切的联系。DOS（Denial of Service），就是服务拒绝，黑客通过各种手段，使得被攻击者无法正常提供服务。DOS这种攻击早已经存在了，而DDoS（Distributed DOS）就是它的升级版，<strong>通过调动分布在各地的客户端发起攻击，使得被攻击站点无法正常服务</strong>。</p><p>DDoS属于“攻击”，但不是“入侵”。两者的区别是，攻击是破坏服务，入侵可能不破坏，但会窃取资料、劫持勒索等等。既然DDoS要破坏服务，那就需要破坏计算资源。那么，什么又是资源？</p><p>一般说的计算机资源还是CPU、内存这些。旨在耗尽CPU和内存资源，这也是早期的攻击形式，也跟很多年前软件病毒肆虐的时候类似。当时的攻击和病毒，主要目的是让被攻击站点本身失去服务能力。另外，早期的黑客大多也是极客，攻击是他们展现技术能力的一种方式。</p><p>不过，随着安全加固技术和意识的不断增强，攻破系统的成本越来越高，于是攻击者转换了方向。其实他们不需要想办法攻入对端，只要在前面的网络环节上搞破坏，同样可以达到让对方服务瘫痪的目的。这时，服务瘫痪的原因已经不是之前的服务本身不可用，而是变成：网络通道不可用了！</p><p>而这，就是DDoS的核心目标：<strong>耗尽网络带宽</strong>。</p><p>假设被攻击的站点的带宽为1Gbps，那么攻击者只要让到达这个站点的流量超过1Gbps，就可以让这个站点失去正常服务的能力。至于这些报文是否属于被攻击站点正在监听的有效流量，是没有关系的。它的目的很直接：把你家门口的路给堵死，让正常的流量没有机会进来。</p><p>我们看一下示意图：</p><p><img src="https://static001.geekbang.org/resource/image/d4/0c/d46a0295a1b4f47b342c11c79f51530c.jpg?wh=2000x1125" alt=""></p><p>理解了原理，那么技术性问题就来了：如何产生巨大的流量呢？</p><p>一种常见的实现方式就是反射型攻击。它的核心方法论是：利用一些协议的“<strong>响应是请求的很多倍</strong>”这样的特点，同时也利用“<strong>IP协议不验证源IP</strong>”的不足，达到把流量引到被攻击站点上去的目的。</p><p>上面的NTP反射攻击和SSDP都是如此。除此以外，你有没有发现别的这种“响应报文是请求报文的很多倍”的情况呢？如果有，那么恭喜你，你也能找到反射攻击的方式了！</p><p>这可真的是“举一反三”。明白了反射型攻击的原理，你是不是好像也有机会自己创造出新的DDoS攻击手段了。当然，还有很多别的事情要搞定，但是核心思路你已经清楚了。现在的你，是不是对DDoS有了更加深刻的认识了呢？</p><p>这里还有个细节。你有没有发现，前面介绍的NTP反射攻击是依托于UDP协议的，其他很多DDoS类型也是利用了UDP。为什么都是UDP呢？让我们再来回答一下这个问题。</p><h2>为什么UDP容易被用来做DDoS攻击？</h2><p>TCP当然也可能被DDoS所用，但是相对来说，如果用同样的成本，选择反射型攻击更加高效。而反射型攻击，主要基于UDP，这是为什么呢？主要有两个原因。</p><h3>UDP报文简单易于构造</h3><p>我们看一下UDP头部。<a href="https://datatracker.ietf.org/doc/html/rfc768">RFC768</a>定义了UDP头部的格式：</p><p><img src="https://static001.geekbang.org/resource/image/26/b2/26ed0d4f50a7cc54e910435c05b27bb2.jpg?wh=473x305" alt="图片"></p><p>由上图可见，UDP头部其实只有8个字节，分别是：</p><ul>\n<li>2个字节的源端口号；</li>\n<li>2个字节的目的端口号；</li>\n<li>2个字节的报文长度；</li>\n<li>2个字节的校验和。</li>\n</ul><p>还是借助Wireshark，我们更加近距离地看一个UDP报文：</p><p><img src="https://static001.geekbang.org/resource/image/90/98/90c05f5c609c11b49045b6f8d57a1298.jpg?wh=609x472" alt="图片"></p><p>而TCP头部就复杂多了，除了源目端口，还有序列号、确认号、各种标志位、各种TCP扩展选项等等。而因为UDP报文头部如此简单，这就减少了攻击者做伪造的难度，只要做好这几件事就好了：</p><ul>\n<li>伪造一个源IP；</li>\n<li>找到NTP等有反射攻击漏洞的服务器；</li>\n<li>向这些服务器发送构造好的虚假的UDP报文。</li>\n</ul><h3>UDP是无状态的</h3><p>这可能是一个<strong>更加关键</strong>的原因。UDP是无状态的，不需要握手。像NTP反射攻击、SSDP反射攻击，都是只要“一问一答”即可，所以攻击者只需要伪造一个请求报文，那么后续的响应报文，自然就发送给了被攻击站点了。</p><p><img src="https://static001.geekbang.org/resource/image/98/17/988449a5e4107ba3a0867f231c772917.jpg?wh=1698x806" alt=""></p><p>但是TCP就非常不同了。首先TCP需要三次握手，如果攻击者的SYN报文的源地址是伪造成被攻击站点的IP，那么SYN+ACK报文就直接回复到那个站点的IP了，而不是攻击者。然后会发生什么呢？</p><p>被攻击站点收到一个莫名其妙的SYN+ACK，就会被RST掉。这次TCP握手就这么结束了，攻击就没法继续了。</p><p><img src="https://static001.geekbang.org/resource/image/04/8c/041893ed942fdf744bccbe8b520f8d8c.jpg?wh=1672x807" alt=""></p><p>那跳过TCP握手，直接发送应用层请求（源地址还是伪造成被攻击站点的IP）给反射服务呢？当然是直接被RST，因为连握手都没做过呢。</p><p><img src="https://static001.geekbang.org/resource/image/bb/03/bb77fb1751yycf77e4bcae072dbc4503.jpg?wh=1687x807" alt=""></p><p>而且，即使通过了握手，后续通信双方还有对序列号和确认号进行校验等机制。虽然这些在技术上都可以实现，但难度大了很多，而选择UDP，就不需要考虑这么多问题。</p><p>所以，<strong>用TCP的话就是直接攻击</strong>，而不是反射攻击了。比如SYN攻击、半连接攻击、全连接攻击、CC攻击等等。从“性价比”上看，反射攻击的优势更大些。</p><h2>如何对付DDoS？</h2><p>前面我们分析了DDoS中最为典型的反射放大攻击。以后如果我们发现服务异常，比如客户端的请求十分卡顿的话，就可以在服务端抓包，然后进行分析，就能快速定位是否是DDoS攻击了。</p><p>当然，还有一个更为简单直接的证据，就是你的公网接口带宽使用图，如果图上有明显的突增，甚至达到了接口带宽的上限，那也基本可以判定是遭遇DDoS了。</p><p><img src="https://static001.geekbang.org/resource/image/90/a4/9022db0b266107b161e2677yy309f8a4.jpg?wh=471x501" alt="图片"></p><p>上面都是排查的手段，那接下来如何处理呢？一般来说，你自己单干是不行的，这里给你介绍几种应对策略，这样你以后就心里有数了。</p><h3>高防</h3><p>一般来说，如果你的服务架设在公有云上，那么可以考虑使用云商或者其他专业安全服务商的高防产品。</p><p><strong>高防是需要放置在源站前面的一类安全防护和清洗系统。</strong>它利用了自身的足够大的带宽，以及强大的防护清洗集群，实现对流量清洗，最终把攻击流量拦截在外面，清洗过后的正常流量进入源站，得以被正常处理。示意图如下：</p><p><img src="https://static001.geekbang.org/resource/image/3c/21/3c8bd54238a4b884dd42bfe95246fb21.jpg?wh=2000x1125" alt=""></p><blockquote>\n<p>补充：这里的源站，就是被攻击的站点。</p>\n</blockquote><p>由于高防按时计费而且费用高昂，一般平时是不接入高防的。只有探测到被攻击时，才自动或者手动转入高防。这里的“接入高防”是什么意思呢？其实就是把站点域名指向高防的域名，这样就把流量先流向高防，再经清洗后回到源站。</p><p><img src="https://static001.geekbang.org/resource/image/59/75/595873d6f7d858f28d9e4be1592f4275.jpg?wh=2000x1125" alt=""></p><p>那如果攻击者不是通过域名解析，而是直接盯着IP做攻击的呢？也不难，你就把老的IP解绑，让攻击流量进入路由黑洞，然后绑定新的IP。这时候，不要暴露新的IP的信息，它只能给高防回源用，不能让更多的人知道。</p><p>你有没有发现，从防护的生效点来说，高防这种方式是作用在<strong>服务端这一侧</strong>。那你可能会想到：如果我们能<strong>在攻击的源头就做防护</strong>，那是不是效果会更好呢？这就是另一类DDoS防护产品的设计思想，其中比较典型的产品是电信云堤。</p><h3>云堤</h3><p>云堤本身属于运营商自己的系统，而无论被攻击站点还是肉机，都依托于运营商的公网线路才能进入因特网，所以云堤<strong>具有“地理”上的天然优势</strong>。它可以作用在肉机的攻击流量进入骨干网之前，所以很可能这些攻击都没有机会走到被攻击站点的跟前了，相当于“扼杀在摇篮里”。我们看一下示意图：</p><p><img src="https://static001.geekbang.org/resource/image/73/c7/733994d215a69b4b6372defde7dd48c7.jpg?wh=2000x1125" alt=""></p><h3>anycast和多POP</h3><p>我们知道了DDoS的本质是挤占网络带宽，那么对付它的核心策略就是：</p><ul>\n<li>用更大的带宽来接纳，先解决正常流量被挤出网络的问题。</li>\n<li>在接纳后进行清洗，把正常流量识别出来，发回给源站，让业务继续进行。</li>\n</ul><p>前面介绍了高防和云堤，两者分别在被攻击站点的近端和远端起到了作用，也都是商业服务。那么，另外一种方式是自己搞定，这也是一些<strong>自身规模比较大的网站会部署的架构</strong>，它就是anycast和多POP。</p><p>anycast是网络术语，是指<strong>多个地点宣告同一个网段或者同一个IP地址的行为</strong>。比如，最典型的电信的DNS服务地址114.114.114.114，还有谷歌的DNS服务8.8.8.8，就是在全国乃至全球各处做了anycast的IP地址。与这个词类似的，还有unicast和multicast，分别是指单播和多播。</p><p>比较大型的网站都会在各地部署POP点（也就是多POP），然后这些POP点会宣告相同的IP段。一旦有DDoS攻击，因为它的目标IP是属于anycast网段的，所以会被因特网的路由策略，相对均匀地分布到这些POP。</p><p>假如你有20个POP点宣告同一个网段，那么你就有机会把DDoS攻击化整为零，平均每个POP点承受1/20的攻击流量，大大降低了危害性。在攻击流量不高的时候，仅依靠自己的多个POP就可以吸收掉这些攻击流量，然后用自己的设备进行清洗就可以了。</p><p>这一点上看，anycast+多个POP+自有的清洗设备，这一整套做好以后，相当于自己建设了一个中小型的高防系统。我画了个示意图供你参考：</p><p><img src="https://static001.geekbang.org/resource/image/dc/67/dc3faa23209f13da54799c62350c0267.jpg?wh=2000x1125" alt=""></p><blockquote>\n<p>补充：这里的anycast一般是作用在网段级别。而在单个IP级别的anycast应用还较局限，目前主要还是主要应用在基于UDP的服务，比如DNS服务上。基于anycast的HTTP是比较前沿的领域，目前有少数公司已经开始实践，相信在不久的将来，应该会看到越来越多的公司应用HTTP over anycast。</p>\n</blockquote><h3>CDN</h3><p>与前一点类似，CDN也是通过“多点分布”来达到防护或者缓解DDoS攻击的目的。而且CDN服务商一般也会采用anycast等策略混合使用，使得其防护DDoS的能力更加出色。</p><h2>小结</h2><p>这节课，我们通过NTP反射攻击和SSDP反射攻击这两个典型的DDoS案例的学习，了解了反射放大攻击的特点，它主要利用了以下三点：</p><ul>\n<li><strong>IP协议不对源IP进行校验</strong>，所以可以伪造源IP，把它设定为被攻击站点的IP，这样就可以把响应流量引向被攻击站点。</li>\n<li><strong>UDP协议是无连接的</strong>，可以直接进行应用层的一问一答，这就使得IP欺骗可以奏效。</li>\n<li>某些服务具有“<strong>响应报文的大小是请求报文的很多倍</strong>”的特点，使攻击行为达到了“四两拨千斤”的攻击效果。</li>\n</ul><p>我们也系统性地分析了DDoS的核心方法，也就是用“<strong>耗尽网络带宽</strong>”的方式，让被攻击站点无法正常提供服务。在排查方面，当我们发现服务异常时，在服务端做抓包分析，可以快速定位是否有DDoS攻击。也可以直接根据带宽使用图，关注到突发的巨型流量时也可以直接判定是DDoS攻击。</p><p>另外，我们还了解了应对DDoS攻击的策略，包括：</p><ul>\n<li><strong>使用高防产品</strong>，可以防护非常巨大的攻击流量。</li>\n<li>如果对防护效果有更高的需求，可以使用运营商的<strong>云堤类的产品。</strong></li>\n<li>如果自身条件足够，可以部署<strong>多POP和anycast</strong>，平均吸收攻击流量。</li>\n<li>也可以<strong>上CDN</strong>，让CDN天然的分布式布局减轻DDoS的影响。</li>\n</ul><p>在技术细节方面，你也可以记住这个新的命令<strong>capinfos</strong>，用它可以快速获取到抓包文件的整体信息，包括抓包时长、总报文量、平均报文大小等信息。关于如何在Wireshark里解读出报文字段的长度，你也要知道至少下面这两种方法：</p><ul>\n<li>选中你要解读的报文字段，然后在下面的字节码部分，数一下有底色的字节个数。</li>\n<li>还是选中你要解读的报文字段，在底边栏里也有对应的字节数的显示。</li>\n</ul><p>最后，你要知道这一点：<strong>UDP载荷最好不要超过512字节</strong>，这也是IPv4协议规范的建议，像NTP和DNS这些基于UDP的协议都实现了这个规范。</p><h2>思考题</h2><p>给你留两道思考题：</p><ul>\n<li>“肉机”发出100Mbps的攻击流量，到达被攻击站点的时候，仍然是100Mbps吗？为什么呢？</li>\n<li>为什么CDN可以达到缓解DDoS的效果呢？</li>\n</ul><p>欢迎你把答案和思考写到留言区，我们一起讨论，进步。</p>',
        article_title: "14 | 安全：用Wireshark把DDoS攻击照出原形",
      },
    ],
  },
  {
    chapterTitle: "春节特别放送",
    children: [
      {
        title: "春节特别放送（一）| 书单推荐",
        id: 482837,
        content:
          '<p>你好，我是胜辉。</p><p>春节快到了，每年一到这个时候，就会深感时间过得如此之快。岁岁年年，点点滴滴。回顾一年，如果我们在生活或者工作上有一些进展，认知上有一点提升，乃至健康状况也有所改善，都可以让我们感觉这一年过得还算不错，挺有意义。</p><p>那么，让我们进步的原因可能有很多，而其中最为关键的，可能还是读书了。所以这里，我就来给你推荐一些这几年让我觉得比较不错的三本书。其中一本是技术相关的，一本是心理类的，第三本是健康类的。希望能帮你兼顾到技术知识的学习、积极心态的树立，还有良好身心状态的养成，让我们的新年，过得更加充实一点。</p><h2><a href="https://book.douban.com/subject/25856314">《Web性能权威指南》</a></h2><p><img src="https://static001.geekbang.org/resource/image/d7/9d/d7e35d86782861ef0278926e96b41e9d.png?wh=381x499" alt="图片"></p><p>这本书不同于我在前面课程里提到过的<a href="https://book.douban.com/subject/1088054">《TCP/IP详解》</a>三卷，也正是这种不同，让我觉得更有必要推荐给你。</p><p>如果说《TCP/IP详解》属于技术理论的经典，那么这本《Web性能权威指南》可以算是技术应用的经典了。可以说，如果你读懂了《TCP/IP详解》，再把《Web性能权威指南》读几遍，那你在Web、TCP还有网络领域的理解层次，应该能超过非常多的人。</p><p>这本书的作者，是谷歌公司的Web性能工程师伊利亚·格里高利克（Ilya Grigorik）。这本书全面展现了业界领先的公司（谷歌等）<strong>在Web性能方面的应用实践</strong>。我们可以在《TCP/IP详解》里面学到TCP的各种概念，比如滑动窗口、选择性确认等等理论知识，而《Web性能权威指南》用一个一个实际的场景告诉我们，这些理论是如何一步步影响我们每一天的“网络生活”的。</p><!-- [[[read_end]]] --><p>不过，我这么说，你的印象可能也不会太深，我这里就摘录书中的一段吧：</p><blockquote>\n<p>在金融市场上，很多常用交易算法首要的考虑因素就是延迟，因为几毫秒的差距可能导致数百万美元的收益或损失。<br>\n&nbsp;<br>\n2011年初，华为与Hibernia Atlantic开始合作铺设一条横跨大西洋，连接伦敦和纽约的近5000km的海底光缆（Hibernia Express）。铺设这条海底光缆的唯一目的，就是减少城市间的路由，（相对于使用其他横跨大西洋的线路）为交易商节省5ms的延迟。开通运营后，这条光缆将只由金融机构使用，耗资预计达4亿美元。<br>\n&nbsp;<br>\n简单计算一下，不难得出节省1ms的成本是8000万美元。延迟的代价由此可见一斑。</p>\n</blockquote><p>为了节省区区五毫秒的时延，人们愿意花费4亿美元。从这个例子里，我们能深深地感受到时延对传输的重要性。你是否也跟我一样觉得，这个具体的例子，要比单纯理论概念的灌输，给我们的印象更加深刻呢？</p><p>在TCP协议优化方面，作者格里高利克也提到了近年来业界对此的探索和进展。</p><blockquote>\n<p>人们正在积极寻找各种方案，其中TFO（TCP Fast Open，TCP快速打开）就是这样一种机制，它致力于减少新建TCP连接带来的性能损失。经过流量分析和网络模拟，谷歌研究人员发现TFO平均可以降低HTTP事务网络延迟15%、整个页面加载时间10%以上。在某些延迟很长的情况下，降低幅度甚至可达40%。<br>\n&nbsp;<br>\nLinux 3.7及之后的内核已经在客户端和服务器中支持TFO，因此成为了客户端和服务器操作系统选型的有力候选方案。即便如此，TFO并不能解决所有问题。</p>\n</blockquote><p>我首次阅读这本书是在2015年，当年还是在Kindle上读的，当时有一种“<strong>经脉被打通</strong>”的感觉，之前学习的网络协议的知识，都变得更加丰富立体，好像触手可及。</p><p>不过，因为这本书是在2013年出版的，有些内容可能稍显陈旧。比如这几年快速发展的QUIC和HTTP/3在成书时还没有出现。我也期望作者能够出第二版，把从出版到现在接近十年的最新应用实践更新到这本书里。</p><p>对了，除了中文版，你还可以直接在<a href="https://hpbn.co/">https://hpbn.co/</a>在线阅读到它的英文原版。</p><h2><a href="https://book.douban.com/subject/26648884">《干法》</a></h2><p><img src="https://static001.geekbang.org/resource/image/24/8d/24698e7ea2dc30720d74200f9490358d.png?wh=375x499" alt="图片"></p><p>看过这本书，你可能会恍惚地觉得，你似乎可以用一句略显粗俗的话来总结：“就是干”。但回头一想，又会觉得完全不是这样。稻盛和夫的文字虽然朴实，叙述的事情也没有任何取巧之处，但给人的印象，就好像路过某个寺庙时听到的钟声，简单而深远，久久萦绕在心头。</p><p>在我们的生活中，不乏有一些人觉得“努力工作”毫无意义，甚至他们会鄙视其他努力的人，好像后者是愚钝不堪、执迷不悟一样。稻盛和夫在书中提到，其实，即使在一个仍然类似于原始社会的部落，人们对勤于劳作，把田地和房子修作的比其他人更加好看整洁的人，给予了最高的尊敬。所以，<strong>对劳动和敬业这种美德的赞赏，本来就代表了人类一直以来的对美好的向往。</strong></p><p>这本书是稻盛和夫对自己事业经验的总结。我们很多时候会迷信一些“窍门”“秘诀”，以为很多成功人士一定掌握了不为常人所知的秘密。从稻盛先生的书里，我看到的却是跟我们父辈经常说的道理差不多，就是坚持、努力、敬业、追求极致等等，其中最为动人的是<strong>“工作就是修行”</strong>这样一个观点。</p><p>在工作中修行，修炼自己的品格，比如耐心、细心、坚持、创新等等。这也跟“大隐隐于市”“修行何必去深山”等类似，人未必需要看特别多的书，其实做事情的过程本身，对人的认知和心性的提升也很大。</p><p>所谓“借事修人”，通过做事情，不断地把自己的缺点克服掉，磨炼出越来越好的品格，最终不仅事业的愿景能达成，自己的品格也能修炼到很高的水平。这是《干法》给我的启示，也推荐给你。</p><h2><a href="https://book.douban.com/subject/30273559">《掌控：开启不疲惫、不焦虑的人生》</a></h2><p><img src="https://static001.geekbang.org/resource/image/8f/98/8fe1bdbaa0bb5b160bfe6c13e37ee098.jpg?wh=390x519" alt=""></p><p>这本书从书名上，看不出是讲健康和健身的，我想作者也是要传达这样一个核心要点：<strong>精力是需要掌控的，有了好的精力，自然就有好的身体和头脑。</strong></p><p>应该说，这本书也切中了都市里很多人的痛点，比如工作和生活很忙没有时间健身，但是身体每况愈下，令人更加焦虑。也揭穿了不少健身方面的一些常见误区，比如“30分钟以上才开始分解脂肪”等等。在合理的锻炼、饮食、休息下，精力得到更好的管理，情绪会变得越来越好，进入一种良性循环。</p><p>我深感我们工程师群体，应该是压力比较大，也容易出现健康状况的人群。在这里我也推荐你多关注一下自己的身体健康，有一副好的身体，不仅自己感觉愉悦，生活和工作也会更有效率，更为顺利。你可以放下手中的《颈椎病康复指南》，跟着《掌控》还有其他健身体育类的书运动起来，让生命的线条更加丰富，让生活的体验更加美好。</p><p>好了，以上就是我的书单推荐，希望这些好书能够充实你的春节假期，也能在新的一年里给你充充电，帮助你更好地成长。另外，如果书单中有触动到你的书，或者你也有想要推荐一读的书，也欢迎在留言区分享出来。我们下次再见。</p>',
        article_title: "春节特别放送（一）| 书单推荐",
      },
      {
        title: "春节特别放送（二）| 聊聊能力陷阱和终身学习",
        id: 484152,
        content:
          "<p>你好，我是胜辉。今天正好是除夕夜，先在这里祝你虎年春节快乐。趁着过年这个轻松的节日气氛，这次我就不讲那些技术知识了，咱们来“务虚”地聊一聊。</p><h2>规避“能力陷阱”</h2><p>当你在某个领域工作了几年后，特别是到了5年乃至10年的时候，你已经在这个领域有了比较深厚的积累。那么恭喜你，此时的你可以说真的是“有所专长”了。</p><p>通过这么多年的积累，你不仅在技术上有颇为独到的见解，也对在这个领域里如何做事情，应该有了自己的沉淀和思路。一方面有技术能力，一方面有做事方法的指导原则，你在这个领域做成事情的概率似乎变得越来越大。</p><p>但是，有时候事情不会总是朝着一个方向走。比如，技术栈本身也经常发生迭代，组织的业务重心也可能发生变动，新的语言和框架不断涌现……这个时候，我们的经验会是推动我们迎接变革的沉着底气，还是反而会成为制约我们的沉重抱负？</p><p>甚至还有这样一种“可怕”的场景，你依赖多年的技术栈忽然有一天被公司告知：这个栈要下去，新的栈要上来。你要不要跟着往前走？</p><p>这其实是已经发生过很多次的事实。从“去IOE”，到“上云”，从授权软件模式到开源软件模式，技术和商业的形式一直在演变。这催生了很多新的技术热点，也悄悄地让不少岗位的光芒逐渐黯淡。</p><!-- [[[read_end]]] --><p>凡此种种，都提醒我们一个重要的事实：技术工作本身就一直在迭代演进，而且其迭代的速度可能远远超过其他行业，在互联网企业这一现象尤为明显。</p><p>当然，我也不是在鼓励内卷，贩卖焦虑。其实，在技术变迁的表象面前，我们内心可以沉静一些，不用过于担心。这是因为：<strong>技术的底层都是相通的</strong>，不同的包装出来的概念和产品，常常是“新瓶装旧酒”，或者是老理论的新实践。</p><p>这是战略上我们可以有用的底气：<strong>只要计算机基础知识掌握得越牢靠，我们就越有希望在技术潮流的动荡中立于不败之地。</strong></p><p>有没有战略，一时半会儿看不出区别，但长远的效果非常明显。在战略的指导下，我们可以看清楚迷雾，认定我们坚持的方向，一步步走向我们的目标。一时的风潮或者人言，也难以扰乱我们的脚步。</p><p>在战术上呢？我觉得还是要重视，特别是要避免前面说的“能力陷阱”。我们通过多年的沉淀，逐步在某个领域建立起来的能力十分宝贵，但同时也不能沉湎其中，要适时地观察周边的技术趋势，结合自己的技术底蕴作出正确的判断，也就是正确的战略。</p><h2>重视“终身学习”</h2><p>但是，想要跳脱出自己的舒适区其实是不容易的，这需要这样一个能力：<strong>终身学习的能力</strong>。其实，与其说是“能力”，不如说是“意识”，是“认知”。不知道你有没有想过，为什么你知道了很多道理，但还是过不好这一生呢？是不是因为：你不去做，或者自认为要做也无法做好，其实本质原因是你并没有真的认同？</p><p>我相信<strong>人人可以终身学习，只要我们从内心认同这个观点</strong>。在技术领域，我觉得这一点尤其重要。</p><p>说说我自己吧，最初工作是在央企做运维，在那个时代的运维，主要靠对操作系统和业务系统的了解来工作，基本很少用到自动化体系。之后很快迎来了虚拟化、云计算、DevOps，我就发现得学习Linux；然后为了让运维工作简便有效率，又去学习Python，写自动化代码；为了把流量管得更好，学习TCP/IP，学习抓包分析；等网络的问题能解决大半的时候，我发现自己对内核的网络行为是一个很大的盲区，于是学习内核网络的知识……</p><p>至今，我还时常惶恐自己的知识掌握还远远不够，正应对了那句话：知道得越多，知道自己不会得也越多。好在，每当我读完一本厚厚的技术著作，或者解决完一个比较棘手的技术问题，内心就平静一些，丰盈一些。这应该就是知识带给我的力量感吧。</p><h3>强化技术思考力，而不是技术能力</h3><p>那是不是我们见到新的技术就扑上去学习，就可以了呢？最近几年的思考，让我觉得<strong>技术思考力，比具体的技术能力更重要</strong>。我记得在一个论坛上看到一名工程师发帖说：“某某大公司来的人，连Docker也不会！”当时就有别的工程师指出：“Docker有什么会不会的呢，如果是怎么用，一两天就熟悉了；如果是熟悉Docker底层代码，那是另说。”</p><p>前一个工程师就是把技术知识误当作技术能力的典型例子。这个问题容易出现在初级工程师身上。其实，对待具体的技术，我们在战略上可以“藐视”，也就是“我们只要想学，总可以学会”。我就算笨一点，别人学一天就会的东西，我就花两天。前提是，我真的认为这个东西值得我花两天。</p><p>那要达到这个认知，就需要你真的想得很清楚，你认同你要学的东西是在你的规划路线上的，是会对你产生价值的，你甚至甘愿在这段时间里<strong>忍受寂寞</strong>。这句话的反面就是，你如果想时时刻刻得到认可，那大概率是会局限在自己擅长的事情上，很难去更广阔的领域为你的下一次升级做储备。</p><h3>打造自己的能力“焦点”</h3><p>我另外一个建议是：<strong>要有焦点，也就是你的核心竞争力</strong>。这个焦点不是说你只去掌握非常细分的领域，而是说，你有一个核心可以提炼的能力特征。</p><p>比如我现在对自己的认知是：我可能在开发上比不过我团队里专门做开发的同学，我可能在思辨上也逊于其他更资深的同事，但我在<strong>网络分析和流量管理</strong>这个主题上，是有非常强的信心的。那么围绕这个核心，我要做的，就是把周边的技术拼图块给完整拼起来。</p><p>为了持续地构造这方面的能力，需要开发的时候就做开发，需要做产品的时候就做产品，需要加强领导力的时候也去学习怎么做到这一点，无论是什么，我们想清楚这件事情在自己规划的职业路线上的价值，那么就会少一些摇摆不定，多几分沉着坚定了。</p><p>如果你是做业务开发的，那么可以想一想，你的核心竞争力是不是这一块业务相关的开发，比如你对业务非常熟悉，开发能力对业务需求来说足够用，那么你大概率会有比较强的竞争力。</p><p>如果你是做运维开发的，那么应该想一想你对系统和网络的掌握，能否明显超过其他工程师，然后思考如何把运维体系做到更高的可用性和更高的效率，甚至是有能力打造出一个自动化运维平台，那么你在这方面的竞争力也会很强。</p><p>我也见过一些不太好的例子。有个别同学对新技术的热情很高，这一点很值得肯定，但问题是见什么都学一点，只学到皮毛，比如学到一些命令、怎么搭建等等，而没有真正理解这个技术或者产品的构建原理是什么，基于什么样的技术设计、为什么要这么设计。</p><p>“见山是山，见水是水”，你觉得每个产品都不一样，那可能是你还没看透它们。在很多地方浅浅地打一口井，甚至都因为太浅而算不上是井，那么说得残忍一点，你投入的时间，并没有获取到对应的该有的能力。<strong>这一点一定要注意。</strong></p><p>当然也有非常正面的例子。有个新同学加入团队，刚开始半年就自己提出，愿意多做一些业务相关的工作，而不是着急着先做开发。这样做了半年，他已经对具体业务摸得很透了，之后开始做业务开发，可以说是顺风顺水，就完全没有别人那种“虽然在做开发，但不知道自己开发的东西是什么用途”的窘况，而是各种项目做一个成一个。</p><p>在这个过程中，这位同学也很快想清楚了自己要打造的核心竞争力是什么，做事情的时候不光自己很聚焦，别人也愿意在各方面给予帮助，因为别人都已经明白他的路线是什么，能给团队带来的帮助是什么。</p><p>好了，今天主要跟你聊的就是这些，也希望你能够不断学习、不断进步。另外，你在技术变迁的过程中，有没有也经历过这种困惑或者选择呢？你自己对终身学习又是怎么看的呢？欢迎在留言区分享出你的观点和看法，我们一同成长。</p>",
        article_title: "春节特别放送（二）| 聊聊能力陷阱和终身学习",
      },
      {
        title: "春节特别放送（三）| 我的学习资料和工具",
        id: 484358,
        content:
          '<p>你好，我是胜辉。今天是正月初二，过年好啊，春节玩得还开心吗？</p><p>在之前的课程中，我发现有不少同学啊，都在问我学习相关的话题，比如说：</p><ul>\n<li>能不能介绍下老师是怎么学习的？</li>\n<li>老师你有没有推荐学习TCP/IP的书单？</li>\n<li>我是Java工程师，怎样可以在网络排查方面快速入门呢？</li>\n</ul><p>所以趁着这个机会呢，我想来跟你聊聊“学习”这个老生常谈的话题。</p><p>其实，极客时间上的牛人太多了，在学习上呢，可能都比我更有经验，也更有成就，那我这里就抛砖引玉吧，一点小小的总结和回顾。如果能帮到你一点点，让你在这个新春时节里觉得有一丝收获，那我会跟你一样开心。</p><h2>做笔记：形成理论体系</h2><p>你有没有发现，当你刚读完一本书的时候，印象还是挺深的，也觉得在这块领域里学到了不少。但是随着时间的推移，对这些知识的印象逐渐模糊，最好笑的是，有时候自己甚至会怀疑：这个技术挺难懂的，也没什么印象了，难道真的是我以前学过的东西吗？</p><p>造成这个问题的原因有很多，我觉得其中比较重要的原因，是没有做好笔记。人接收信息的渠道有很多种，比如听说读写，比如实践操作等等。那其中，写应该是比较容易做到的，所以我也挺早就养成了记笔记的习惯。</p><p>最早我是在笔记本电脑上直接记录文档，比如用Windows的记事本，或者Word文档。当然，它们的缺点也显而易见：在这个移动办公的时代，文档不能在手机等移动设备上查看和编辑，真是一大缺憾。</p><!-- [[[read_end]]] --><p>所以这几年，我就以<strong>在线笔记类App</strong> 为主了，我也用了好几种App，包括印象笔记、有道云笔记、石墨笔记等等。这些App都支持电脑端和手机端，基本上到哪儿都能看笔记或者编辑笔记，十分方便。可以说是让学习深度“侵入”到我的生活中了，但我也乐在其中。看一点笔记，又多懂了一点，以后也可以忘得少一点，这个投入，值得。</p><p>就以印象笔记（国外版叫Evernote）为例，你可以在笔记本、手机、iPad等终端上安装它，然后多设备同步。出门在外，只要你手机上有印象笔记，就可以继续学习。特别是它的Markdown笔记支持代码格式，在阅读一些片段代码的时候很有用。另外，它还有个浏览器插件，当你看到感兴趣的网页时，你就用这个插件把内容给保存为笔记，非常实用。比如像下面这样，我可以把网页中的文稿部分，单独保存为一篇笔记，以后就可以在印象笔记中查看它了：</p><p><img src="https://static001.geekbang.org/resource/image/7e/7f/7ec201e39ba79bd3d0d67ded7723e87f.jpg?wh=1096x343" alt="图片"></p><p><strong>思维导图</strong>也是一类比较有用的学习工具。很多知识未必适合用文档或者表格来做精确的整理，但用思维导图来概括和记录，却更加随性、灵活。我一般在电脑上用XMind。比如下面就是我学习Envoy时候梳理的思维导图：</p><p><img src="https://static001.geekbang.org/resource/image/c4/0c/c400d165f183db6f2c129a246a27510c.jpg?wh=1673x782" alt="图片"></p><p>我觉得，用思维导图的好处之一就是，如果要找某个知识点，很容易从中心点“顺藤摸瓜”找到它。当然好处也不止于此，思维导图的本质，是利用了人的大脑在接收信息方面的特点：<strong>大脑对图形的记忆效率远远超过文字</strong>，通过思维导图，这些知识从文字变成了“树枝图像”，可以更加深刻地印到大脑里。</p><p>我在学习一个新领域的知识的时候，经常会新建一个思维导图文件，然后不断往里添加知识的枝干，之后要复习的时候，一眼就能看到这个体系的脉络，确实很有帮助。除了XMind，国内有个叫MindMaster的思维导图产品也不错。</p><p>当然，你在工作中同样可以使用思维导图。比如我几年前做的一个比较大的项目，涉及因素众多，一开始我就是用思维导图把各个方面摸清楚的。对着思维导图，我比较容易地抓到了问题的关键，之后推进起来就比较容易了。推荐你也可以试试。</p><h2>用正确的方法找学习资料</h2><p>现在这个时代，知识爆炸乃至泛滥，我们在网上随便一搜，可能正确的和错误的知识各占一半，这种惊人的比例，让我们不得不认真审视学习资料来源的问题。有时候找不到信息还算好的，就怕找到的是错误的信息，浪费时间不说，还让错误的知识占据了大脑，那正确的知识就进不来了。所以，在寻找学习资料方面，我的建议有这么几个：</p><ul>\n<li><strong>英文原版</strong></li>\n</ul><p>咱们最好加强一下英文，然后阅读原版书籍。这倒不是否认国内翻译者的贡献，而是相对来说，原版的意思会更加贴近原意，然后时效性也更强一点。甚至有些不错的书，未必会有中文译本，特别是一些技术论文，那你要等待中译版几乎是不可能了。你不能总是期望有“野生翻译君”来帮你。</p><p>其实，技术类的英文不是太复杂，一旦熟悉了技术名词就会容易许多。再加上我们学习的编程语言也都是英文的，我想你基础再差也不会差到哪里去，所以还是要多一点信心，只要你愿意去尝试，阅读英文技术文章或者书籍都不是难事。从它的收效来说，你的“投资”绝对值得。</p><ul>\n<li><strong>要读经典</strong></li>\n</ul><p>现在知识碎片化的趋势越来越明显，不可否认，有时候一小段文字也能给我们不少启迪。不过，要在自己头脑里构建一个扎实并且全面的技术蓝图，还是离不开对经典的研读。</p><p>就拿TCP/IP来说，我还是推荐经典的Richard Stevens等人写就的<a href="https://book.douban.com/series/12438">《TCP/IP详解》</a>，开发的同学可以看<a href="https://book.douban.com/subject/1500149">《UNIX网络编程》</a>。在大多数情况下，读《TCP/IP详解》第一卷应该就满足日常排查的需求了。在操作系统方面，你也可以读一下<a href="https://book.douban.com/subject/3907181">《Linux系统编程》</a>，作者是写过多本技术畅销书的Robert Love，他本人也还年轻，是一名八零后。</p><p>不过，如果你平时开发任务很忙，或者网络基础比较薄弱一点，一下子读大部头确实相对困难，那么你可以选择稍微简单一点的入门书。比如<a href="https://book.douban.com/subject/25863515">《图解HTTP》</a>和<a href="https://book.douban.com/subject/24737674">《图解TCP/IP》</a>，这两本书的作者都来自日本，用了比较多的示意图来展示知识点。这跟我前面谈到的思维导图就很类似，图形会给大脑留下更加深刻的印象，所以用这两本来入门也是不错的选择。</p><p><img src="https://static001.geekbang.org/resource/image/6d/cd/6d029a0d239bb393a13975478bfa9bcd.jpg?wh=872x447" alt="图片"></p><ul>\n<li><strong>要找权威来源</strong></li>\n</ul><p>如果我们随便去网上搜索，可能出来很多个人博客，不少内容是抄来抄去，良莠不齐。我建议你直接到比较权威的站点去查看。比如以HTTP协议来说，要了解各种header和返回码的规范细节，我建议可以去这里看：<a href="https://developer.mozilla.org/en-US/docs/Web/HTTP">HTTP | MDN (mozilla.org)</a> 。</p><ul>\n<li><strong>要读RFC</strong></li>\n</ul><p>另外一块很大的内容，同时也是很容易把你和其他人拉开差距的地方，就是RFC文档。我经常看到，很多人会为了一个技术点争论不休，事实上这个东西就在RFC上清清楚楚地写着，只要用心的话，去找到文档仔细读一下就行。也就花一点时间读一下文档，就能收获一个“专业”的对外形象，你应该更有动力了吧？</p><p>另外，RFC文档一般不是太长，内容精练，基本上没什么废话，所以阅读的投入产出比还是挺高的，这也是我推荐你阅读它的另一个很重要的原因。</p><ul>\n<li><strong>要读源码</strong></li>\n</ul><p>最后一块内容，就是内核源码。这是因为，书相对来说更新起来不是那么及时，看源码可以知道最新的行为。另外，还有两个问题你不得不考虑：</p><ul>\n<li>有些情况没有在RFC里面规定，那这些情况在现实世界里又是如何运作的呢？</li>\n<li>RFC的这些规范，在代码层面是如何具体落实的呢？</li>\n</ul><p>比如我们最近遇到的一个小困惑。我们发现，TCP三次握手里面，第二个报文也就是SYN+ACK，它的IP ID居然是0。</p><p><img src="https://static001.geekbang.org/resource/image/b1/a7/b1c76b580c70b24f9d537154f5df27a7.jpg?wh=374x167" alt="图片"></p><p>我们就直接查Linux内核源代码，确认是对端Linux自身的行为了。这样的例子有很多。总之，脱离了代码谈网络，总还是会有盲区。</p><h2>做中学，学中做</h2><p>“纸上得来终觉浅”，我们可以在测试环境中，不断把自己学到的知识通过实验进行验证，加深印象。比如，对于TCP/IP的各种行为，你就可以一边curl，一边tcpdump，观察输出的报文细节，再结合自己学到的理论，看看是否能解释通了？如果还有疑问，那么恭喜你，又可以进步了！人学习的过程，不就是一个不断发现漏洞，不断弥补的过程吗？</p><p>如果在实际工作中遇到网络问题，或者你怀疑是网络引起的问题，那么也是很好的机会。你可以参考我在课程里介绍的步骤，比如做抓包，然后Wireshark打开做分析。对着看得见的报文来学习网络协议，这个体验确实比单纯看书或者RFC有趣得多，也更容易坚持。这也是我的一个小秘诀，分享给你。</p><p>另外，你也可以参考这么一条学习和实践路线，来逐步深入掌握TCP/IP网络知识：</p><p><img src="https://static001.geekbang.org/resource/image/5f/cb/5f5203c9cc875bef70d44484699264cb.jpg?wh=1920x538" alt="图片"></p><p>好了，我的学习资料和学习工具，大概就介绍到这里。欢迎你畅所欲言，在留言区里分享出你认为好用的学习方法，相信我们都可以学到一大筐非常好用的工具和资料，我也很期待！</p>',
        article_title: "春节特别放送（三）| 我的学习资料和工具",
      },
      {
        title: "春节特别放送（四）| 测一测你的网络排查能力",
        id: 484454,
        content:
          '<p>你好，我是胜辉。</p><p>咱们的春节假期就快要结束了，不知道你对课程的理解和学习有没有放松过呢？这里我给你准备了一份测试题，一共有10道单选题，满分100，核心考点都出自课程前面讲到的所有重要知识，希望可以帮助你进行一场自测。</p><p>好了，话不多说，请你来做一做这套测试题吧！</p><p><a href="http://time.geekbang.org/quiz/intro?act_id=1567&exam_id=3858"><img src="https://static001.geekbang.org/resource/image/28/a4/28d1be62669b4f3cc01c36466bf811a4.png?wh=1142*201" alt=""></a></p><!-- [[[read_end]]] -->',
        article_title: "春节特别放送（四）| 测一测你的网络排查能力",
      },
    ],
  },
  {
    chapterTitle: "实战二：应用层真实案例揭秘篇",
    children: [
      {
        title: "15 | Nginx的499状态码是怎么回事？",
        id: 488979,
        content:
          '<p>你好，我是胜辉。</p><p>“实战一：TCP真实案例解密篇”刚刚结束。在过去的十几讲里，我们全面回顾了TCP的各种技术细节，从握手到挥手，从重传等容错机制，到传输速度等效率机制，应该说也是对我们的TCP知识做了一个全面的“体检”。如果你发现自己对TCP的掌握还有不少漏洞，也别着急，可以回头复习一下相应部分的内容，或者在留言区提问，我会给你解答。</p><p>从这节课开始，我们要进入网络排查的“实战二：应用层真实案例解密篇”了。今天要给你讲解的是一个关于Nginx的排查案例。</p><h2>Nginx的499状态码是怎么回事？</h2><p>你肯定听说过Nginx，或者经常用到它。作为一个高性能的HTTP和反向代理服务器，Nginx不管是用来搭建Web Server，还是用作负载均衡都很合适，并且它可供配置的日志字段也很丰富，从各类HTTP头部到内部的性能数据都有。</p><p>不过，你在日常维护Nginx时有没有遇到过这种情况：<strong>在Nginx的访问日志中，存在499状态码的日志。</strong>但是常见的4xx家族的状态码只有400、401、403、404等，这个499并未在HTTP的RFC文档中定义，是不是很奇怪？</p><p>这个499错误日志，在流量较大的场景下，特别是面向Internet的Web站点场景下还是很常见的 。但如果你遇到过，第一感觉可能会是一头雾水，不知道499这个状态码具体是用来干啥的，因为确实跟其他的400系列状态码太不同了。</p><!-- [[[read_end]]] --><p>我在公有云的时候，做过的一个案例正好是关于Nginx的499日志。当时一位客户向我反馈：他们的Nginx服务器会连续几天记录较多的499错误日志，之后几天可能趋零，然后再回升，整体状况起伏不定。</p><p>这个客户经营的是To C的电子产品，跟手机端App协同工作。这个App会定时把消息上传到微信消息网关，后者再把这些消息推送到该客户的服务端（在公有云上）做业务处理，整体的消息量约每日三十万条。那么，对消息网关来说，这个服务端就是一个Web回调接口。下面是架构简图：</p><p><img src="https://static001.geekbang.org/resource/image/93/46/93ea615940b140d18e2c3087e0545346.jpg?wh=1697x809" alt=""></p><p>他们给我提供了499日志趋势图：</p><p><img src="https://static001.geekbang.org/resource/image/4d/71/4d99d187880ecaa09e454526e8a9b371.png?wh=1405x368" alt="图片"></p><p>由于大量499日志的存在，客户非常担心业务已经受到影响，比如他们的终端消费者是否经常上传数据失败？是否已经严重影响了消费者的体验？所以，我们需要搞清楚499错误日志的含义。</p><p>那么，499这个状态码本身能帮到我们什么呢？我们可以查一下它在Nginx里的<a href="https://www.Nginx.com/resources/wiki/extending/api/http">官方定义</a>：</p><blockquote>\n<p>NGX_HTTP_CLIENT_CLOSED_REQUEST     |    499</p>\n</blockquote><p>可是，什么叫client closed request（客户端关闭了请求）呢？好像说了跟没说也没太大区别。我们知道499是客户端关闭请求引起的，那又是什么原因，引起了“客户端关闭了请求”呢？关于这个问题，Nginx的文档并没有提及。</p><p>有一句话叫做“解决问题的办法，可能不在问题自身所处的这个层面”。<strong>应用层日志，其实记录的依然是表象。</strong>更深层次的原因，很可能在更底层，比如在传输层或者网络层。</p><p>所以，搞清楚499这个状态码是什么意思，对于我们来说，不仅是理解这个499码的底层含义，而且通过这种排查，我们还能掌握一套<strong>对HTTP返回码进行网络分析的方法</strong>。这种方法，对于维护好Nginx以及其他Web服务，都是很有帮助的。</p><p>那么接下来，我们就根据这个案例，一起探讨下如何用抓包分析，来拆解HTTP返回码的真正含义。</p><h2><strong>锚定到网络层</strong></h2><p>如前面所说，我是选择用<strong>抓包分析</strong>这个方法来展开排查的。之所以采用这个方法，是因为我前面也说过，从软件文档已经无法查清楚问题根因了，所以需要下沉到网络层排查。如果你在处理应用层故障，比如HTTP异常返回码（4xx和5xx系列）场景中，也遇到了在应用层找不到答案的情况，你就可以考虑采用抓包分析的方法。</p><blockquote>\n<p>补充：下文中的“客户端”都指微信消息网关，“服务端”指这个客户在公有云的服务器。</p>\n</blockquote><p>这样，我在<strong>服务端</strong>使用tcpdump工具做了抓包，然后用Wireshark打开抓包文件展开分析。从抓包文件中，我一般会寻找一些比较可疑的报文。正好，这次抓包里有不少RST报文，于是我过滤出了一个典型的带RST报文的TCP流，请看下图：</p><p><img src="https://static001.geekbang.org/resource/image/63/8c/63546e3e94690a685f9291eyy837318c.jpg?wh=2418x446" alt=""></p><blockquote>\n<p>补充：抓包示例文件已经上传至<a href="https://gitee.com/steelvictor/network-analysis/tree/master/15">Gitee</a>，建议用Wireshark打开文件，结合文稿一起学习。</p>\n</blockquote><p>相信你也一眼就看到了那个结尾处的RST。但问题是，<strong>这个TCP流一定跟499日志有关系吗？</strong></p><p>得益于TCP/IP的精妙的分层设计，应用层只需要通过系统调用，就可以像使用文件IO那样使用网络IO，具体的网络细节都由内核处理了。可是由此也带来了一个问题：<strong>以应用层的视角，是无法“看到”具体的网络报文的</strong>。</p><p><img src="https://static001.geekbang.org/resource/image/ba/a5/ba258d0aa729da8dc7d35a746cec55a5.jpg?wh=1893x831" alt=""></p><p>我们需要根据一些关键信息来确定应用层日志跟网络报文的对应关系。比如在这里，我可以确认上面这个带有RST的TCP流，就是日志中记录的一条499日志记录。这是如何做到的呢？</p><p>就是因为以下三点。</p><ul>\n<li><strong>客户端IP</strong>：日志中的remote IP跟抓包文件里面的IP符合。</li>\n<li><strong>时间戳</strong>：日志的时间戳也跟这个TCP流的时间吻合。</li>\n<li><strong>应用层请求</strong>：日志里的HTTP URL路径和这个TCP流里的URL相同。</li>\n</ul><blockquote>\n<p>补充：如果你对<a href="https://time.geekbang.org/column/article/480068">第4讲</a>有印象，应该记得当时也是用类似的方式找到了应用日志跟报文的对应关系。</p>\n</blockquote><p>实际上，在真实的抓包分析场景中，“如何把应用层问题跟网络层抓包关联起来”，始终是一个关键环节。同时，这也是比较令人困扰的关键技术障碍，很多人就是在这一关前败下阵来，导致没有办法真正彻底地查到根因。所以这里的方法可以作为给你的参考，当你以后再处理这种关键环节的时候，也可以根据上面提到的三个维度的信息，即IP、时间戳、应用层请求（包括URL和header），来达到“把应用层问题锚定到网络层数据包”的目的。</p><p>好，既然确定这个流就是代表了一次499事件，那么我们就需要好好分析一下这些报文里面的文章了。</p><h2>TCP流的解读</h2><p>这里，你可以先注意一下我在下图的这个TCP流示意图中，标记出的红框部分，在后续的分析过程中，我会重点分析这几个部分。</p><p><img src="https://static001.geekbang.org/resource/image/e3/93/e32e1ef5e9006c7ef650d7a60930d193.jpg?wh=2230x452" alt=""></p><p>首先是报文1~3，表示TCP握手成功。</p><p>然后是报文4（客户端发出），表示客户端（消息网关）向服务器发送报文，这个报文里只包含HTTP header，其声明该请求为POST方法，但不含POST body。这其实是正常的，因为HTTP协议就是这样规定的，数据的先后顺序是：先header（包含method、URL、headers），后body。所以，既然方法（method）和URL单独位于一个报文里面了，那么按顺序来说，body就是在后续的报文里面。</p><p>接下来是报文5（服务端发出），它是一个确认报文。它的意思是：我（服务端）确认收到了你（客户端）发过来的报文4。</p><p>紧接着是报文6（客户端发出），此时距离上一个报文的时间是2秒。这个报文被Wireshark标记为了红色，注释为TCP Previous segment not captured，意思是它之前的TCP报文段没有被抓到。</p><p>什么叫做“之前的TCP报文”呢？其实就是按TCP序列号顺序，排在当前报文之前的报文。我对这个6号报文标注了3处红框，它们都有很重要的含义。这里我们先关注下右边一个红框圈出来的FIN标志位，这说明，<strong>这是一个客户端主动关闭连接的报文。</strong></p><p>我们可以把到目前为止的报文情况，用下面这个示意图来表示：</p><p><img src="https://static001.geekbang.org/resource/image/54/85/54b3cf96a80e2ac2638840c226ea7285.jpg?wh=2000x1125" alt=""></p><p>你看这里是不是很奇怪？明明HTTP POST请求的body（也称为HTTP载荷）部分还没发过来，这个客户端就嚷嚷着要关闭连接了？这就好比有个朋友跟你说：“我有个事情要你帮忙，嗯，拜拜~”，你刚听到上半句他的求助意向，还没听到这个忙具体是什么，他就跟你说再见了。惊不惊喜，意不意外？你可能暂时看不出这里究竟出了什么问题，不过没关系，先放一放。</p><p>我们继续看报文7（服务端发出）。服务器收到了FIN+ACK报文（6号报文），但发现序列号并不是它期待的309，而是777，于是服务器TCP协议栈判断：有一个长度为777-309=468的TCP段（TCP segment）丢失了。</p><p>按TCP的约定，这时候服务端只可以确认其收到的字节的最后位置，在这里，就是上一次（报文5）的ACK位置。形式上，报文7就成了一个DupAck（重复确认）。</p><p>当客户端收到DupAck的时候，它就需要长一个心眼了：“情况有点微妙，如果凑满3个DupAck可能有丢包啊”。</p><blockquote>\n<p>补充：如果凑满3个DupAck就重传的机制，被称为快速重传机制，我们在<a href="https://time.geekbang.org/column/article/487082">第12讲</a>我们有深入学习过。</p>\n</blockquote><p>为了帮助理解，这里我再展示下报文4的TCP信息：</p><p><img src="https://static001.geekbang.org/resource/image/b2/83/b253c16c2b861402a215fec7d2598883.jpg?wh=1406x396" alt=""></p><p>那么，按TCP的设计，客户端将要发送的下一个报文的序列号（309）=&nbsp;本次序列号（1）&nbsp;+&nbsp;本次数据长度（308），也就是图中的Next sequence number。</p><p>我们再来看报文8（客户端发出），过了16秒之久，客户端<strong>重传</strong>了这个报文，包含POST body的数据，长度为 <strong>468</strong> 字节。你看，这是不是就跟前面说的777-309=468对应起来了。</p><p>可能你在这里又有点困惑了，明明这个468字节的报文是第一次出现，怎么就算重传了呢？</p><p>其实是因为，这个抓包文件是在服务端生成的，所以它的视角，是无法看到多次传送同样这个报文的现象的。但我判断，在客户端抓包的话，一定可以看到这个468字节的报文被试图传送了多次。</p><p><img src="https://static001.geekbang.org/resource/image/1e/c9/1ed4b43aba2eb399a8137d0a6f9869c9.jpg?wh=1639x722" alt=""></p><p>我们就以服务端视角来判断，一开始这个报文应该是走丢了，没有达到服务端，所以没有在这个服务端抓包文件里现身。又因为过了16秒之久才到达，很可能不是单纯一次重传，而是多次重传后才最终到达的。因此从这一点上讲，确实属于重传。</p><p>我们继续分析。接下来就是报文9，服务端对这个POST body的数据包回复了确认报文。</p><p>最后是报文10，服务端发送了HTTP 400的响应报文给消息网关。这个信息并没有被Wireshark直接按HTTP格式进行展示，但是因为HTTP是文本编码的，所以我们可以鼠标选中Transmission Control Protcol部分，在底下的文本栏直接看到HTTP 400这段文本：</p><p><img src="https://static001.geekbang.org/resource/image/ae/2a/aeed98d07a4cbaeb97a202877e33cc2a.jpg?wh=1656x850" alt=""></p><p>有趣的是，这个 <strong>HTTP 400报文也是带FIN标志位的</strong>，也就是服务端操作系统“图省事”了，把应用层的应答数据（HTTP 400），跟操作系统对TCP连接关闭的控制报文（这个FIN），合并在同一个报文里面了。也就是我们在<a href="https://time.geekbang.org/column/article/479163">第3讲</a>提到的搭顺风车（Piggybacking），提升了网络利用效率。</p><p>这个阶段的报文图示如下：</p><p><img src="https://static001.geekbang.org/resource/image/37/e2/37c17a924795069688bed6b9d69838e2.jpg?wh=1523x739" alt=""></p><p>那么，从这些报文的顺序来看，我们会发现它确实是有问题的。特别是有以下几个疑点：</p><ul>\n<li>服务端先收到了HTTP header报文，随后并没有收到期望的HTTP body报文，而是收到了FIN报文，即客户端试图关闭连接。这个行为十分古怪，要知道HTTP请求还没发送到服务端，服务端回复HTTP响应更是无从谈起，这个时候客户端发送FIN就不符合常理了（即前面说的朋友求帮忙的类比）。</li>\n<li>服务端回复了HTTP 400，并且也发送FIN关闭了这个连接。</li>\n<li>客户端回复RST彻底关闭这个连接。</li>\n</ul><p>而把上面这几条信息综合起来看，你有没有发现一个重要的线索？<strong>客户端先发送了FIN，之后才发送POST body。</strong>现在让我们把全部过程拼接起来，看一下全景图：</p><p><img src="https://static001.geekbang.org/resource/image/a4/17/a45e1cf729d62b8e7a4ce6258e3fe817.jpg?wh=2000x1125" alt=""></p><p>这么古怪的行为，可以描述为“<strong>服务端还没回复数据而客户端已经要关闭连接</strong>”。按照499的官方定义，这种行为就被Nginx判定为了499状态。对内表现为记录499日志，对外表现为回复HTTP 400给消息网关。</p><p>所以，在服务端的Nginx日志中，就留下了大量的499日志条目；而在消息网关那头，如果它也做Web日志的话，相信就不是499日志，而是400的报错了。</p><p>那么到这里，问题是水落石出了吗？其实不是。</p><h2><strong>从现象到本质</strong></h2><p>我们还需要搞清楚最底层的疑问：为什么客户端先发送FIN，然后才发送POST body？</p><p>我们回到Wireshark窗口，再次关注下6号报文：</p><p><img src="https://static001.geekbang.org/resource/image/d2/01/d2ee60e008e21e9b971ac7894f915601.jpg?wh=1712x84" alt=""></p><p>它离上一个报文相差了2秒，而我们知道这个信息，是因为Wireshark很友好地显示了报文之间的间隔时长。</p><p>我们再往前看4号报文：</p><p><img src="https://static001.geekbang.org/resource/image/8f/29/8f79f673e07621c9a8dyycyy151c1929.jpg?wh=1568x82" alt=""></p><p>离3号报文相差了2.997秒，几乎就是3秒整了。那么加起来，6号报文离TCP握手完成，正好隔了 <strong>5秒整</strong>。</p><p>一般出现这种整数，就越发可疑了，因为如果是系统或者网络的错乱导致的行为，其时间分布上应该是<strong>随机的</strong>，不可能卡在整数时间上。就我的经验来看，<strong>这往往跟某种人为的设置有关系</strong>。</p><p>所以，经过我的提醒，客户自己仔细查看了微信网关的使用文档，果然发现了它确实有5秒超时的设置。也就是说，如果一个HTTP事务（在这个例子里是HTTP POST事务）无法在5秒内完成，就关闭这个连接。</p><p><img src="https://static001.geekbang.org/resource/image/1d/80/1d35834f2da80aa64da94b0edca08980.jpg?wh=1659x711" alt=""></p><p>这个“无法完成”，在这个抓包里面体现为：HTTP header报文发过去了，但HTTP body报文没有一起过去（网络原因导致）。而由于初始阶段报文少，<strong>无法凑齐3个DupAck</strong>，所以快速重传没有被启动，只好依赖超时重传（关于超时重传的知识在<a href="https://time.geekbang.org/column/article/487082">第12讲</a>也有详细的介绍），而且这多次超时重传也失败了，服务端只好持续等待这个丢失的报文。5秒钟过后，客户端（微信消息网关）没有收到服务端的响应，就主动关闭了这次连接（可以下次再试，这次就不继续干等了）。</p><p>也就是说，这个场景里的Nginx 499错误日志的产生，主要是由于两个因素造成的：</p><ul>\n<li><strong>“消息网关—&gt;服务器”方向上的一个TCP包丢失（案例里是HTTP POST body报文），引起服务端空闲等待；</strong></li>\n<li><strong>消息网关有一个5秒超时的设置，即连接达到5秒时，消息网关就发送FIN关闭连接。</strong></li>\n</ul><p>所以到这里，想必你也明白了这里的逻辑链条，也就是：</p><ul>\n<li>要解决499报错的问题，就需要解决5秒超时的问题；</li>\n<li>要解决5秒超时的问题，就需要解决丢包问题；</li>\n<li>要解决丢包的问题，就需要改善网络链路质量。</li>\n</ul><p>最根本的解决方案，就是如何确保客户端到服务端的<strong>网络连接</strong>可靠稳定，使得类似的报文延迟的现象降到最低。只要不丢包不延迟，HTTP事务就能在5秒内完成，消息网关就不会启动5秒超时断开连接的机制。</p><p>这样，我们跟客户还有网关的工程师一起配合，确实发现网关到我们公有云的一条链路有问题。更换为另外一条链路后，丢包率大幅降低，问题得到了极大改善。虽然还是有极小比例的错误日志（大约万分之一），但是这对于客户来说，完全在可接受范围之内了。</p><p>另外，因为丢包的存在，客户端的FIN报文跟HTTP POST body报文一样，也可能会丢失。不过，无论这个FIN是否被服务端及时收到，这次HTTP事务本身也已经在客户端被记为失败了，也就是不改变这件事的结果。</p><p>你可能会问了：链路丢包这种问题应该挺明显的，为什么没有在第一时间发现呢？</p><p>这其实是多种因素导致的：</p><ul>\n<li>我们虽然对主要链路的整体状况有细致的监控，但这里的网关到客户的公有云服务属于“点到点”的链接，本身也属于客户自身的业务，公有云难以对这种情况做监控，理想情况是客户自己来实现监控。</li>\n<li>客户的消息量很大，哪怕整体失败比例不高，但乘以绝对的消息量，产生的错误的绝对数也就比较可观了。</li>\n</ul><p>至于Nginx为什么要“创造”499这个独有的状态码的原因，其实在 <a href="https://github.com/Nginx/Nginx/blob/a6cb8210905f35977276cb3861184e4dad99cc2a/src/http/ngx_http_request.h">Nginx源码</a>的注释部分里，已经写得非常清楚了。它并非标新立异，而确实是为了弥补标准HTTP协议的不足。相关代码如下：</p><pre><code>/*\n * HTTP does not define the code for the case when a client closed\n * the connection while we are processing its request so we introduce\n * own code to log such situation when a client has closed the connection\n * before we even try to send the HTTP header to it\n */\n#define NGX_HTTP_CLIENT_CLOSED_REQUEST     499\n</code></pre><p>翻译过来就是：HTTP并没有对服务端还在处理请求的时候客户端就关闭连接的情况，做一个状态码的定义。所以我们定义了自己的状态码（499），以记录这种“还没来得及发送返回，客户端就关闭了连接”的情形。</p><h2>小结</h2><p>现在，我们就清楚在这个例子里，造成499状态码的根因了。不过基于普适性的应用需求，我想把这个案例再延伸拓展一下，希望可以帮助你了解到更多的知识，并且在理解了这些知识点之后，你能够有效应用在类似的HTTP异常码的故障排查里。</p><p>首先，我们要知道，<strong>Nginx 499是Nginx自身定义的状态码，并非任何RFC中定义的HTTP状态码</strong>。它表示的是“Nginx收到完整的HTTP request前（或者已经接收到完整的request但还没来得及发送HTTP response前），客户端试图关闭TCP连接”这种反常情况。</p><p>第二，<strong>超时时间跟499报错数量也有直接关系</strong>。如果我们有办法延长消息网关的超时时间，比如从5秒改为50秒，那么客户端就有比较充足的时间去等待丢失的报文被成功重传，从而在50秒内完成HTTP事务，499日志也会少很多。</p><p>第三，<strong>我们要关注网络延迟对通信的影响</strong>。比如客户端发出的两个报文（报文3和报文4）间隔了3秒钟，这在网络通信中是个非常大的延迟。而造成这么大延迟的原因，会有两种可能：一是消息网关端本身是在握手后隔了3秒才发送了这个报文，属于<strong>应用层问题</strong>；二是消息网关在握手后立刻发送了这个报文，但在公网上丢失了，微信消息网关就根据“超时重传”的机制重新发了这个报文，并于3秒后到达。这属于<strong>网络链路问题</strong>。</p><p>由于上面的抓包是在服务端做的，所以未到达服务器的包自然也不可能抓到，也就是无法确定是具体哪一种原因（客户端应用层问题或网络链路问题）导致，但这并不影响结论。</p><p>最后一点，就是我们要清楚，<strong>公网上丢包现象不可能完全消失</strong>。千分之一左右的公网丢包率属于正常范围。由于客户发送量比较大（这是主要原因），加上微信消息网关设置的5秒超时相对比较短（这是次要原因），这两个因素一结合，问题就会在这个案例中被集中暴露出来。</p><p>那么，像上面第二点说的那样，设置更长的超时阈值（比如50秒）能解决问题吗？相信出错率会降低不少，但是这样新的问题也来了：</p><ul>\n<li>消息网关会有更多的资源消耗（内存、TCP源端口、计算能力等）；</li>\n<li>消息网关处理事务的平均耗时会增加。</li>\n</ul><p>所以，选择5秒应该是一个做过权衡后的适中的方案。</p><p>而从排查的方法论上来说，对于更广泛的应用层报错日志的排查，我的推荐是这样的：</p><ul>\n<li><strong>首先查看应用文档，初步确定问题性质，大体确定排查方向。</strong></li>\n<li><strong>通过对比应用日志和抓取的报文，在传输层和网络层寻找可疑报文。</strong>在这一步，可以采用以下的比对策略来找到可疑报文：\n<ul>\n<li>日志中的IP跟报文中的IP对应；</li>\n<li>日志和报文的时间戳对应；</li>\n<li>应用层请求信息和报文信息对应。</li>\n</ul>\n</li>\n<li><strong>结合协议规范和报文现象，推导出根因。</strong></li>\n</ul><h2><strong>思考题</strong></h2><p>给你留两个思考题，欢迎在留言区分享你的答案和思考，我们一起交流讨论。</p><ul>\n<li>第7个报文是DupAck，为什么没有触发快速重传呢？</li>\n<li>消息网关那头的应用日志应该不是499，那会是什么样的日志呢？</li>\n</ul><p>欢迎你把今天的内容分享给更多的朋友，我们一起成长。</p>',
        article_title: "15 | Nginx的499状态码是怎么回事？",
      },
      {
        title: "答疑（三）| 第11~15讲思考题答案",
        id: 495852,
        content:
          '<p>你好，我是胜辉。这节课我们继续来解答前面课程的思考题。</p><p>第11到14讲的内容呢，主要是关于TCP传输的，涵盖了各种TCP重传、超时、拥塞控制、DDoS等我们都比较关心的问题，这些知识也可以说是TCP体系里比较复杂的部分了。到了第15讲，就进入应用层真实案例了，可能离开发和运维开发的同学更近一些，也相信你对此有不少的问题要问、不少的话要说。那我们就在这节答疑课里进一步沟通吧，我们从第11讲开始。</p><h2>11讲的答疑</h2><h3>思考题</h3><p>你在工作中有没有遇到拥塞引起的问题，或者有没有在抓包分析过程中，观察到过拥塞现象呢？</p><h3>答案</h3><p>这是一个开放式的问题，可以激发我们对这个话题的思考。可能是因为<a href="https://time.geekbang.org/column/article/486281">这一讲</a>的内容理论性多过了实践性，所以留言不算多，但是每个问题都很有价值。比如<strong>@Chao</strong>同学的问题：</p><blockquote>\n<p>慢启动阈值（ssthresh）的初始值是如何确定的，当慢启动过程中没有进入重传状态，如何进入拥塞避免状态？</p>\n</blockquote><p>这确实是一个很好的问题。我在这一讲里（也包括其他很多资料）介绍的TCP慢启动过程，是下图这样的：</p><p><img src="https://static001.geekbang.org/resource/image/78/52/78b3a9eacab3f1a6332efa1259255b52.jpg?wh=2000x1125" alt=""></p><p>其中，第二次慢启动阈值是基于第一次拥塞窗口减半得到的。那么第一次慢启动阈值又是如何得到的呢？是不是也跟初始拥塞窗口一样，初始慢启动阈值也有一个默认值呢？</p><!-- [[[read_end]]] --><p>在具体的实现上，Linux（应该也包括其他OS）的第一次慢启动阈值，实际上是“<strong>无穷大</strong>”。它被定义为：</p><pre><code class="language-bash">#define TCP_INFINITE_SSTHRESH   0x7fffffff\n</code></pre><p>这个十六进制的 <code>7fffffff</code> 的十进制值，是2147483647，也就是2G。我们知道，发送窗口是拥塞窗口和对方的接收窗口之间的较小值。由于接收窗口理论最大值也只有1G，因而发送窗口的最大值也是1G，那么拥塞窗口超过1G也已经没有意义了，所以这里的2G，事实上就是无穷大。</p><p>所以说，<strong>在Linux的实现里，初始慢启动阈值肯定在第一个拥塞点之上。</strong>这就造成一个现象：Linux的TCP连接在慢启动后，先碰到的是拥塞点，而不是初始慢启动阈值。从现象上看就是，TCP的拥塞窗口在慢启动过程中不断爬升，直到遇到第一个拥塞点（发生丢包或者超时），此时这个拥塞窗口的一半，就是第二次慢启动阈值了。示意图如下：</p><p><img src="https://static001.geekbang.org/resource/image/6d/06/6d5b42604b43b06de79724ce59d28506.jpg?wh=2000x1125" alt=""></p><p>那么，Linux为什么会选择这样的初始慢启动阈值呢？</p><p>主要原因，是当今的网络条件比多年前好了很多，所以为了“压榨”网络性能，让传输的启动阶段<strong>尽可能快地达到理想的传输速度</strong>，Linux在传输刚开始还没有网络质量信息的时候，直接用了“碰到拥塞再快速恢复”的方式。这比预设一个折中的初始慢启动阈值的情况，会更快地达到理想的传输速度。</p><h2>12讲的答疑</h2><h3>思考题</h3><ol>\n<li>TCP的确认报文如果丢失了，发送端还会不会重传呢？为什么？</li>\n<li>你有没有遇到过重传引发的问题，你是怎么处理的呢？</li>\n</ol><h3>答案</h3><p>在回答第一个问题之前，我们先思考另外一个问题：<strong>TCP对数据报文有确认机制，对确认报文也有确认机制吗？</strong>比如是否会这样，就是A给B发一个数据报文，B回复一个确认报文，A再回复一个对确认报文的确认报文，然后B也如此操作一番……</p><p><img src="https://static001.geekbang.org/resource/image/67/65/67a9a7aa24e1e8b70a89ba3f29852365.jpg?wh=1745x1000" alt=""></p><p>这就无穷无尽了，显然不是正确的做法。所以在TCP里，确认报文只发送一次。那你可能会问：“假如确认报文丢失了，那对端如何知道这件事呢？”</p><p>其实，这就是<strong>超时重传</strong>机制的用武之地了。比如，同样还是上面的图中，第一个ACK丢失的话，B也不会重传这个ACK，而A就会等待这个ACK。一旦这个时间达到重传超时阈值（200ms以上），A就会启动超时重传机制。</p><p><img src="https://static001.geekbang.org/resource/image/c6/e4/c67a30cea019d991b35600f1b0295ae4.jpg?wh=1842x988" alt=""></p><p>当然，另一种很常见的情况是A还有更多的数据报文要发，于是情况就从超时重传，演变为快速重传：</p><p><img src="https://static001.geekbang.org/resource/image/a0/33/a0b6670ff9a4320439bea3cff8cfa933.jpg?wh=1665x887" alt=""></p><p>第二个问题，<strong>@ERROR404</strong>同学的回复是这样的：</p><blockquote>\n<p>重传还是看影不影响到业务，实际是允许重传存在的，比如像是互联网线路丢包，或者是营销等而引起流量突升的重传。</p>\n</blockquote><p>说得没错。其实重传也是TCP的必要特性，也正是<strong>通过重传，TCP实现了传输可靠性这一根本性的优势和特点</strong>。反观UDP，它就没有重传机制，当然也不能保证传输可靠性了。所以在UDP应用里，一般业务本身就是允许丢包的，或者在应用层自己实现传输可靠性，比如实现类似TCP重传的机制。</p><p>重传本身不是TCP的问题，可以说反而是它的特性，而引起重传的主要原因——丢包，才是真正的问题。这一般是链路质量导致的，常常要从TCP之外寻求答案。</p><p>关于重传的知识点，我在第11~13讲里都做了比较全面的介绍，希望对你建立起对重传的正确认识有所帮助。</p><h2>13讲的答疑</h2><h3>思考题</h3><ol>\n<li>如果接收端收到一个确认包，其确认号为200，而当前的被确认的位置在500，那么接收端会怎么处理这个看起来“迟到并且重复”的确认包呢？</li>\n<li>你有没有遇到过这种“确认号在中间位置”的情况？当时有没有引起什么问题呢？</li>\n</ol><h3>答案</h3><p>第一个问题，就是说收到的报文确认了一个之前已经确认过的位置，如下图：</p><p><img src="https://static001.geekbang.org/resource/image/fb/7e/fb161e19560157f603ffb70d8660d77e.jpg?wh=2000x686" alt=""></p><p>这种情况，接收端可以“无视”。因为确认号是表示收到的连续字节的最新的位置，那么显然，一个数字接近但是更低的确认号（不是序列号回绕复用的那种情况），等于是一次信息的重复，接收端不会做任何处理。</p><blockquote>\n<p>补充：当然，因为序列号是4G之内循环使用的，当序列号越过4G后，下一个序列号就从低处开始了。不过因为这样的两个序列号离得远，并不是上面的那种情况。</p>\n</blockquote><p>第二个问题，确认号在中间的情况，暂时没有同学在留言区说遇到过。这样也挺好的，我们已经知道原理了，不来最好，来也不怕，对不对？</p><h2>14讲的答疑</h2><h3>思考题</h3><ol>\n<li>“肉机”发出100Mbps的攻击流量，到达被攻击站点的时候，仍然是100Mbps吗？为什么呢？</li>\n<li>为什么CDN可以达到缓解DDoS的效果呢？</li>\n</ol><h3>答案</h3><p>第一个问题的核心，其实是关于公网流量的“衰减”问题。从网络路径来看，从“肉机”出来一直到达被攻击站点，途径的网络跳数一般也有10到20多跳。每一跳其实都有可能会遇到不同程度的衰减，比如以下这些：</p><ul>\n<li>“肉机”本身上行线路的<strong>带宽限制</strong>。</li>\n<li>攻击流量在公网路径上因拥挤而导致的<strong>流量损失</strong>。</li>\n</ul><p>一般来说，“肉机”初始的攻击流量，在到达被攻击站点的时候，只有出发时的几分之一。因而作为攻击者，肯定要越发加大起始流量，这样经过公网衰减之后，还能有不少的流量到达被攻击站点。</p><p><img src="https://static001.geekbang.org/resource/image/e3/b7/e30e9a02ab9528005a033ece0a9779b7.jpg?wh=2000x1065" alt=""></p><p>第二个问题跟 <strong>IP数量和DNS解析</strong>有关系。一般站点的IP就几个，而CDN因为是分布式的，一般能达到几十个IP（几十个点）之多，而且不同地点解析到的IP也不同。这样的话，在DDoS攻击时，攻击流量也被分散到几十个IP，这就被平均分散掉了。除了受影响严重的地区，其他地区依然能提供服务，这就在事实上起到了一定的防护效果。</p><p>当然，这并不是鼓励你要用CDN来扛DDoS，而且它本身还有很多问题，比如：</p><ul>\n<li>这些CDN节点覆盖的地区失去了服务能力；</li>\n<li>对这些CDN节点的其他客户产生了严重的影响。</li>\n</ul><p>那么，如果有人就是想把CDN当“廉价高防”来用，该怎么办呢？其实这个时候，CDN服务商也不会默默地忍受，对于恶意使用CDN的情况，也是有可能“劝退客户”的。</p><h2>15讲的答疑</h2><h3>思考题</h3><ol>\n<li>第7个报文是DupAck，为什么没有触发快速重传呢？</li>\n<li>消息网关那头的应用日志应该不是499，那会是什么样的日志呢？</li>\n</ol><h3>答案</h3><p>第一个问题，这第7个报文确实是DupAck，但是DupAck只有它一个，而快速重传的条件是什么？是<strong>累积有3个或者以上的DupAck</strong>。</p><p>所以显然，单凭这一次的DupAck是无法触发快速重传的。</p><p>第二个问题，其实在<a href="https://time.geekbang.org/column/article/488979">课程里</a>我有提到，就是：</p><blockquote>\n<p>最后是报文10，服务端发送了HTTP 400的响应报文给消息网关。</p>\n</blockquote><p>虽然作为服务端的Nginx记录在日志里的是499，但是回复的HTTP响应，依然是遵循了标准HTTP协议，也就是回复了HTTP 400。那么消息网关作为客户端，它一定是收到了HTTP 400。</p><p>至于它是如何计入Web日志的，就是一个有意思的话题了。比如Nginx就没有记为400而是499，那消息网关在日志里记录的，也未必一定是400，也可能是它自己定义的某些值。所以说，这个问题的答案可能有两个：</p><ul>\n<li>记录的日志就是原始的HTTP 400。</li>\n<li>记录的是某种timeout，如果它更关心的是业务状态，而不是HTTP响应。</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/c3/03/c31828f3fc871503856b29c032290603.jpg?wh=2000x746" alt=""></p><p>好了，这次的答疑就到这里。你看完答案后有没有一些新的感悟呢？或者会不会有新的疑问呢？无论是哪种，都欢迎你在留言区跟大家一起分享，我们一同成长。</p>',
        article_title: "答疑（三）| 第11~15讲思考题答案",
      },
      {
        title: "16 | 服务器为什么回复HTTP 400？",
        id: 489700,
        content:
          '<p>你好，我是胜辉。</p><p>在上节课里，我们回顾了一个与HTTP协议相关的Nginx 499的案例。在应用层的众多“明星”里，HTTP协议无疑是“顶流”了，可以说目前互联网上的大部分业务（电商、社交等），都是基于HTTP协议，当然也包括我们用极客时间学习的时候，也是在用HTTP。那么相应的，<strong>HTTP方面的排查能力</strong>，对于我们做开发和运维技术工作来说，就更加重要了。因为不少现实场景中的故障和难题，就与我们对HTTP的理解以及排查能力，有着密切的联系。</p><p>所以这一讲，我们会来看一个HTTP相关的报错案例，深入学习这其中的排查技巧。同时，我也会带你学习HTTP这个重要协议的规范部分。这样，以后你处理类似的像HTTP 4xx、5xx的报错，或者其他跟HTTP协议本身相关的问题时，就有分寸，知道问题大概的方向在哪里、如何开展排查了。</p><p>那么在介绍案例之前，我们先简单地回顾一下HTTP协议。</p><h2>HTTP协议的前世今生</h2><p>HTTP的英文全称是Hypertext Transfer Protocol，中文是超文本传输协议，它的奠基者是英国计算机科学家蒂姆·博纳斯·李（Tim Berners-Lee）。1990年，他为了解决任职的欧洲核子研究组织（CERN）里，科学家们无法方便地分享文件和信息的问题，由此创造了HTTP协议。</p><!-- [[[read_end]]] --><p>实际上，在当时也有其他一些协议能实现信息共享的功能，比如FTP、SMTP、NNTP等，为什么还要另外创造HTTP呢？这是因为这些协议并不满足博纳斯·李的需求，比如：</p><ul>\n<li>FTP只是用来传输和获取文件，它无法方便地展示文本和图片；</li>\n<li>NNTP用来传输新闻，但不适合展示存档资料；</li>\n<li>SMTP是邮件传输协议，缺乏目录结构。</li>\n</ul><p>而博纳斯·李需要的是“图形化的、只要点击一下就能进入到其他资料的系统”。鉴于以上协议无法实现，他就设计了HTTP。也因为这个巨大的贡献，博纳斯·李获得了2016年的图灵奖，可以说是图灵奖的一次“回国”。</p><p>在2015年之前，HTTP先后有0.9、1.0、1.1三个版本，其中HTTP/1.0和1.1合称HTTP/1.x。虽然谷歌在2009年就提出了SPDY，但最终被接纳成为HTTP/2，也已经是2015年的事了。最近几年蓬勃发展的还有HTTP/3（也就是QUIC上的HTTP/2）。<strong>但从语义上说，HTTP/2跟HTTP/1.x是保持一致的。</strong>HTTP/2不同，主要是在传输过程中，在TCP和HTTP之间，增加了一层传输方面的逻辑。</p><blockquote>\n<p>补充：<a href="https://datatracker.ietf.org/doc/html/rfc7540">RFC7540</a>定义了HTTP/2的协议规范，而HTTP/1.1在1999年6月的<a href="https://datatracker.ietf.org/doc/html/rfc2616">RFC2616</a>里已经确定了大部分内容。</p>\n</blockquote><p>什么叫做“语义上是一致的”呢？举个例子，在HTTP/2里面，header和body的定义和规则，就跟HTTP/1.x一样。比如 <code>User-agent: curl/7.68.0</code> 这样一个header，在HTTP/1.x里是代表了这次访问的客户端的名称和版本，而在HTTP/2里，依然是这个含义，没有任何变化。</p><p>从这一点上看，你甚至可以把HTTP/2理解为是在HTTP/1.x的语义的基础上，增加了一个介于TCP和HTTP之间的新的“传输层”。也就是下图这样：</p><p><img src="https://static001.geekbang.org/resource/image/78/12/784ef6da887086ef500b955b90dc2512.jpg?wh=2000x700" alt=""></p><p>目前最新的HTTP/3仍在讨论过程中，还未正式发布。它也依然保持了之前版本HTTP的语义，但在传输层上做了彻底的“革命”：把传输层协议从TCP换成了 <strong>UDP</strong>。根据w3techs.com（一家网络技术调查网站）的<a href="https://w3techs.com/technologies/details/ce-http3">数据</a>，截至2022年2月22日，有25.2%的站点已经支持了HTTP/3。</p><p>好，回顾完HTTP的历史，我们已经比较清楚它的来龙去脉了。那么接下来要讲的案例，就会帮助我们拆解HTTP协议的一些细节，梳理对这种类型的问题的排查思路。</p><h2>案例：服务器为什么回复HTTP 400？</h2><p>这是前几年我在公有云服务时候的一个案例。当时一个客户测试我们的对象存储服务，这个服务是通过HTTP协议存放和读取文件的。它比较适合存放非结构化的数据，比如日志文件、图片文件等。因为依托于HTTP协议，浏览这种存储的方法很方便，比如用浏览器就可以直接访问。</p><p>但是，在客户的测试结果中报告大量HTTP 400的报错。我们也很意外，其他客户用的都挺好，为什么这个客户就不行呢？</p><h3>开始排查</h3><p>按照惯例，我们还是进行了抓包。这次是在客户端抓取的，我们看一下Expert Information：</p><p><img src="https://static001.geekbang.org/resource/image/cb/e7/cb1521098f852a9642462d4e0a36cee7.jpg?wh=1810x290" alt="图片"></p><p>其中，我们需要重点关注HTTP事务，也就是上图中的<code>Chat HTTP/1.1 200 OK\\r\\n</code>这部分，这里面都是HTTP事务的报文。由于第一个被Wireshark判定为HTTP事务的报文，是一个HTTP 200 OK的返回报文，所以就显示为这里的Summary栏的信息。</p><blockquote>\n<p>补充：这里我修改过抓包，所以展现在Expert Information里面的样子，跟正常抓取完整报文的情况略有不同。比如这个示例文件里，第一个HTTP报文其实是POST，那么Summary栏显示的，应该是POST请求而不是HTTP/1.1 200 OK。但是，这不影响排查和分析。</p>\n</blockquote><p>既然这次是明确要排查HTTP 400报错，所以我们直接点开这些HTTP事务：</p><p><img src="https://static001.geekbang.org/resource/image/9e/b0/9eaa90c8d33c5b4f73b600ba4530a1b0.jpg?wh=1820x440" alt="图片"></p><p>可见，这里有200 OK这样的正常响应，也有400 Bad Request这样的异常响应。</p><p>我们找一个请求，Follow TCP Stream来看一下详细情况。比如，我们选中23号报文，此时主界面也自动跳转到了这个报文的位置。我们选中它，右单击后选择Follow -&gt; TCP Stream：</p><p><img src="https://static001.geekbang.org/resource/image/14/0d/14564a4652fcb3ea42639ede9e36920d.jpg?wh=1648x1230" alt="图片"></p><p>我们来看一下整个TCP流：</p><p><img src="https://static001.geekbang.org/resource/image/61/5f/61fe4d82d3fc9e02137e8f3e572d0c5f.jpg?wh=1646x1238" alt="图片"></p><p>在Wireshark里，HTTP请求是红色字体，而HTTP响应是蓝色字体。显然，紧随在请求之后就是响应了，而蓝色字的第一行就是HTTP/1.1 400 Bad Request。这就是我们要排查的问题。</p><p>然后我们需要搞清楚问题的定义了：HTTP 400到底是什么？</p><h3>究竟什么是HTTP 400？</h3><p>要回答这个问题，最准确的办法，还是<strong>阅读RFC</strong>，看看标准里面到底怎么说。HTTP的RFC有过好几版，1999年6月的<a href="https://datatracker.ietf.org/doc/html/rfc2616">RFC2616</a>确定了HTTP的大部分规范，而后在<a href="https://datatracker.ietf.org/doc/html/rfc7230">7230</a>、<a href="https://datatracker.ietf.org/doc/html/rfc7231">7231</a>、<a href="https://datatracker.ietf.org/doc/html/rfc7232">7232</a>等RFC中做了更新和细化。RFC2616是这样定义400 Bad Request的：</p><pre><code class="language-plain">400 Bad Request\n\n   The request could not be understood by the server due to malformed\n   syntax. The client SHOULD NOT repeat the request without\n   modifications.\n</code></pre><p>也就是：这个请求因为语法错误而无法被服务端理解。客户端不可以不做修改就重复同样的请求。</p><p>此外，RFC2616里还定义了几种必须返回400的情况，比如：</p><pre><code class="language-plain">A client MUST include a Host header field in all HTTP/1.1 request\n   messages . If the requested URI does not include an Internet host\n   name for the service being requested, then the Host header field MUST\n   be given with an empty value. An HTTP/1.1 proxy MUST ensure that any\n   request message it forwards does contain an appropriate Host header\n   field that identifies the service being requested by the proxy. All\n   Internet-based HTTP/1.1 servers MUST respond with a 400 (Bad Request)\n   status code to any HTTP/1.1 request message which lacks a Host header\n   field.\n</code></pre><p>其他还有好几种情况，就不一一罗列了。</p><p>那么显然，400 Bad Request的语义，就是让服务端告诉客户端：<strong>你发过来的请求不合规，我无法理解，所以我用400来告诉你这一点</strong>。</p><p>但是，我们也不可能去穷举所有可能出现的不合规类型。那么在这个案例里面，究竟是哪里出了问题呢？</p><h3>寻找突破口</h3><p>有时候，我们做排查工作，需要一点灵感，也需要一点耐心。对着这个页面，如果你对HTTP协议并不是很熟悉，那么很难直接用肉眼就“看出”问题来。</p><p>那我们来玩个游戏怎么样：“大家来找茬”。你应该已经明白我的意思了，我们要做的是：<strong>对比分析</strong>。我们只需要把一个正常和一个异常的响应报文放在一起比较，也许就能找到原因了。</p><p>正巧，这次客户做的测试里，也有成功的请求。比如这个抓包文件里的HTTP 200 OK。那么，我们就借助这样的一个200 OK的TCP流，来对比分析下。</p><p>说到这里，你可能已经想起我们在<a href="https://time.geekbang.org/column/article/481042">第5讲</a>的时候，也用过这种对比分析的方法。当时是排查一个乱序引起应用层故障的问题，我们对比了客户端抓包文件和服务端抓包文件这两个文件，而它们代表的是同一个TCP流，我们也因此找到了问题的关键，也就是防火墙引发了报文乱序的现象。</p><p><img src="https://static001.geekbang.org/resource/image/c0/47/c0a13a14a7ab884e0439c78c124f0d47.jpg?wh=2000x290" alt=""></p><p>当前的案例跟第5讲的案例就有所不同了，这次比较的是同一个抓包文件里的两个不同的TCP流，也可以说是两个不同的应用层事务。这两个事务，一个成功，一个失败。</p><p><img src="https://static001.geekbang.org/resource/image/a4/f3/a4324897b7f11c9285ff1807fcef6ff3.jpg?wh=2000x287" alt=""></p><p>我们还是需要一个大一点的显示屏，把HTTP 200的报文找到后，Follow TCP Stream，随后的弹窗里就展示了这次成功的应用层消息的细节；然后选取HTTP 400的报文，也同样做一遍。然后我们把两个窗口挪到齐平的位置。</p><p>好，我们的对比开始了：</p><p><img src="https://static001.geekbang.org/resource/image/b6/59/b6d491f6b8a5085163650129a9dc1559.jpg?wh=1588x570" alt="图片"></p><p>你能找到几个“茬”呢？因为这是两次不同的事务，所以请求和回复的字符肯定也十分不同，所以我们应该集中在<strong>格式</strong>上，而不是字符。</p><p>你可能首先注意到了两次的HTTP方法不同：左边是PUT，右边是POST。</p><p>这是否说明，问题就是服务端不支持PUT方法导致了HTTP 400呢？这个很容易排除。因为，如果真的是服务端对PUT的处理有问题，那么其他客户还怎么使用PUT呢？所以，即使这个问题跟PUT还是有点关系的话，我们也要转换一下问题描述，变成：<strong>为什么这个客户端发送的PUT请求会引起HTTP 400？</strong></p><p>然后，你可能会发现，左边的PUT请求里有Authorization头部，而右边POST请求里没有这个头部。</p><p>左边这个Authorization请求的头部格式是这样的：</p><ul>\n<li>一开始是 <code>PUT /123456 HTTP/1.1</code>，然后换行；</li>\n<li>接着是 <code>Authorization: UCloud</code> 这样一个头部，然后换行；</li>\n<li>然后是 <code>abc@def.com:blahblah</code> 这种形式，看起来是一个邮箱地址后接冒号，然后是一串编码过的字符串。</li>\n</ul><p>你是不是觉得这部分的格式有点问题？这其实也是一个知识点了：HTTP Authorization头部的格式。</p><h3>Authorization头部</h3><p>我们看看<a href="https://datatracker.ietf.org/doc/html/rfc2616#section-14.8">RFC2616</a>里，对Authorization头部是怎么规定的：</p><pre><code class="language-plain">14.8 Authorization\n\n      A user agent that wishes to authenticate itself with a server--\n      usually, but not necessarily, after receiving a 401 response--does\n      so by including an Authorization request-header field with the\n      request.  The Authorization field value consists of credentials\n      containing the authentication information of the user agent for\n      the realm of the resource being requested.\n\n          Authorization  = "Authorization" ":" credentials\n</code></pre><p>简单来说，它也跟其他的HTTP头部的规定一样，也是 <code>key:value</code> 的形式。语法格式是这样：</p><pre><code class="language-plain">Authorization: &lt;auth-scheme&gt; &lt;authorization-parameters&gt;\n</code></pre><blockquote>\n<p>补充：如果要了解关于这个头部的更多细节，还可以参考Mozilla Developer Network关于这个头部的更多的<a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Authorization">详细介绍</a>。</p>\n</blockquote><p>这里的 <code>&lt;auth-scheme&gt;</code>，比较常见的是Basic和Digest。如果是Basic类型，那么它的格式是：</p><pre><code class="language-plain">Authorization: Basic &lt;credentials&gt;\n</code></pre><p>这里的credentials，可以是 <code>username@site.com:hashedPassword</code> 这种形式。</p><p>而我们在抓包里看到的是什么格式呢？</p><pre><code class="language-plain">Authorization: UCloud\nucloudabcdef.yu@testtest.com144731865200013974915:gABCDEFGQSLLsdyOjIlo21fap6o=\n        #这里是一个空行\n</code></pre><p>这个 <code>Authorization: UCloud</code> 后面多了一个换行，这就已经是一个问题了。</p><p>更严重的是，在第二行的后面，有两次回车，或者说两次CRLF，而这个问题更加严重。说到这里，我们就需要复习一下HTTP报文格式的知识了。</p><h3>HTTP报文分隔</h3><p>跟IP、TCP类似，HTTP也分为头部（headers）和载荷（body或者payload）。</p><p><img src="https://static001.geekbang.org/resource/image/7c/ef/7c8f99a2ee15a21da8a1d60da1c6eeef.jpg?wh=2000x966" alt=""></p><p>既然分成了两个部分，那么显然，接收者需要知道header和payload的分界线，要不然就会导致信息解读错误，这是致命的。</p><p><strong>在IP协议里</strong>，IP header是用一个Total Length字段，表示了包含IP头部在内的整个IP报文的长度。那怎么区分IP头部和载荷呢？IP头部还有一个字段是Header Length，表示了头部自身的长度。这样两个Length值的差，就是IP载荷的大小了。</p><p><strong>在TCP协议里</strong>，TCP header里的Data offset，表示了TCP载荷开始的位置（也是TCP头部截止的位置），也就相应地可以计算出TCP头部的长度。那么TCP载荷长度是怎么来的呢？我们用一个简单的减法就好了：</p><pre><code class="language-plain">TCP payload Length = IP Total Length - IP Header Length - TCP Header Length\n</code></pre><p>这些头部长度的关系，我用了一张示意图来概括，供你参考：</p><p><img src="https://static001.geekbang.org/resource/image/58/d3/588f8a3ec1f6b5e9ed30d77a546112d3.jpg?wh=2000x812" alt=""></p><p><strong>而在HTTP里</strong>，载荷的长度一般也是由一个HTTP header（这里指的是某一个头部项，而不是整个HTTP头部），也就是Content-length来表示的。假设你有一次PUT或者POST请求，比如上传一个文件，那么这个文件的大小，就会被你的HTTP客户端程序（无论是curl还是Chrome等）获取到，并设置为Content-Length头部的值，然后把这个header封装到整体的HTTP请求报文中去。</p><p><img src="https://static001.geekbang.org/resource/image/be/2a/be79d3c36ae164284yycc2cef58cf42a.jpg?wh=1274x151" alt="图片"></p><p>既然HTTP报文内容，分成了头部（headers）和载荷（Payload或者body）两部分，那么这两者的分界线在哪呢？</p><p><strong>HTTP规定，头部和载荷的分界线是两次CRLF。</strong></p><pre><code class="language-plain">A request message from a client to a server includes, within the\n   first line of that message, the method to be applied to the resource,\n   the identifier of the resource, and the protocol version in use.\n\n        Request       = Request-Line              ; Section 5.1\n                        *(( general-header        ; Section 4.5\n                         | request-header         ; Section 5.3\n                         | entity-header ) CRLF)  ; Section 7.1\n                        CRLF\n                        [ message-body ]          ; Section 4.3\n</code></pre><p>也就是在最后一个header之后，需要有两个CRLF，这就是头部和载荷之间的分割线。之后就是载荷（message body）的开始了。</p><p>那么，前面引发HTTP 400的PUT请求，其Authorization后面也出现了两个CRLF，这就会被认为是headers的结束，payload的开始。但实际上，后面跟的又是剩余的HTTP头部项，在最后一个头部之后，又是两个CRLF。所以这对于Web服务端来说就懵了：“你这说的可不是人话啊！我只能表示我不理解。”</p><p><img src="https://static001.geekbang.org/resource/image/41/bb/41ebb0e77beb6219a4acdee855f947bb.jpg?wh=1562x1152" alt="图片"></p><h3>定位不合规处</h3><p>原来如此，这次的<strong>400 Bad Request的根因，是客户发送的HTTP PUT请求的格式出现了问题</strong>。它违背了HTTP/1.1（RFC2616）的规定，在Authorization头部后面，错误地添加了两次回车（CRLF）。这样就导致服务端认为，后续的数据都属于payload，也就导致服务器无法正常读取这个请求，只能用HTTP 400来反馈这种状况了。</p><p>既然咱们的课程叫“网络排查案例课”，那么这次案例的根因，跟网络有没有关系呢？</p><p>我觉得要看你怎么定义“网络”。</p><p>如果是传统和狭义上的网络，只包含交换机、路由器、防火墙、负载均衡等环节，那么这里并没有什么问题。没什么重传，也不丢包，更不影响应用消息本身。</p><p>如果是广义的网络，那就包含了至少以下几个领域：</p><ul>\n<li>对应用层协议的理解；</li>\n<li>对传输层和应用层两者协同的理解；</li>\n<li>对操作系统的网络部分的理解。</li>\n</ul><p>在这个案例里，我们依托于<strong>对应用层协议的理解</strong>，找到了网络行为以外的根因。这个根因虽然可能根源是开发方面的问题，但无论是开发、运维或者SRE，在处理这种问题的时候，如果能具备比较全面的知识，从而推导出根因，那么无论是对组织效率的提升，还是个人能力的提升，是不是都更有意义呢？</p><h2>实验</h2><p>现在，我们也来做几个简便的小实验，模拟出HTTP 400 Bad Request这样的响应。</p><h3>实验1：对HTTP发送不合规的请求</h3><p>如果我们直接用高级语言来调用HTTP库，可能反而不容易做到这种“非法”请求。因为这些库的设计目的之一，就是要尽量避免人工的编码错误以及提升开发效率，我们想借助它去构造非法请求，恐怕不太容易。</p><p>当然，如果你熟悉Python的话，可能会想到用Scapy库等工具来实现。但这个步骤就稍多了点。</p><p>其实，我们也可以用最简单的方法，就是直接用 <strong>telnet命令</strong>。我们在<a href="https://time.geekbang.org/column/article/478189">第2讲</a>里，用视频的形式介绍了如何一边用telnet模拟发送HTTP请求，一边用tcpdump的-X参数，展示抓取的报文里面的文本细节。</p><p>那么这里，我们也用类似的方法，只要手动执行下面的命令，就可以向目标站点发送一个不合规的请求：</p><pre><code class="language-plain">$ telnet www.baidu.com 80\nTrying 180.101.49.12...\nConnected to www.a.shifen.com.\nEscape character is \'^]\'.\nGET / HTTP/1.1\nAuthorization  #这里是一次回车\n               #这里是又一次回车\nHTTP/1.1 400 Bad Request\n\nConnection closed by foreign host.\n</code></pre><p>也就是telnet目标站点的80端口，在提示符下输入：</p><pre><code class="language-plain">GET / HTTP/1.1\n</code></pre><p>然后回车，再输入：</p><pre><code class="language-plain">Authorization\n</code></pre><p>注意这里不要输入更多内容，直接<strong>回车两次</strong>。这时，两次回车被对端Web服务器收到后，它是这么解读的：</p><ul>\n<li>这是一个GET /的HTTP/1.1版本的请求。</li>\n<li>有一个Authorization头部，但是这个头部并没有值。</li>\n<li>两次回车就表示这次请求发送结束。</li>\n</ul><p>由于请求不合规，目标站点立刻回复了HTTP 400 Bad Request。</p><p>而如果我们在输入Authorization时，后面加上“: Basic”，会收到HTTP 500。这是因为服务端认为Authorization: Basic这个格式本身是正确的，只是后面缺少了真正的凭据（Credential），所以报告了HTTP 500。</p><p>所以，两者的区别就是：</p><ul>\n<li><strong>Authorization后面直接回车</strong>，就表示它并没有带上 <code>&lt;auth-scheme&gt;</code>，所以属于不合规，应该回复HTTP 400。</li>\n<li><strong>Authorization: Basic后直接回车</strong>，它的Authorization头部有 <code>&lt;auth-scheme&gt;</code>，但是没有带上有效的凭据，应该回复HTTP 500。</li>\n</ul><h3>实验2：对HTTPS发送不合规的请求</h3><p>前面实验的是HTTP站点，我们用telnet发送明文请求比较直观。而要是对方站点是HTTPS的话，如果还是用telnet，会遇到TLS握手，这一关就过不去了。那么该怎么办呢？</p><p>其实，我们可以用<strong>openssl命令</strong>。执行<code>openssl s_client -connect 站点名:443</code>，就可以跟对端站点建立TLS握手。比如像下面这样：</p><pre><code class="language-plain">$ openssl s_client -connect www.baidu.com:443\nCONNECTED(00000006)\ndepth=2 C = BE, O = GlobalSign nv-sa, OU = Root CA, CN = GlobalSign Root CA\nverify return:1\ndepth=1 C = BE, O = GlobalSign nv-sa, CN = GlobalSign Organization Validation CA - SHA256 - G2\nverify return:1\ndepth=0 C = CN, ST = beijing, L = beijing, OU = service operation department, O = "Beijing Baidu Netcom Science Technology Co., Ltd", CN = baidu.com\nverify return:1\n---\n......\n</code></pre><p>另外还有一点，我们这个时候怎么发送HTTP请求呢？不少人会在这里卡住。其实，openssl也是一个交互式的命令，跟telnet一样，直接键入HTTP请求就好了！</p><pre><code class="language-plain">---\nGET / HTTP/1.1\nAuthorization  #这里是一次回车\n               #这里是又一次回车\nHTTP/1.1 400 Bad Request\n\nclosed\n</code></pre><p>这样一来，也可以得到跟telnet 80一样的响应。</p><p>其实，<strong>网络协议就是这样，是一种“方言”，互相要用对方听得懂的方式对话。</strong>如果语法出现了问题，我们的自然语言就是“不明白你的意思，你说啥”。在HTTP这个“方言”里，就是用HTTP 400表达了同样的意思。</p><h2>小结</h2><p>这节课，我们通过一个服务器回复HTTP 400的案例，学习了这种对HTTP返回码进行排查的方法。</p><p>使用这种方法的前提，还是需要你对HTTP协议本身有比较深入的掌握，然后结合对HTTP语义的理解，分析出根因。而熟悉HTTP协议的方法，就是熟读RFC2616，以及2014年6月的更新RFC（7230, 7231, 7232, 7233, 7234, 7235）。</p><p>具体的方法，我们可以借鉴这样的方式：</p><ul>\n<li>我们可以把错误的报文跟成功的报文放一起，进行<strong>对比分析</strong>。这样会比较快地发现两者之间的差别，从而更快地定位到根因。</li>\n<li>我们也可以通过telnet和openssl，分别<strong>模拟复现HTTP和HTTPS的</strong>请求，重放给服务端，观察其是否也返回同样的报错。</li>\n<li>对比协议规范和报文中抓取到的实际行为，找到不符合规范之处，很可能这就是根因。</li>\n</ul><p>同时，我们也回顾了不少HTTP协议的知识，包括：</p><ul>\n<li>HTTP的各种版本的知识点：<strong>HTTP/2和HTTP/3的语义跟HTTP/1.x是一致的</strong>，不同的是HTTP/2和HTTP/3在传输效率方面，采用了更加先进的方案。</li>\n<li>Authorization头部的知识点：它的格式为 <code>Authorization: &lt;auth-scheme&gt; &lt;authorization-parameters&gt;</code>，如果缺少了某一部分，就可能引发服务端报HTTP 400或者500。</li>\n<li>HTTP报文的知识点：<strong>两次回车（两个CRLF）是分隔HTTP头部和载荷的分隔符</strong>。</li>\n<li>HTTP返回码的知识点：HTTP 400 Bad Request在语义上表示的是<strong>请求不符合HTTP规范</strong>的情况，各种不合规的请求都可能导致服务端回复HTTP 400。</li>\n</ul><p>最后，我们通过两个小实验，学习了用简单的方式模拟HTTP请求的方法。如果服务端是HTTP，我们用telnet；如果服务端是HTTPS，就用openssl。</p><h2>思考题</h2><p>给你留两道思考题：</p><ul>\n<li>在HTTP请求里，我们用Content-Length表示了HTTP载荷，或者说HTTP body的长度，那有时候无法提前计算出这种长度，HTTP是如何表示这种“动态”的长度呢？</li>\n<li>HTTP请求的动词加URL部分，比如GET /abc，它是属于headers，还是属于body，或者哪种都不属于，是独立的呢？</li>\n</ul><p>你可以在留言区说说你的想法和思考，我们一起交流。另外也欢迎你把今天的内容分享给更多的朋友。</p>',
        article_title: "16 | 服务器为什么回复HTTP 400？",
      },
      {
        title: "17 | 为什么前端页面里多选一个城市就报错？",
        id: 490397,
        content:
          '<p>你好，我是胜辉。</p><p>在<a href="https://time.geekbang.org/column/article/488979">第15讲</a>中，我给你介绍了Nginx的499状态码的排查过程，这种排查方法其实也适用于其他HTTP状态码的排查。另外可能你也注意到了，这个案例是聚焦在后端Web日志方面的，那么如果遇到<strong>前端方面</strong>的报错，我们的排查又该如何开展呢？</p><p>所以今天这节课，我们就来探讨下这方面的排查技巧。跟往常一样，我们还是从案例说起。在这个过程中，你可以跟随我的脚步，通过抓包分析，把问题表象拆解为底层的数据流，然后深入到协议和应用的细节来找到根因。这样，以后你对类似的看起来凌乱没有头绪问题，也能有一套行之有效的方法来开展排查工作了。</p><h2>案例：为什么前端页面里多选一个城市就报错？</h2><p>我们曾经服务过一家垂直OTA（Online Travel Agency，在线旅游），他们专注于欧洲旅行市场，取得了不俗的业绩。有一天，客户的运维负责人找到我们，报告了一个奇怪的问题。</p><p>他们最近推出了一个旅游产品，可以让用户自主选择在欧洲旅行的多个城市之间，以自定义的顺序展开旅行。比如，你可以选择从西班牙的巴塞罗那启程，然后来到法国巴黎，随后踏上风车王国荷兰的领土，最后把日不落帝国的伦敦作为最后一站来结束旅行。以上旅程是由4个国家（城市）组成的：</p><!-- [[[read_end]]] --><p><img src="https://static001.geekbang.org/resource/image/d5/06/d51251e74yy4f44f0bab295f412e9a06.png?wh=521x496" alt="图片"></p><p>但是问题来了，如果你在网站上选择行程时多选一个地点，比如，中途增加一次到丹麦的旅程，使途经的国家/城市从4个变成5个，在提交旅行计划的时候，网站却会报错。</p><p><img src="https://static001.geekbang.org/resource/image/76/a4/760c1f6feea04e8d2415a8be5e76c1a4.png?wh=542x498" alt="图片"></p><p>好像哪里有个环节跟公司作对似的，用户想多花点钱都不行？</p><h2>开展排查</h2><p>我们先来看一下整体的架构，他们这个还是比较典型的Web应用的基础架构：</p><p><img src="https://static001.geekbang.org/resource/image/1c/52/1c63fa92bfa458d31bdc36b245b8f852.jpg?wh=2000x295" alt=""></p><p>云LB是基于HAProxy方案的软件负载均衡产品，它分发流量给后端的云主机Nginx，上面运行着Web程序，再后面就是云数据库了。</p><h3>确认和重现</h3><p>问题排查的第一步是什么？</p><p>一般来说，是<strong>问题的准确描述和重现</strong>。就是说，如果问题不是你自己发现而是其他人报告给你的，那么你需要确认对方描述的每一个事实细节。我们按照客户描述的顺序，登录网站后依次选择了5个城市，在提交行程的时候果然遇到报错了。而改为只选择4个城市，就能提交成功。</p><p>问题可以重现，第一步完成。</p><h3>先做排除法</h3><p>问题排查的第二步是什么？</p><p>一般来说，是可以做<strong>排除法</strong>筛选问题根因。这跟我们考试做选择题的时候类似，如果你在A、B、C、D四个选项中无法一下子就找到正确选项，那可以先排除那些明显错误的选项，最后剩下的就是答案了。</p><p>我们让客户自查了云数据库、云主机应用服务器，都没有发现问题。而且客户报告，如果绕过云LB，直接访问云主机Nginx或者云主机应用服务器，同样的方式预订5个城市的旅程，都能提交成功。访问外部站点（经过云LB），就会提交失败。所以我们再对比一下：</p><ul>\n<li>失败场景：公网用户 -&gt; <strong>云LB</strong> -&gt; 云主机Nginx</li>\n<li>成功场景：内网用户 --------&gt; 云主机Nginx</li>\n</ul><p>这样看起来，问题就集中到云LB上了。</p><p>于是我们在云LB上开始排查，当然这里少不了用tcpdump做抓包。对于云LB来说，对同一个应用请求，它其实需要处理两个TCP连接。</p><ul>\n<li>客户侧连接：公网客户IP &lt;-&gt; 云LB弹性IP。</li>\n<li>服务侧连接：云LB内部IP &lt;-&gt; 云主机Nginx内部IP。</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/94/d1/94af5d7e8e9a01f930114f3734e80fd1.jpg?wh=2000x478" alt=""></p><p>于是我们把这两段不同的TCP连接也都抓取了。好在这个问题是必现的，我们只要选择那五个城市，问题就必然出现，所以很容易就抓取到了问题报文。先预告一下，在后面的课程中，我还会提到对于偶发性问题的排查思路，它跟必现型问题的排查确实有挺大的不同。</p><h3>tshark命令</h3><p>我们先看一下客户侧抓包的情况。因为是HTTP应用，我们可以重点关注其HTTP返回码的情况。这里，我们要学习一个新的强大的命令行工具：<strong>tshark</strong>。</p><p>在安装Wireshark软件包的时候，它默认也会连带安装其他几个强大的命令行工具，比如capinfos、tshark、dumpcap、editcap、mergecap等。这里的tshark事实上可以起到类似tcpdump的作用，比如在我使用的macOS笔记本上，用Wireshark、tcpdump、dumpcap，还有tshark，都可以抓包。</p><p>当然，tshark也可以读取和解析抓包文件。关于tshark的更多说明，可以参考<a href="https://www.wireshark.org/docs/man-pages/tshark.html">官方文档</a>。</p><p>你可能会问：“既然tshark跟tcpdump差不多，为什么一定要用tshark呢？”</p><p>这是因为，tshark解读文件时，可以像Wireshark那样解读到应用层，而这一点，tcpdump就无法做到了。在当前这个案例里，我们需要用上tshark的报文分析功能，过滤并统计HTTP返回码的分布情况。命令如下：</p><pre><code class="language-plain">$ tshark -r lesson17-in.pcap -T fields -e http.response.code | grep -v ^$ | sort | uniq -c | sort -r\n2883 200\n704 502\n227 304\n141 400\n45 301\n41 302\n16 408\n13 403\n6 503\n6 404\n2 206\n</code></pre><blockquote>\n<p>补充：抓包示例文件已经上传至<a href="https://gitee.com/steelvictor/network-analysis/blob/master/17/lesson17-in-shorten.pcap">Gitee</a>，建议结合文稿和Wiresahrk、tshark一起学习。</p>\n</blockquote><p>可以看到，返回码200的情况还是占了绝大多数（2883个），其次是返回码502（704个），然后余下的是的3xx系列和4xx系列的返回码，还有6个503返回码。</p><p>当然，你用Wireshark图形界面也很容易获得这种信息。在Wireshark的Statistics下拉菜单里，选择HTTP -&gt; Packet Counter：</p><p><img src="https://static001.geekbang.org/resource/image/e1/15/e127b6909ba4ed237963fbc775862015.jpg?wh=507x474" alt="图片"></p><p>然后就能看到统计信息了。可见，这些数据跟我们用tshark命令行工具做解析的数字是一致的：</p><p><img src="https://static001.geekbang.org/resource/image/48/d5/488b380c8427c0ed43823cd7bd5237d5.jpg?wh=1590x956" alt="图片"></p><p>可能你要问了，显然图形界面更加方便一点，tshark工具的价值又在哪里呢？这里我来说说我的看法吧。</p><ul>\n<li><strong>当我们需要分享抓包分析信息给其他人的时候</strong>，tshark的输出信息是文本格式，就很方便分享了。而Wireshark的是截图，就没有文本那么方便。</li>\n<li><strong>当我们有多个文件需要做同一种分析的时候</strong>，tshark命令行工具优势就体现出来了，因为不需要打开多个Wireshark窗口，而是在同一个命令行窗口里就可以对多个抓包文件依次执行相似的命令，然后对比这些输出，十分方便。</li>\n<li><strong>当我们要对抓包分析进行自动化的时候</strong>，tshark这样的命令行工具以及类似的开发库就很有用了，可以帮助我们把人的经验沉淀到代码里去，减少人工的工作量。</li>\n</ul><h3>HTTP 5xx系列</h3><p>回到这个案例。这么多502/503的返回码确实不太正常，这又跟502/503本身的语义有关。我们分别来看一下协议中定义的<a href="https://datatracker.ietf.org/doc/html/rfc2616#section-10.5.3">502</a>、<a href="https://datatracker.ietf.org/doc/html/rfc2616#section-10.5.4">503</a>和<a href="https://datatracker.ietf.org/doc/html/rfc2616#section-10.5.5">504</a>。</p><p>在学习HTTP协议的时候，除了阅读RFC2616等RFC文档，还可以参考MDN（Mozilla Developer Network），因为是有中文版的，所以对我们更加友好。这里，我们就用它的中文解释：</p><p><a href="https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Status/502">502 Bad Gateway - HTTP | MDN (mozilla.org)</a>：</p><pre><code class="language-plain">502&nbsp;Bad Gateway&nbsp;是一种HTTP协议的服务器端错误状态代码，它表示作为网关或代理角色的服务器，从上游服务器（如tomcat、php-fpm）中接收到的响应是无效的。\n</code></pre><p><a href="https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Status/503">503 Service Unavailable - HTTP | MDN (mozilla.org)</a>：</p><pre><code class="language-plain">503&nbsp;Service Unavailable&nbsp;是一种HTTP协议的服务器端错误状态代码，它表示服务器尚未处于可以接受请求的状态。\n</code></pre><p><a href="https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Status/504">504 Gateway Timeout - HTTP | MDN (mozilla.org)</a>：</p><pre><code class="language-plain">504&nbsp;Gateway Timeout&nbsp;是一种HTTP协议的服务器端错误状态代码，表示扮演网关或者代理的服务器无法在规定的时间内获得想要的响应。\n</code></pre><p>你可能也看出来了，502/503/504这几个返回码，都是给 <strong>反向代理 / LB</strong> 用的。反向代理 / LB位于客户和服务之间，起到了转发、分流、处理的作用。这里我稍微再给你展开一下这几类功能。</p><ul>\n<li><strong>转发</strong></li>\n</ul><p>这个最基础最直观，就是把客户发过来的请求，转发给后端服务器。因为一般来说反向代理跟后端服务器是 <code>1:n</code> 的关系，所以需要用一些算法来进行分发，来保证后端分到的请求数量是符合预期的。常见的算法有轮询、权重、最小连接数、哈希等等。</p><p><img src="https://static001.geekbang.org/resource/image/ea/f7/ea593306e357c302ed35ea936e83d3f7.jpg?wh=2000x891" alt=""></p><ul>\n<li><strong>路由分流</strong></li>\n</ul><p>这是第七层的路由分发，把符合一定条件的HTTP请求分发到路由配置的对应的后端集群，可以说是带条件判断的转发。搜索引擎优化（Search Engine Optimization，简称SEO）就是一个典型的例子，它把各种子域名集中到主站域名下面，比如：</p><blockquote>\n<ul>\n<li>register.abc.com转为www.abc.com/register（注册）</li>\n<li>cart.abc.com转为www.abc.com/cart（购物车）</li>\n</ul>\n</blockquote><p><img src="https://static001.geekbang.org/resource/image/a8/67/a8ca8ec3b989d0585a47232385ac7267.jpg?wh=2000x885" alt=""></p><p>这样的话，多个子域名就统一为单一域名www.abc.com，原本分散到各个子域名的搜索得分也被合并了，一下子提升了主站的排名。</p><ul>\n<li><strong>处理</strong></li>\n</ul><p>这里是指应用层的业务处理。这个会比较多样，比如可能直接由反向代理回复给客户一个301/302 http redirect，也可能改写URL后转发给后端进一步处理，等等，总之是应用层的行为。</p><p><img src="https://static001.geekbang.org/resource/image/38/d9/3872e19b18302ab8619f4415c6dfafd9.jpg?wh=1743x378" alt=""></p><p>好，我们继续502/503/504的话题。因为反向代理 / LB是位于客户和服务之间的，如果服务坏了，而反向代理 / LB本身没坏，那么该给客户回复哪个返回码呢？500吗？可是服务端故障，但我反向代理 / LB自己可没故障啊。</p><p><img src="https://static001.geekbang.org/resource/image/c8/53/c8914abe72b217f300f1a495ce3e0953.jpg?wh=2000x621" alt=""></p><p>为了“撇清”这层关系，反向代理 / LB可以用502/503/504这些返回码向客户端表明清白之身：我自己没问题，是我后面的服务出了问题。所以说，遇到大量502/503/504时，你应该重点查一下产生这些返回码的背后的原因。</p><p><img src="https://static001.geekbang.org/resource/image/5d/9b/5d8ece35ecfaa1a51503c2b88e8cf59b.jpg?wh=2000x612" alt=""></p><p>我们从抓包文件里基于测试机的外网IP，过滤出了问题重现时发生的HTTP事务，做下一步的分析。输入过滤条件：<code>ip.addr == x.x.x.x</code>（此处隐去了真实IP），然后过滤到这个测试引发的数据包：</p><p><img src="https://static001.geekbang.org/resource/image/c6/8d/c6d9d32f9e0fbe86d198b842dfdfde8d.jpg?wh=966x215" alt="图片"></p><blockquote>\n<p>补充：HTTP 502的示例文件已经上传至<a href="https://gitee.com/steelvictor/network-analysis/blob/master/17/lesson17-frontend-http502-shorten.pcap">Gitee</a>。</p>\n</blockquote><p>其中，赫然出现HTTP 502。接下来你也知道，我们需要对这个TCP流进行重点分析。选中HTTP 502这个包，右单击Follow，在弹出子菜单中选中TCP stream。此时会有弹出窗口，里面展示了HTTP应用层面的信息，即HTTP请求和对应的HTTP返回。</p><p><img src="https://static001.geekbang.org/resource/image/8d/c7/8d12bc7c302507dcbe187bf4e9d158c7.jpg?wh=790x212" alt="图片"></p><blockquote>\n<p>补充： 由于我们可以想到的原因，这里把敏感信息抹去了。</p>\n</blockquote><p>一个POST请求，得到了HTTP 502，这并不正常，会不会跟这个奇怪的前端问题有关系呢？由于<strong>这只是客户侧的情况，我们必须跑到云LB的另外一侧即服务侧，看看那边发生了什么</strong>。分析那边的数据包，也许就能定位到502产生的原因了。</p><p><img src="https://static001.geekbang.org/resource/image/1c/e9/1cdafe675b9ea35be24b7ee0479089e9.jpg?wh=2000x410" alt=""></p><h3>请求的映射</h3><p>云LB的<strong>左边</strong>是一个TCP连接，<strong>右边</strong>是另外一个TCP连接，两者在网络层面毫无关联。只有云LB自己知道，左边连接里的某个HTTP事务，对应的是右边连接的哪个HTTP事务。那么，<strong>如何根据客户侧的数据，找到对应的服务侧的数据呢？</strong></p><p>这也是一个不小的挑战。这个问题的抽象描述，就是：如何在一个 <code>m:n</code> 的场景里，找到确定的 <code>1:1</code> 关系。下图中，我用虚线表示了这种映射关系的未知性。</p><p><img src="https://static001.geekbang.org/resource/image/43/4e/4382db0a820cb4d6fff22c8c4701d34e.jpg?wh=2000x712" alt=""></p><p>相信这个问题的答案不止一种。这里我想分享给你的经验是，利用Wireshark提供的一个<strong>过滤器：tcp contains</strong>。</p><p>使用这个方法的根据是：进入到客户侧的请求，一般会由LB或反向代理大体不改动地转发到服务侧。这里说“大体不改动”，是因为反向代理或者LB可能会插入一些HTTP header（比如常见的X-Forwarded-For），但一般不改写原有的URL和header。</p><p><img src="https://static001.geekbang.org/resource/image/da/0b/da7ed96f6e58dc8150236a2ccdf35f0b.jpg?wh=2000x601" alt=""></p><blockquote>\n<p>补充：除了这个方法以外，一些商业LB会提供更为强大的TCP流映射抓包功能，就是在指定抓取某个客户端IP的流量的同时，还能实时把对应的服务侧连接的数据包都抓取到。当然，在这个案例里并不是非要这种强大功能不可，用我刚介绍的过滤器也可以做到。</p>\n</blockquote><p>首先，回到我们客户侧的数据包，找到一个容易区分该HTTP请求跟其他HTTP请求的标志。比如应用层时常会用uuid，作为区分不同HTTP请求的方法，正好可以为我们所用。我们看一下这个HTTP请求，看来“sk=xxx”这个uuid比较适合作为过滤条件，也就是图中圈出来的部分，它在不同的请求间重复的可能性为零。</p><p><img src="https://static001.geekbang.org/resource/image/e8/31/e8c0ae77ab17f9cdb3fac6ac12429c31.jpg?wh=784x116" alt="图片"></p><p>然后，用Wireshark打开服务侧抓包文件，在过滤器输入框中输入下面的条件：</p><pre><code class="language-plain">tcp contains "eucir_e3fb2a65b12c36bfbde7aa0a6e6f0041"\n</code></pre><p>这样就能过滤出相关的服务侧的数据包，而这些报文就是对应了客户侧的同一个请求：</p><p><img src="https://static001.geekbang.org/resource/image/49/70/497409e46f3b30e3202a4d20a3820870.png?wh=1093x305" alt="图片"></p><p>Wireshark提示我们，这些报文都是TCP segment of a reassembled PDU，也就是属于同一个大的应用层数据的不同数据段。我们选中其中一个报文并右键Follow -&gt; TCP stream，得到这个TCP连接的完整数据：</p><p><img src="https://static001.geekbang.org/resource/image/74/1d/74f7c1e0124e1fe8a86505ff2ab01e1d.jpg?wh=789x612" alt="图片"></p><p>HTTP请求是红色字体，HTTP响应是蓝色字体。你注意下这个HTTP响应，是否发现了不同寻常的事情？</p><p>原来，在服务侧这个HTTP请求得到的不是502，而是<strong>正常的200！</strong></p><p>让我们更多地解读一下这个200返回带给我们的信息：</p><ul>\n<li>请求中的sk=xxx跟客户侧的请求的sk=xxx值一致，也就是我们可以确认：该服务侧请求即客户侧请求。</li>\n<li>该返回的头部（header）包含 <code>Server: nginx</code>，由此得知，云LB后面的这个Server是Nginx。</li>\n<li>返回的头部信息也比较大，有很多个Set-Cookie。</li>\n</ul><p>那么排查到这里，我们就可以大致拼接出来问题的全貌了：</p><ul>\n<li>公网客户访问云LB，得到HTTP 502；</li>\n<li>云LB访问后端云主机，得到HTTP 200。</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/ec/ff/eced6183f911d9b865b5208c70bbd2ff.jpg?wh=2000x416" alt=""></p><p>于是，整个排查过程有了非常重要的进展。只要能回答“是什么原因让云LB把HTTP 200变成HTTP 502”这个问题，整件事情就算水落石出了。</p><h3>根因分析</h3><p>在揭示真相之前，让我们再次回到HTTP 502的语义本身，看看我们在说502的时候，我们说的到底是什么。这是RFC2616中针对<a href="https://datatracker.ietf.org/doc/html/rfc2616#section-10.5.3">502&nbsp;Bad Gateway</a>所给出的定义：</p><pre><code class="language-plain">502 Bad Gateway\n   The server, while acting as a gateway or proxy, received an invalid\n   response from the upstream server it accessed in attempting to\n   fulfill the request.\n</code></pre><blockquote>\n<p>作为网关或者代理的服务器，在试图从它的上游服务器（后端服务器）执行HTTP请求时，接收到了一个无效的响应。</p>\n</blockquote><p>所以，云LB（基于HAProxy）认为，后端返回的HTTP响应并不符合它对于<strong>“有效”</strong>的定义。但是，显然后端回复的HTTP 200怎么看都是正常的、标准的，那HAProxy又有什么理由认为其无效呢？如果协议标准定义里面没有这个答案，那么只可能在HAProxy自己的定义/配置里面找寻答案了。</p><p>我们以“HAproxy HTTP 502”为条件进行搜索，发现有多种情况会导致HAProxy回复502给客户端，比如：</p><ul>\n<li>后端服务器返回的HTTP响应不符合HTTP规范；</li>\n<li>后端服务器没有及时响应；</li>\n<li>header的大小写问题；</li>\n<li>header size超限。</li>\n</ul><p>考虑到客户在内网直接测试Nginx可以正常完成，那么1、2、3基本可以排除。header size要重点排查，因为你也可以看到在Wireshark中，HTTP响应（蓝色字体）的header部分比较大，比如有好几个大尺寸的Set-Cookie头部，在Wireshark应用层信息窗口里翻好几页才能看完。</p><p>为了获取最权威的解释，我查阅了HAProxy版本1.5.0的<a href="https://cbonte.github.io/haproxy-dconv/1.5/configuration.html#tune.bufsize">官方文档</a>，并对比了v1.5.0的<a href="https://github.com/haproxy/haproxy/tree/v1.5.0">源代码</a>，终于发现了header size的秘密。</p><p>关键代码就在include/common/defaults.h文件中：</p><pre><code class="language-plain">/*\n * BUFSIZE defines the size of a read and write buffer. It is the maximum\n * amount of bytes which can be stored by the proxy for each session. However,\n * when reading HTTP headers, the proxy needs some spare space to add or rewrite\n * headers if needed. The size of this spare is defined with MAXREWRITE. So it\n * is not possible to process headers longer than BUFSIZE-MAXREWRITE bytes. By\n * default, BUFSIZE=16384 bytes and MAXREWRITE=BUFSIZE/2, so the maximum length\n * of headers accepted is 8192 bytes, which is in line with Apache\'s limits.\n */\n#ifndef BUFSIZE\n#define BUFSIZE         16384\n#endif\n\n// reserved buffer space for header rewriting\n#ifndef MAXREWRITE\n#define MAXREWRITE      (BUFSIZE / 2)\n</code></pre><p>也就是说：</p><ul>\n<li>HAProxy定义了一个读写缓存BUFSIZE。</li>\n<li>每次读取HTTP头部的时候，有可能会做增加header和改写header的操作，所以预留了一部分空间MAXREWRITE，它的值等于BUFSIZE/2。</li>\n<li>真正可以用来临时存放HTTP头部的缓存大小就是：<strong>BUFSIZE - MAXREWRITE = 16384 - 16384/2 = 8192字节。</strong> 也就是真正能接纳的HTTP请求的头部的大小，只有8192字节！</li>\n</ul><p>那么接下来，我们就来验证下header size是否真的超出了8KB。</p><p>依然是在Wireshark界面里，我们再次审视服务侧的请求和响应数据包，计算一下整体的header size。我们用这样一个过滤器，让展示出来的报文信息便于我们做统计：</p><pre><code class="language-plain">tcp.stream eq 0 and tcp.srcport eq 80\n</code></pre><p>这样的话，这次TCP流里的从后端服务器（源端口80）发回的数据量就清晰可见了：</p><p><img src="https://static001.geekbang.org/resource/image/fd/e8/fd9b6723374d4fe1a63bd369357af9e8.jpg?wh=1920x454" alt="图片"></p><p>上图中的红框部分，就是后端云主机Nginx返回的HTTP响应的大小。这里，又分别有两种方法来获得这个数值：</p><ul>\n<li>把TCP Seglen列的数字求和；</li>\n<li>直接用最后一个报文（24号报文）的nextSeq-1。</li>\n</ul><blockquote>\n<p>注意：减去的1是握手阶段的1。</p>\n</blockquote><p>用任何一种方法，算出来的都是 <strong>10791</strong> 字节。不过先别急，这是整个HTTP响应的大小，并不是HTTP headers的大小。我们还要减去HTTP body，这个body的大小要怎么获取呢？</p><p>你应该还记得在上节课里，我们学习过HTTP协议头部的构造，其中Content-Length头部就是表示了HTTP body的大小。那么在这里我们就可以利用这个属性：</p><p><img src="https://static001.geekbang.org/resource/image/88/d3/8867cca898e886289dbfa6895417f8d3.jpg?wh=464x75" alt="图片"></p><p>可见，HTTP body的大小就是940字节。我们做个减法：<strong>10791- 940 = 9851</strong></p><p>再去掉HTTP headers和body的分隔符即两个CRLF，它们是4个字节，那么最终得到HTTP headers的大小是：<strong>9851 - 4 = 9847</strong></p><p>显然9847超过了8192。根因已经一目了然了：<strong>HTTP Respose header部分的大小超过了默认限制的8KB</strong>！</p><p>这个原因也很好地解释了为什么选5个城市就会失败，而4个城市就能成功，因为前者生成的HTTP请求头部超过了8192字节，而后者正好没超。</p><p>后来的故事就比较简单了，我们做了两件事：</p><ul>\n<li>临时修改了云LB（HAProxy）的配置，把它的限制从8KB提升到16KB，这个问题立刻被解决了。</li>\n<li>作为长期方案，我们建议客户合理使用Set-Cookie头部，确保整体的HTTP Response size在一定的合理区间之内（8KB），避免无谓的系统开销和难以预料的问题的发生。</li>\n</ul><p>这样，客户的客户终于可以开开心心地去旅游玩耍，想去几个城市就去几个城市了。</p><p>我最后再简单回顾一下整个排查过程，希望对你有所启发：</p><pre><code>-&gt; 确认问题症状 \n-&gt; 排除法确定问题在LB \n-&gt; tshark统计发现大量502 \n-&gt; 根据前端连接的应用层uuid，找到后端连接的对应TCP流\n-&gt; 发现后端连接实际返回200，定位是HAProxy导致\n-&gt; 从文档和源码中确认是header size的限制 \n-&gt; 计算抓包文件中字节数，确认根因是超限\n-&gt; 提升header size limit，问题解决\n</code></pre><h2>小结</h2><p>这节课，我通过一个比较有趣的问题的排查过程，带你了解并学习了以下这些知识点，你需要重点掌握好。</p><ul>\n<li><strong>HTTP 502/503/504状态码的本质</strong></li>\n</ul><p>HTTP 5xx系列状态码的语义的本质：<strong>跟500不同，502、503、504都是LB /反向代理的后端的服务出了问题</strong>。基于这些理解，下一次你再遇到5xx的问题，相信就已经有比较充足的知识储备，能判断出是Web服务器本身有问题，还是反向代理 / LB有问题了。</p><ul>\n<li><strong>两侧不同TCP连接的映射</strong></li>\n</ul><p>在排查LB /反向代理的问题的时候，经常遇到一个重大的挑战：在左右两侧的不同TCP连接中，找到同一个应用层事务。这次我给你介绍了<strong>用应用层的uuid作为映射线索的方法</strong>。先在一侧的抓包文件中选定一个uuid，然后在另一侧的抓包文件中使用 <code>tcp contains "uuid"</code> 这样一个过滤器，找到对应这同一个应用层事务的另一侧的报文。</p><p>在<a href="https://time.geekbang.org/column/article/481042">第5讲</a>中，我也介绍过另外一种类似的找到对应报文的方法。但是注意，你不要把它们混淆起来了，其实这两个方法本质上是不同的，因为它们的场景完全不同。</p><p>这节课的场景是，客户端请求发给LB，LB转发请求给服务端，这是两个<strong>完全不同</strong>的TCP连接，只是因为是属于同一个应用层事务，所以<strong>同样的应用层数据</strong>（比如uuid）在两侧抓包中都有体现。</p><p>第5讲的场景是，客户端和服务端对同样的连接做了抓包，这两个抓包文件里的报文都是属于<strong>同一个TCP连接</strong>的，所以<strong>同样的传输层信息</strong>（比如序列号）在两端抓包中都有体现。</p><ul>\n<li><strong>结合产品文档和代码查找根因</strong></li>\n</ul><p>然后，我还给你介绍了如何结合程序文档（有时候要阅读源代码）和抓包分析中观察到的现象，彻底定位问题根因的方法。</p><p>在这个案例中，我们查看源码，发现了header size方面的限制，然后对抓包文件中的报文进行仔细的核对，终于证实了这个推断。你也可以借鉴这种思路，在遇到<strong>跟数据长度限制之类的的问题</strong>的时候，来完成类似的推理验证。</p><ul>\n<li><strong>tshark工具</strong></li>\n</ul><p>在工具方面，这节课我们也学习了一个新的强大工具：<strong>tshark</strong>。用tshark，我们可以方便的在命令行中就实现Wireshark图形界面中能做的各种过滤操作，对于快速排查问题、统计各种指标，都非常有帮助。比如用这条命令可以查看HTTP返回码：</p><pre><code class="language-plain">tshark -r file.pcap -T fields -e http.response.code\n</code></pre><h2>思考题</h2><p>最后，给你留两道思考题：</p><ul>\n<li>如果LB / 反向代理给客户端回复HTTP 503，表示什么呢？如果LB / 反向代理给客户端回复HTTP 500，又表示什么呢？</li>\n<li>这节课里，我介绍了使用应用层的某些特殊信息，比如uuid来找到LB两侧的报文的对应关系。你有没有别的好方法也可以做到这一点呢？</li>\n</ul><p>欢迎在留言区分享你的答案，也欢迎你把今天的内容分享给更多的朋友。</p><h2>附录</h2><p>示例文件已经上传至<a href="https://gitee.com/steelvictor/network-analysis/tree/master/17">Gitee</a>，建议结合文稿和Wireshark、tshark打开示例文件一起学习。</p>',
        article_title: "17 | 为什么前端页面里多选一个城市就报错？",
      },
      {
        title: "18 | 偶发性问题如何排查？",
        id: 491017,
        content:
          '<p>你好，我是胜辉。</p><p>在开始今天的课之前，我先问你一个问题：你在工作中没有遇到过那种“神出鬼没”的故障？就是说大部分时候情况都是正常的，但偶尔会来一下故障的那种。我猜有99%的可能你遇到过。</p><p>这种问题挺麻烦，经常是我们准备排查的时候，现场就没有了。那么，没有第一手的详细数据，我们还能查到问题的根因吗？</p><p>在我以往的实践当中，我发现不少人在对这个问题的认识上，会有两个常见的误区。</p><p><strong>误区1：没有现场，也没有抓包，但只要我们有历史记录，就能通过它查到根因。</strong></p><p>这里说的历史记录，是指应用日志、性能指标等。的确，很多问题通过查看历史记录，就可以解决。但还有一些问题场景，单靠历史记录是无法查到根因的。</p><p>比如这样一个场景：用户访问页面时偶尔遇到HTTP 503错误。而LB日志里，记录的其实也是一次HTTP 503，以及“后端服务不可用”这类信息。但是如果我们继续追问：为什么当时后端服务不可用呢？当时网络上有什么问题导致这种不可用呢？</p><p>日志并不能告诉我们这些问题的答案。这是它本身的局限性导致的，即它的视角是应用层，并不是网络层，所以天生就无法了解底层网络到底发生了什么。这种时候，你是不是有一种“隔靴搔痒”的感觉？</p><p>事实上，这类问题的排查，需要在<strong>重现</strong>（reproduce）问题时做全面的排查工作（比如抓包），拿到最真实全面的信息后，才能真正彻底完成。</p><!-- [[[read_end]]] --><p><strong>误区2：为了任何时候都能有现场数据，就一直抓包，一旦有问题就直接看抓包文件。</strong></p><p>这样做，理论上确实保留了现场，但是现实中却并不可行。主要有两大原因：</p><ul>\n<li>第一，<strong>抓包数据量太大</strong>。以一个平均流量在1Gbps的服务为例，1天就有10TB的网络报文数据。现在一个企业有几十个、上百个服务的情况也很常见，10TB再乘以10或者100这种系数，数据量就又上了几个数量级，这对存储、读取来说，都是极大的压力。</li>\n<li>第二，<strong>常态的抓包对系统性能的影响不能低估</strong>。因为抓包本身也占用内存和CPU资源，这会对已经有问题的系统产生“叠加伤害”，有可能会出现“不抓包的时候只有老问题，而抓包之后，除了老问题，还出现了新问题”。所以，常态化的抓包工作，既不可行，也没必要。</li>\n</ul><p>因此，在这个大背景下，我们如何抓包、抓多久，就很有讲究了。</p><p>那么今天，我们就通过一个实际的案例，一起来探讨下在面临偶发性问题时，我们都可以采取哪些有效的排查手段，来解决这种“想查的时候问题不来，问题来的时候没在查”的困境。</p><p>在这个过程中，你可以观察下整个重现和排查的方法论，并重点关注我<strong>对大量报文进行某个指标分析</strong>的技巧。这样，当你在日后遇到偶发性问题时，就可以参考今天所讲的内容，把问题真正地解决。</p><h2>我们可以采用怎样的重现+排查策略？</h2><p>我们先来了解下需要采用什么样的重现和排查策略。事实上，网络排查是一个更偏重实践的领域，所以只要能解决问题，我觉得都是合适的办法。针对偶发性问题的重现和排查，我个人会选择如下的策略。</p><h4>1.初步估计问题出现的时间跨度，对于问题何时重现有个预期</h4><p>通过对用户反馈、应用日志，以及监控仪表盘（Dashboard）的异常事件做初步统计，了解到问题是多久出现一次，这样就决定了我们在观察和抓包上需要投入的时间成本是多少。</p><p>举个例子，一个偶发性问题是几分钟出现一次，另外一个偶发性问题是几天才出现一次。如果你对后者只观察几分钟，显然是无效的。反过来，对前一个问题用好几天的时间去观察和抓包，则远超实际需求，虽然有效，却很低效。</p><blockquote>\n<p>另外，在这里我还要做个提醒。针对频率一会儿密集一会儿稀疏的问题，最好选取稀疏频率，也就是做保守估计。比如，某个问题在第一个小时出现了10次（平均6分钟一次），第二个小时却一次也没有，到第三个小时才又出现一次，那么我们认为其时间跨度是2小时（即第二和第三小时），而不是6分钟。</p>\n</blockquote><h4>2.在问题机器上发起抓包，根据预估的时间跨度，指定抓包方式。</h4><p>基于前一点的分析，我们知道了问题重现的大致时间跨度，由此也确定了做监控和抓包的时间跨度。无论是我们在监控上花的时间长短，还是抓包文件占用磁盘的大小，这些都是需要控制的成本。</p><p>这里面还有个矛盾：tcpdump抓包抓久了文件太大，抓短了又可能错过问题现场。那么，<strong>如何做到既可以抓取到期待的报文，同时抓包文件也不至于特别大呢？</strong>给你分享两个技巧。</p><p><strong>技巧一：利用tcpdump里跟循环抓包相关的几个参数。</strong></p><ul>\n<li>-W 个数：生成的循环文件数量，生成到最大数量后，新的报文数据会覆盖写入第一个文件，以此类推。</li>\n<li>-C 尺寸：每个文件的大小，单位是MB。</li>\n<li>-G 间隔时间：每次新生成文件的间隔时间，单位是分钟。</li>\n</ul><p>下面这个例子，就是每100MB或者60分钟（满足任一条件即可）就生成一个文件，一共10个文件：</p><pre><code class="language-plain">tcpdump -i eth0 -w file.pcap -W 10 -C 100 -G 60\n</code></pre><p><strong>技巧二：在循环抓包的基础上，再利用tcpdump的-s参数可以大幅减小抓包文件的大小。</strong></p><p>比如：</p><ul>\n<li>tcpdump -s 54：抓取到从二层到四层的报文头部（不带TCP Options）了。</li>\n<li>tcpdump -s 74：可以抓取到所有TCP Options，对排查TCP行为足够用。</li>\n</ul><p>如果要对应用层的头部（比如HTTP头部）也进行抓取，那可以设置更大的-s参数值。整体来说，这样的抓包文件，要比抓满的情况（1514字节）小很多。</p><blockquote>\n<p>补充：1514字节是网络层MTU的1500字节，加上帧头的14字节。所以一般不指定-s参数的抓包文件，其满载的帧大小是1514字节。</p>\n</blockquote><h4>3.定时观察，在问题重现时停止抓包。</h4><p>这里又分成两种不同的做法：</p><ul>\n<li><strong>直接人工观察应用日志或者仪表盘</strong>，比如每隔几分钟就刷新一次，直到问题重现。在问题出现频率相对高的场景下，这样做最为简单。这种做法类似软件设计里的轮询机制。</li>\n<li><strong>设定自动告警机制</strong>，当通过邮箱或者手机短信等途径收到相应告警时，就知道问题重现了。这样做的好处是，抓取的问题时间点更准确。现在有告警通知功能的监控系统很多，比如Prometheus + Grafana。这种做法类似软件设计里的事件通知机制。</li>\n</ul><h4>4.导出抓包文件，结合应用日志展开分析。</h4><p>这就是我们前面提到的“问题现场”详情了，在抓包文件里，所有的网络行为都会被记录下来。我们把报文情况跟应用层的报错情况做对比，就能极大地推动根因排查工作。</p><p>我把这个方法论整理成了一个简单的流程图，供你参考：</p><p><img src="https://static001.geekbang.org/resource/image/5c/71/5c0ea3757e87b3d2df8d04c6eb9d3171.jpg?wh=2000x528" alt=""></p><p>下面，我们就进入实际案例。</p><h2>实战案例：网站偶尔会变慢？</h2><p>曾经有个客户向我们报告了这样一个情况：他们的网站偶尔会变慢，比如正常1秒内就能完成的请求，可能会变成5秒钟或者更长。</p><p>但是这个情况不是每次必现，客户还做了功课，他们发现，问题出现的频率大约是几百次里面有一次。客户怀疑，是不是网络有丢包或者什么问题，导致了有时候HTTP请求或者响应没有被及时传输呢？所以就委托我们查清楚原因。</p><h3>预估：如何预估问题的出现频率和周期？</h3><p>按照前面的方法论，我们<strong>先做预估</strong>，也就是估计一下问题重现的时间跨度。</p><p>客户说“每几百次出现一次”，我们可以保守估计为1000次中出现一次问题。那么通过这个数据，能得出时间跨度吗？还不行，因为我们还不知道<strong>请求的频率</strong>是多少。</p><p>我们再次跟客户确认后，了解到他们业务还处在起步阶段，并不十分繁忙，访问频率大概在10次/分钟。现在我们做个简单的算术题：1000/10=100分钟，这就是预估的时间跨度。</p><h3>抓包：抓多长时间合适？</h3><p>有了预估的时间跨度，我们就可以做相应时长的抓包了。我们让客户在主机上用tcpdump抓取了100分钟左右的数据包，并发回给我们做分析。</p><h3>监控：如何重现问题？</h3><p>在这个时间段内，这个站点依然在对外服务。除此以外，客户那边的几个工程师也用浏览器不断刷新页面作为测试，发现也有遇到一次页面卡顿的情况。所以我们已经清楚，问题现场也就是跟问题相关的数据包，已经在抓包文件里了。</p><p>在这个案例里，因为客户那边的监控手段比较缺乏，没有全面的客户端日志，所以动用了“人肉”重现的方法。</p><p>其实，这个看起来很朴实的方法，在不少场景下也是挺有用的。我前面也提过，<strong>网络排查是一项偏向实践性的工作，只要有效的方法，我们都可以参考采用</strong>。当然如果你有Grafana等比较先进的监控系统，自然可以采用告警通知等更好的办法。</p><p><img src="https://static001.geekbang.org/resource/image/2a/dd/2aa3b64b995d21037cb9323ccc57d9dd.jpg?wh=2000x1125" alt=""></p><h2>抓包分析1：如何在网络视角上精确定义问题？</h2><p>接下来，就进入了分析阶段。不过在具体展开前，我们先思考两个简单的问题：</p><ul>\n<li>什么是慢/卡顿？</li>\n<li>多慢才是慢？</li>\n</ul><p>客户的应用是HTTP协议的，业务是在线二手商品交易，他们的客户是通过浏览器和App进来交易的。所以，针对第一个问题，“慢”对于浏览器来说，就可能是页面一直显示转圈圈的图，要很久才能加载好页面。对于App，就是界面里有很多内容空白，过一会儿才能正常使用。</p><p>第二个问题没有标准答案，但一般来说，一个页面响应超过3~4秒问题就很大了，1秒以内完成是最好的。页面打开越快，客户体验越好，成交量就越高。有研究发现，电商网站页面的加载时间只要快0.1秒，成交量就有明显的提升。</p><p>所以，基于以上的分析，我们确定了查找条件，就是“响应时间超过3秒的HTTP事务”相关的报文。但是当我们打开抓包文件，随机选取一个HTTP事务的报文，我们就能很方便地找到“响应时间超过3秒的HTTP事务”吗？</p><p><img src="https://static001.geekbang.org/resource/image/b0/d2/b085eaf6dac2598e3cbc3af7858eb6d2.png?wh=1155x206" alt="图片"></p><p>上图是某个随机选取的HTTP事务的TCP流。当然这里也是 <strong>Wireshark的一个小的知识点</strong>：左侧的两个水平方向的箭头（一个向右，一个向左），分别表示这两个数据包是HTTP请求和HTTP响应。然后我们可以根据第二列的时间差（Time），算出来HTTP耗时。</p><p>这次事务的耗时主要发生在642号和641号报文之间，大约是115毫秒，所以不符合3秒的条件。</p><h2>抓包分析2：如何用Wireshark实现对大量HTTP事务的性能分析？</h2><p>前面的做法是，先找到HTTP事务的报文，然后用Follow TCP Stream的方式找到这个TCP流，最后分析其HTTP请求和响应之间的耗时。如果抓包文件里只有几个这样的事务，这样做倒也无妨。但是，如果抓包文件里有几百几千个HTTP事务，难道也这么一个一个去对比分析？怕是几天都看不完。</p><p>事实上，我们还有更好的办法，也就是利用Wireshark的高级用法，来帮助我们实现对大量HTTP事务的某个性能指标进行解析和统计，也就是可以借助它的两个特性：<strong>过滤器（filter）和自定义列</strong>。</p><h3>过滤器</h3><p>我们先来看看如何使用Wireshark的过滤器，来实现方便的数据分析。</p><p>在这个案例里，我们关注的是HTTP事务的耗时。其实在Wireshark窗口里，我们可以选中一个HTTP响应报文，然后找到它的耗时。比如前面提到的115毫秒的那个事务，它的HTTP响应报文的详情是这样的：</p><p><img src="https://static001.geekbang.org/resource/image/34/1a/343587d46c4850yy81dfc65164ec4d1a.png?wh=618x399" alt="图片"></p><p>可以看到，<code>[Time since request: xxx seconds]</code> 就表示了HTTP耗时。它本身不是报文属性，而是Wireshark对报文的分析属性。所有Wireshark提供的分析属性，都会用<strong>方括号</strong>标识出来（这跟ps等命令里，把内核线程也用方括号标识出来的做法类似）。</p><p>这个HTTP耗时指标对应的Wireshark过滤器是 <code>http.time</code>。我们可以直接在Wireshark过滤器输入框里输入 <code>http.time</code>，看看会出现什么：</p><p><img src="https://static001.geekbang.org/resource/image/eb/ea/ebbc2c46d17f36881400a7073bd3b8ea.png?wh=653x307" alt="图片"></p><p>可见，所有的HTTP响应报文都被过滤显示出来了。这是因为Wireshark把http.time这个分析属性体现在HTTP响应报文上，所以这里搜出来的就都是响应报文了。而如果用过滤器http，就会同时显示HTTP请求和HTTP响应的报文。现在没有其他TCP包的干扰，看起来清爽多了。</p><p>不过等等，我们想要看的<strong>耗时信息在哪里呢？</strong>没有地方展示？</p><h3>自定义列</h3><p>到这里，就需要第二个特性上场了，也就是<strong>自定义列</strong>。跟之前课程里介绍的<a href="https://time.geekbang.org/column/article/481782">添加自定义列</a>的方法一样，我们在Wireshark窗口下方的http详情部分，选中前面介绍过的Time since request，右单击，选择Apply as column，然后Wireshark主窗口里就多了一列 <code>http.time</code>。</p><p>不过新增加的列默认出现在最右侧，你可以根据需要把它拖放到合适的位置，比如我把它拖到第三列，插入在info列之前，查看起来十分方便。</p><p><img src="https://static001.geekbang.org/resource/image/30/36/301281a684daaa0be292522485dee436.png?wh=729x309" alt="图片"></p><p>这样，我们结合 <code>http.time</code> 过滤器和 <code>http.time</code> 自定义列，就可以很直观地看清楚每个HTTP事务的耗时了。</p><p>好了，这时我们点击 <code>http.time</code> 列就可以排序了，一下子就可以找到耗时最高的几个事务，如下：</p><p><img src="https://static001.geekbang.org/resource/image/7b/e3/7b255eef494b951007941f65a72f0fe3.png?wh=738x216" alt="图片"></p><p>我们可以很清楚地看到，有两个事务消耗了长达5到6秒的时间，其他事务就相对很低了，都在1秒以内。既然已经定位到问题事务了，接下来按照常规套路，右单击这个包，选择Follow -&gt; TCP Stream，我们就可以跟踪到这个高耗时事务的前后TCP行为，进而可以定位是否是网络问题。</p><p><img src="https://static001.geekbang.org/resource/image/9e/b6/9e8882627282ca8baa15c944a1b065b6.png?wh=736x459" alt="图片"></p><p>这个完整的TCP流是这样的：</p><p><img src="https://static001.geekbang.org/resource/image/92/de/9244f5f49154e8494245d18dcde988de.png?wh=903x611" alt="图片"></p><p>显然，从第二列Time来看，6秒多的时间耗费在576号报文和647号报文之间，具体来说：</p><ul>\n<li>576号报文：服务端（监听在8082端口）回复了TCP确认，确认收到客户端发送过来的POST请求。</li>\n<li>中间<strong>停顿了6.5秒左右</strong>。</li>\n<li>647号报文：服务端回复的HTTP响应头部，可以在详情部分里看到HTTP返回码和多个header。</li>\n<li>648号报文：客户端确认收到HTTP响应头部。</li>\n<li>649号报文：服务端继续发送HTTP响应的body部分。在这里，因为HTTP响应报文已经完整（648+649号报文），Wireshark可以在这649报文上显示 <code>HTTP/1.1 200 OK</code> 这样的信息了。</li>\n<li>650号报文：客户端确认收到HTTP响应body部分，同时携带FIN标志位结束这个连接。</li>\n</ul><p>非常明显，问题是<strong>服务端收到HTTP请求后，花了大约6.5秒的时间才回复了HTTP响应</strong>。这充分说明两点：</p><ul>\n<li>网络上没有任何丢包、重传等问题；</li>\n<li>服务端响应耗时高达6秒以上。</li>\n</ul><p>既然排查了网络的嫌疑，客户就转而去检查应用层面的问题了。</p><h2><strong>是否还有其他可能？</strong></h2><p>当然你可能会问，是否有可能服务端及时发送HTTP响应了，只是一直丢包，导致6.5秒后客户端才收到呢？就是下图这种情况：</p><p><img src="https://static001.geekbang.org/resource/image/ee/4c/ee7ccdfd9f4834bb12cc5e948854744c.jpg?wh=1794x1013" alt=""></p><p>这个可能性当然是可能存在的，如果做得透彻一点，我们应该同时在客户端和服务端抓包，进行对比即可确认或排查这个可能性。具体的例子可以参考我们在<a href="https://time.geekbang.org/column/article/481042">第5讲</a>介绍的防火墙的案例，里面就是用两侧抓包来查出问题的。</p><p>但是在当前的案例中，假设真有网络问题引起的重传，那么这个<strong>重传行为不会只集中在HTTP响应</strong>上。</p><p>也就是说，一个有丢包表现的网络，不会“聪明”到只盯着HTTP响应去丢包，而是其他阶段也会有丢包。这里说的其他阶段，就是TCP握手、HTTP请求、TCP挥手，等等，丢包同样会引起这些阶段出现重传等现象。但是我们在这个抓包文件中并没有看到这类现象，因此反过来就说明，这个推测不太可能成立。</p><p>另外，细心的你可能已经注意到了，图中假想的丢包和重传，遵循了<a href="https://en.wikipedia.org/wiki/Exponential_backoff">指数退避原则</a>。这个知识点我们在<a href="https://time.geekbang.org/column/article/479163">第3讲</a>有介绍过，你如果觉得生疏了，可以去复习一下。</p><h2><strong>如何用命令行工具做同样的分析？</strong></h2><p>前面我介绍了如何在Wireshark里结合过滤器和自定义列，来定位“高耗时HTTP事务”。不过因为图形界面的关系，这个过程有好几步操作，稍显麻烦，有没有更快一些的办法呢？</p><p>其实是有的，这次我们不用Wireshark图形界面程序，而是利用它自带的另外一个命令行工具 <strong>tshark</strong>，我们在上节课里也介绍过这个工具<strong>。</strong></p><p>就拿前面这个案例来说，我们可以直接用tshark在命令行中，实现对高耗时HTTP事务的获取。直接运行以下命令：</p><pre><code class="language-plain">tshark -r 文件名 -T fields -e http.time | grep -v ^$\n</code></pre><p>示例输出如下：</p><p><img src="https://static001.geekbang.org/resource/image/d6/52/d66f4b615a651dc0b389e0e0eyyde252.png?wh=479x321" alt="图片"></p><p>注意，我们在命令的后面，必须加上<strong>管道符“ | ”和“grep -v ^$”</strong>，要不然就会看到下面这种，有大段空白的输出（因为凡是非HTTP的数据包都没有输出，会成为一个空行）：</p><p><img src="https://static001.geekbang.org/resource/image/13/46/13075fe935cfb325511371f6b2c07946.png?wh=480x290" alt="图片"></p><p>当然，如果要针对某个高耗时的事务进行数据面的分析，那光看http.time这一个指标是不够的，你需要知道这个事务对应的包号、TCP流号等信息，并追踪这个流里面的数据包。比如，你可以运行以下命令，把http.time耗时最高的事务对应的TCP流数据包，都在命令行里展示出来：</p><pre><code class="language-plain">tshark -r 文件名 -T fields -e frame.number -e http.time -e tcp.stream | sort -k2 -r | head -1 | awk \'{print $3}\' | xargs -n 1 -I {} tshark -r captured.pcap -Y "tcp.stream eq {}"\n</code></pre><p>我来解释一下其中的几个参数：</p><ul>\n<li><code>-T fields -e frame.number -e http.time -e tcp.stream</code>：表示要展示数据包哪几个列的信息。frame.number表示帧号（包号）、http.time表示HTTP耗时、tcp.stream表示TCP流号。</li>\n<li><code>-Y "tcp.stream eq {}"</code>：在管道符后面使用，指的是需要把这个TCP流号相关的数据包都展示出来。因为结合了xargs命令，所以eq后面是一个<code>{}</code>符号，这个值就是TCP流的编号（由awk命令输出）。</li>\n</ul><p>下图是对这个案例应用此命令得到的输出，这一系列数据包跟我们前面在Wireshark图形界面里Follow TCP Stream，而得到的那些数据包是一致的：</p><p><img src="https://static001.geekbang.org/resource/image/ab/98/abf3a85e16062d1546103b8a305ed698.png?wh=744x277" alt="图片"></p><p>是不是有点神奇？我们在图形界面里能做的事情，在命令行里也一样能做到，只要借助tshark。而正因为它是命令行形式，所以具备下面这些<strong>优势</strong>：</p><ul>\n<li>图形界面里需要反复多次进行的操作，在命令行里利用管道特性，一条命令就可以完成。</li>\n<li>图形程序集中在单个文件的处理，而命令行可以方便地对多个文件进行批量处理。</li>\n<li>命令行的输入和输出都是文本，便于复制，在多人沟通协作场景下更显便利。</li>\n</ul><p>这样，通过利用tshark对大量HTTP事务的耗时进行统计，我们很容易就能得出整体的HTTP事务性能状况了，比如平均耗时、各耗时区间的事务占全部事务的比例，等等。这不仅能在<strong>问题排查</strong>的时候提供帮助，而且在一些<strong>性能调优</strong>的场合，还可以给我们提供应用日志以外的另一种性能数据源。</p><h2>小结</h2><p>这节课，我给你介绍了一种排查偶发性问题，你可以重点关注和回顾以下这些知识点。</p><ul>\n<li><strong>偶发性问题的排查思路</strong></li>\n</ul><p>即先预估时间，然后一边抓包一边观察问题是否出现，在问题出现后停止抓包，最后进行抓包分析。也就是这样的流程：</p><p><img src="https://static001.geekbang.org/resource/image/5c/71/5c0ea3757e87b3d2df8d04c6eb9d3171.jpg?wh=2000x528" alt=""></p><ul>\n<li><strong>排查技巧</strong></li>\n</ul><p>首先是控制tcpdump抓包大小的方法。</p><p>为了控制抓包文件在合理的范围，我们可以用两种方法。</p><ul>\n<li>方法一：运行tcpdump时指定循环参数，比如每隔多少MB或者每隔多少分钟就生成一个文件，一共生成n个文件，循环使用。</li>\n<li>方法二：tcpdump -s参数，指定-s后面的数值为54到74，就可以抓取到二层到四层的头部信息了。如果要抓取应用层头部，可以指定相应的更大的值。</li>\n</ul><p>第二是找到HTTP耗时最高事务的方法。</p><p>为了找到HTTP耗时最高的事务，我们可以通过<strong>过滤器</strong> <code>http.time</code> 或者 <code>http</code>，把HTTP事务都过滤出来，然后增加一个<strong>自定义列</strong>来展示HTTP耗时。最后，我们点击这个列，就完成对HTTP耗时这个自定义列的排序了。</p><p>而除了Wireshark这个工具以外，我们用<strong>tshark</strong>命令行工具，也同样可以实现“从大量报文中对某个指标进行解析和排序”的目的。</p><p>第三是找到HTTP事务的报文对的方法。</p><p>在Wireshark中，选中HTTP的请求或者响应报文时，这个报文以及与它对应的响应或者请求报文的左边，会出现<strong>两个水平方向的箭头（一个向右，一个向左），表示它们属于同一个HTTP事务。</strong></p><h2>思考题</h2><p>最后，给你留两道思考题：</p><ul>\n<li>前面我介绍了使用tshark来找到耗时最高的HTTP事务的方法。关于tshark，你自己还有哪些使用经验呢？</li>\n<li>在“是否还有其他可能？”这里，我提到了可能的重传。如果要验证是否真的存在这种重传，你觉得应该做什么呢？</li>\n</ul><p>欢迎在留言区分享你的答案，也欢迎你把今天的内容分享给更多的朋友，我们一起成长、进步。</p>',
        article_title: "18 | 偶发性问题如何排查？",
      },
      {
        title: "19 | TLS的各种特性：TLS握手为什么会失败？",
        id: 491674,
        content:
          '<p>你好，我是胜辉。</p><p>在前面三节课里，我带你排查了HTTP协议相关的问题。不知你有没有注意到，这三个案例里的HTTP都没有做加密，这就使得我们的排查工作省去了不少的麻烦，在抓包文件里直接就可以看清楚应用层的信息了。但在现实场景下，越来越多的站点已经做了HTTPS加密，所以像前面的三讲那样，在Wireshark里直接看到应用层信息的情况，已经越来越少了。</p><p>根据w3techs.com的<a href="https://w3techs.com/technologies/details/ce-httpsdefault">调查数据</a>，目前Internet上78%以上的站点，都默认使用了HTTPS。显而易见，要对Internet上的问题做应用层方面的分析，TLS是一道绕不开的坎。</p><p>那你可能会问了：“我主要处理内网的问题，应该不用关心太多HTTPS的事了吧？”</p><p>这句话也许目前还勉强算对，但是随着各大企业不断推进零信任（<a href="https://en.wikipedia.org/wiki/Zero_trust_security_model">Zero Trust</a>）安全策略，越来越多的内网流量也终将运行在HTTPS上，内网和公网将没有区别。</p><p>所以说，掌握HTTPS/TLS的相关知识和排查技巧，对于我们开展网络排查来说，是一项必备的技能了。</p><p>那么接下来的两节课，我们会集中到HTTPS/TLS这个主题上，来全面学习一下它的工作原理、常见问题和排查思路。这样以后面临HTTPS/TLS的问题时，你就可以运用这两讲里学到的知识和方法，展开排查工作了。</p><!-- [[[read_end]]] --><h2>什么是HTTPS？</h2><p>首先我们要认识一下HTTPS。它其实不是某个独立的协议，而是HTTP over TLS，也就是把HTTP消息用TLS进行加密传输。两者相互协同又各自独立，依然遵循了网络分层模型的思想：</p><p><img src="https://static001.geekbang.org/resource/image/e8/72/e84ea94b614dcb227ecfeeb158cc9a72.jpg?wh=2000x1125" alt=""></p><blockquote>\n<p>补充：这也就是我们在<a href="https://time.geekbang.org/column/article/477510">第1讲</a>学习网络分层模型时候的图。</p>\n</blockquote><p>为了更好地理解HTTPS，我们也来简单学习一下加密技术，因为它是HTTPS的核心。</p><h3>加密技术基础</h3><p>加密技术其实也是一个古老的话题。早在公元前400年，斯巴达人就创造了密码棒加密法。就是把纸条缠绕在一根木棒上，然后在纸上写字，这张纸条离开这根木棒后，就无法正确读取了。要“破解”它，就得找到同样粗细的木棒，然后把纸条绕上去后，才能解读。</p><p><img src="https://static001.geekbang.org/resource/image/94/4a/949941bea434b5b7a9407fd2d73c134a.png?wh=640x366" alt=""></p><p>那么在这里，纸条就相当于密文，而木棒，就相当于密钥了。而因为加密和解密用的木棒是相同的，所以它属于<strong>对称加密算法</strong>。</p><p>时间推进到现代，密码专家们已经开发出了众多优秀的对称加密算法，比如AES、DES。与木棒加密法类似，Alice和Bob都知道同一把密钥，Alice用这个密钥做加密，Bob收到密文后，也是用这把密钥做解密，就得到了明文。如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/97/1b/975da34817e7154f4fyy27b943862c1b.jpg?wh=2000x914" alt=""></p><p>另外一类算法就是<strong>非对称算法</strong>，这也是PKI（<a href="https://zh.wikipedia.org/wiki/%E5%85%AC%E9%96%8B%E9%87%91%E9%91%B0%E5%9F%BA%E7%A4%8E%E5%BB%BA%E8%A8%AD">公开密钥架构</a>）的基础。在非对称算法中，加密和解密用了不同的密钥，这两个密钥形成了密钥对。比如Bob和Alice都各自生成了密钥对，然后互相交换了公钥。Alice用Bob的公钥对明文做了加密，变成密文传给Bob。Bob收到后，用自己的私钥解密，就还原出了明文。如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/52/e9/52aceaa7c0c2b358b1db90fa94dc63e9.jpg?wh=2000x946" alt=""></p><h3>TLS基础</h3><p>那么TLS跟加密技术的关系具体是怎样的呢？实际上，<strong>TLS同时使用了对称算法和非对称算法</strong>。TLS的整个过程大致可以分为两个主要阶段：</p><ul>\n<li>握手阶段，完成验证，协商出密码套件，进而生成对称密钥，用于后续的加密通信。</li>\n<li>加密通信阶段，数据由对称加密算法来加解密。</li>\n</ul><p>TLS综合利用了对称算法和非对称算法的优点，因为对称算法的效率高，而非对称算法的安全性高，所以两者结合，就兼顾到了效率和安全性。不得不说，TLS确实是一个很精妙的设计。</p><p>那么同样地，我们对TLS相关问题的排查，也就面临着<strong>两类问题</strong>：一类是TLS握手阶段的问题，一类是TLS通信过程中的问题。</p><p>在TLS握手阶段，真正的加密还没开始，所以依托于明文形式的握手信息，我们还有可能找到握手失败的原因。在这一阶段，我们需要掌握TLS握手的原理和技术细节，这样才能指导我们展开排查工作。</p><p>而在TLS数据交互阶段，加密已经开始，所有的数据已经是密文了。假如应用层发生了什么，而我们又看不到，那如何做排查呢？这个时候，我们需要<strong>把密文解密</strong>，才能找到根因。不过你可能会问：“TLS要是能随便解密，是不是说明这个协议还有漏洞啊？”</p><p>放心，TLS是很安全的。我说的解密，当然是有前提条件的，跟数据安全性并不冲突。具体的细节，我到下节课会给你详细展开。</p><p>下面呢，我们就来看看案例，一起来学习下TLS握手失败的问题排查思路。</p><h2>案例1：TLS握手失败</h2><p>TLS握手失败，估计你也遇到过。引起这个问题的原因还是比较多的，比如域名不匹配、证书过期等等。不过，这些问题一般都可以通过“忽略验证”这个简单的操作来跳过。比如，在浏览器的警告弹窗里点击“忽略”，就可以让整个TLS的过程继续下去。</p><p>而还有一些问题，就无法跳过了。</p><p>我们曾经遇到的一个例子就是这样。当时，我们有一个应用需要访问Kubernetes集群的API server。因为我们有很多个集群，所以相应的API server也有很多个。这个问题是，从同一台客户端去访问API server 1是可以的，但访问API server 2就不行。进而发现，失败原因就是TLS握手失败。</p><p><img src="https://static001.geekbang.org/resource/image/8e/52/8e4fb5692af4f8b36f62b4b70c8af952.jpg?wh=1531x604" alt=""></p><p>在客户端的应用日志里，报告的是这段错误：</p><pre><code class="language-plain">javax.net.ssl.SSLHandshakeException: Received fatal alert: handshake_failure\n</code></pre><p>这段日志有没有告诉我们有价值的信息呢？好像并不多，只是告诉我们握手失败了。这也是我反复提及的，网络排查中两大鸿沟之一的<strong>应用现象跟网络现象之间的鸿沟</strong>：你可能看得懂应用层的日志，但是不知道网络上具体发生了什么。</p><blockquote>\n<p>补充：我在<a href="https://time.geekbang.org/column/article/480068">第4讲</a>里有介绍这两大鸿沟，我们要在网络排查方面取得实质性的进步，关键在于突破这两个鸿沟。</p>\n</blockquote><p>同样的，这里的日志也无法告诉我们：到底TLS握手哪里出了问题。所以我们需要做点别的事情。</p><h3>排除服务端问题</h3><p>首先，我们用另外一个趁手的小工具 <strong>curl</strong>，从这台客户端发起对API server 2（也就是握手失败的那个）的TLS握手，发现其实是可以成功的。这说明，API server 2至少在某些条件下是可以正常工作的。我们来看一下当时的输出：</p><pre><code class="language-plain">curl -vk https://api.server.777.abcd.io\n* Rebuilt URL to: https://api.server.777.abcd.io/\n* Trying 10.100.20.200...\n* Connected to api.server.777.abcd.io (10.100.20.200) port 443 (#0)\n* found 153 certificates in /etc/ssl/certs/ca-certificates.crt\n* found 617 certificates in /etc/ssl/certs\n* ALPN, offering http/1.1\n* SSL connection using TLS1.2 / ECDHE_RSA_AES_128_GCM_SHA256\n* server certificate verification SKIPPED\n* server certificate status verification SKIPPED\n* common name: server (does not match \'api.server.777.abcd.io\')\n* server certificate expiration date OK\n* server certificate activation date OK\n* certificate public key: RSA\n* certificate version: #3\n* subject: CN=server\n* start date: Thu, 24 Sep 2020 21:42:00 GMT\n* expire date: Tue, 23 Sep 2025 21:42:00 GMT\n* issuer: C=US,ST=San Francisco,L=CA,O=My Company Name,OU=Org Unit 2,CN=kubernetes-certs\n* compression: NULL\n</code></pre><blockquote>\n<p>补充：在第8行可以看到协商出的密码套件 <code>* SSL connection using TLS1.2 / ECDHE_RSA_AES_128_GCM_SHA256</code>。</p>\n</blockquote><p>既然curl是可以TLS握手成功的，那是不是客户端程序本身有点问题呢？我们就进行了“问题复现”。在<a href="https://time.geekbang.org/column/article/491017">上节课</a>里我们讨论了偶发性问题的“复现+抓包”的策略，而这里的问题是必现的，所以只要发起一次请求，同时做好抓包就可以了。</p><p>我们来看一下抓包文件：</p><p><img src="https://static001.geekbang.org/resource/image/37/dd/37326f7b60d56b9c7dc0yy74afd430dd.jpg?wh=1159x200" alt="图片"></p><p>还真是“话不投机半句多”，客户端也就发了一个Client Hello报文，服务端就回复TLS Alert报文，结束了这次对话。那为啥聊不起来呢？我们看一下这个Alert报文：</p><p><img src="https://static001.geekbang.org/resource/image/21/76/21381c953b6b3b33cc49d34d29e93676.jpg?wh=712x254" alt="图片"></p><p>这个TLS Alert报文显示，它的编号是40，指代的是Handshake Failure这个错误类型。到这一步，我们需要去了解这个错误类型的具体定义。<strong>正确的做法是：去RFC里寻找答案</strong>，而不是随意地去网络上搜索，因为很可能你会被一些似是而非的信息误导。</p><p>因为这次握手用的是TLS1.2协议，我们就来看它的<a href="https://datatracker.ietf.org/doc/html/rfc5246">RFC5246</a>。在这个RFC里，找到Alert Protocol部分，我们看看它是怎么说的：</p><pre><code class="language-plain">   handshake_failure\n      Reception of a handshake_failure alert message indicates that the\n      sender was unable to negotiate an acceptable set of security\n      parameters given the options available.  This is a fatal error.\n</code></pre><p>结合这里的实际场景，这段话的意思就是：“基于已经收到的Client Hello报文中的选项，TLS服务端无法协商出一个可以接受的安全参数集”。而这个所谓的安全参数集，在这里具体指的就是加密算法套件 <strong>Cipher Suite</strong>。我们来认识一下它。</p><blockquote>\n<p>补充：这里的suite读音是sweet而不是suit，我也错读过很多年。另外，suite还有旅馆套房的意思。</p>\n</blockquote><h3>Cipher Suite</h3><p>前面提到过，在TLS中，真正的数据传输用的加密方式是<strong>对称加密</strong>；而对称密钥的交换，才是使用了<strong>非对称加密</strong>。实际上，TLS的握手阶段需要在下面四个环节里实现不同类型的安全性，它们可以说是TLS的“四大护法”。</p><ul>\n<li><strong>密钥交换算法</strong>：保证对称密钥的交换是安全的，典型算法包括DHE、ECDHE。</li>\n<li><strong>身份验证和签名算法</strong>：确认服务端的身份，其实就是对证书的验证，非对称算法就用在这里。典型算法包括RSA、ECDSA。</li>\n</ul><blockquote>\n<p>补充：如果是双向验证（mTLS），服务端会验证客户端的证书。</p>\n</blockquote><ul>\n<li><strong>对称加密算法</strong>：对应用层数据进行加密，典型算法包括AES、DES。</li>\n<li><strong>消息完整性校验算法</strong>：确保消息不被篡改，典型算法包括SHA1、SHA256。</li>\n</ul><p>每一个类型都有很多不同的具体算法实现，它们的组合，就是密码套件Cipher Suite。你可能以前也见过它，这次咱们来拆解认识一下它的组成结构。</p><p>先看一个典型的密码套件：</p><center>\n<p><strong>TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA（0xc013）</strong></p>\n</center><ul>\n<li>TLS不用多说，代表了TLS协议。</li>\n<li>ECDHE是密钥交换算法，双方通过它就不用直接传输对称密钥，而只需通过交换双方生成的随机数等信息，就可以各自计算出对称密钥。</li>\n<li>RSA是身份验证和签名算法，主要是客户端来验证服务端证书的有效性，确保服务端是本尊，非假冒。</li>\n<li>AES128_CBC是对称加密算法，应用层的数据就是用这个算法来加解密的。这里的CBC属于块式加密模式，另外一类模式是流式加密。</li>\n<li>SHA就是最后的完整性校验算法（哈希算法）了，它用来保证密文不被篡改。</li>\n<li>0xc013呢，是这个密码套件的编号，每种密码套件都有独立的编号。完整的编号列表在 <a href="https://www.iana.org/assignments/tls-parameters/tls-parameters.xhtml">IANA的网站</a>上可以找到。</li>\n</ul><p>另外，在不同的客户端和服务端软件上，这些密码套件也各不相同。所以，TLS握手的重要任务之一，就是<strong>找到双方共同支持的那个密码套件</strong>，也就是找到大家的“共同语言”，否则握手就必定会失败。</p><p>所以这个案例排查的下一步，就是要搞清楚，客户端和服务端到底都支持了哪些Cipher Suite。</p><p>那么客户端的密码套件有哪些呢？你可能很快想到了前面curl命令里的输出。确实，那里就明确显示，双方协商出来的是 <strong>ECDHE_RSA_AES_128_GCM_SHA256</strong>。但是，这里有两个问题：</p><ul>\n<li>这个是协商后达成的结果，只是一个套件，而不是套件列表。</li>\n<li>更加关键的是，这个密码套件是curl这个客户端的，而不是出问题的客户端。</li>\n</ul><p>所谓出问题的客户端，就是实际的业务代码去连接API server时候用的客户端，它是一个Java库，而不是curl，这一点一定要分清。</p><p><img src="https://static001.geekbang.org/resource/image/09/d4/09fd882c5222d05cfae9ce3f753633d4.jpg?wh=1517x608" alt=""></p><p>那么，我们怎么获得这个Java库能支持的密码套件列表呢？其实最直接的办法，还是用<strong>抓包分析</strong>。我们回到前面那个抓包文件，检查一下Client Hello报文。在那里，就有Java库支持的密码套件列表。</p><p><img src="https://static001.geekbang.org/resource/image/eb/2a/eb521f76bd305904098d6f76c6188e2a.jpg?wh=1792x1396" alt="图片"></p><blockquote>\n<p>补充：这个列表往下还有，因为屏幕小，我没有全部展示。</p>\n</blockquote><p>找到了客户端的密码套件列表，接下来是不是就去找服务端的密码套件的列表呢？不过，这个抓包里，服务端直接回复了Alert消息，并没有提供它支持的密码套件列表。那我们的排查如何继续推进呢？</p><p>其实，可以换个思路：看看服务端在TLS握手成功后用了哪个密码套件，而不是去拿到它的全部列表。前面curl已经成功了，<strong>我们来看下curl那次协商出来的套件是哪个，看它是否被Java库支持，就可以判定了</strong>。</p><p>我们要导出这次Client Hello里面的密码套件列表，可以这样做：选中Cipher Suite，右单击，选中Copy，在次级菜单中选中All Visible Selected Tree Items：</p><p><img src="https://static001.geekbang.org/resource/image/48/b0/484f4d01d57635fa3de48eda952794b0.jpg?wh=1472x690" alt="图片"></p><p>这样，我们就得到了下面这个列表：</p><pre><code class="language-plain">Cipher Suites (28 suites)\n&nbsp; &nbsp; Cipher Suite: TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256 (0xc023)\n&nbsp; &nbsp; Cipher Suite: TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256 (0xc027)\n&nbsp; &nbsp; Cipher Suite: TLS_RSA_WITH_AES_128_CBC_SHA256 (0x003c)\n&nbsp; &nbsp; Cipher Suite: TLS_ECDH_ECDSA_WITH_AES_128_CBC_SHA256 (0xc025)\n&nbsp; &nbsp; Cipher Suite: TLS_ECDH_RSA_WITH_AES_128_CBC_SHA256 (0xc029)\n&nbsp; &nbsp; Cipher Suite: TLS_DHE_RSA_WITH_AES_128_CBC_SHA256 (0x0067)\n&nbsp; &nbsp; Cipher Suite: TLS_DHE_DSS_WITH_AES_128_CBC_SHA256 (0x0040)\n&nbsp; &nbsp; Cipher Suite: TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA (0xc009)\n&nbsp; &nbsp; Cipher Suite: TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA (0xc013)\n&nbsp; &nbsp; Cipher Suite: TLS_RSA_WITH_AES_128_CBC_SHA (0x002f)\n&nbsp; &nbsp; Cipher Suite: TLS_ECDH_ECDSA_WITH_AES_128_CBC_SHA (0xc004)\n&nbsp; &nbsp; Cipher Suite: TLS_ECDH_RSA_WITH_AES_128_CBC_SHA (0xc00e)\n&nbsp; &nbsp; Cipher Suite: TLS_DHE_RSA_WITH_AES_128_CBC_SHA (0x0033)\n&nbsp; &nbsp; Cipher Suite: TLS_DHE_DSS_WITH_AES_128_CBC_SHA (0x0032)\n&nbsp; &nbsp; Cipher Suite: TLS_ECDHE_ECDSA_WITH_3DES_EDE_CBC_SHA (0xc008)\n&nbsp; &nbsp; Cipher Suite: TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA (0xc012)\n&nbsp; &nbsp; Cipher Suite: TLS_RSA_WITH_3DES_EDE_CBC_SHA (0x000a)\n&nbsp; &nbsp; Cipher Suite: TLS_ECDH_ECDSA_WITH_3DES_EDE_CBC_SHA (0xc003)\n&nbsp; &nbsp; Cipher Suite: TLS_ECDH_RSA_WITH_3DES_EDE_CBC_SHA (0xc00d)\n&nbsp; &nbsp; Cipher Suite: TLS_DHE_RSA_WITH_3DES_EDE_CBC_SHA (0x0016)\n&nbsp; &nbsp; Cipher Suite: TLS_DHE_DSS_WITH_3DES_EDE_CBC_SHA (0x0013)\n&nbsp; &nbsp; Cipher Suite: TLS_ECDHE_ECDSA_WITH_RC4_128_SHA (0xc007)\n&nbsp; &nbsp; Cipher Suite: TLS_ECDHE_RSA_WITH_RC4_128_SHA (0xc011)\n&nbsp; &nbsp; Cipher Suite: TLS_RSA_WITH_RC4_128_SHA (0x0005)\n&nbsp; &nbsp; Cipher Suite: TLS_ECDH_ECDSA_WITH_RC4_128_SHA (0xc002)\n&nbsp; &nbsp; Cipher Suite: TLS_ECDH_RSA_WITH_RC4_128_SHA (0xc00c)\n&nbsp; &nbsp; Cipher Suite: TLS_RSA_WITH_RC4_128_MD5 (0x0004)\n&nbsp; &nbsp; Cipher Suite: TLS_EMPTY_RENEGOTIATION_INFO_SCSV (0x00ff)\n</code></pre><p>可见，里面确实没有 <strong>ECDHE_RSA_AES_128_GCM_SHA256</strong> 这个套件。所以到这里，我们可以确认问题根因了：因为这个Java库和API server 2之间，没有找到共同的密码套件，所以TLS握手失败。</p><p>根因找到了，下一步就是升级Java库，让双方能够协商成功。</p><blockquote>\n<p>补充：API server 1能兼容这个相对旧的Java库，所以没有问题。</p>\n</blockquote><p>你觉得这个问题难吗？其实还好，对吧？这是因为我们一旦对协议本身有准确的理解，那么很多问题就容易被“看穿”。这也说明了理论知识的重要性。</p><p>好，我们再来看一个复杂一点的案例。</p><h2>案例2：有效期内的证书为什么报告无效？</h2><p>有一次，一个产品开发团队向我们运维团队报告了一个问题：他们的应用在做了代码发布后，就无法正常访问一个内部的HTTPS站点了，报错信息是：certificate has expired。</p><p>这就很奇怪了，我们日常对证书都做了自动更新处理，不会有“漏网之鱼”。然后我们也手工检查了这个HTTPS站点的证书，确定是在有效期内的，这就使得这个报错显得尤其古怪。</p><p>既然是代码发布后新出现的问题，那我们自然认为问题是跟发布有关。我们了解到：这次确实有个变更，会在客户端打开服务端证书校验的特性，而这个特性在以前是不打开的。但这还是无法解释，为什么客户端居然会认为，一个明明在有效期内的证书是过期的。</p><p><img src="https://static001.geekbang.org/resource/image/fb/f3/fb642fyy4cf4caf2f72a5b11c779ddf3.jpg?wh=1652x863" alt=""></p><p>真是“秀才遇到兵”，感觉“讲理”是行不通了，于是我们换了个思路，不纠结在有效期的问题上。跟前一个案例类似，我们用交叉验证的方式来推进排查。具体做法是：在这台客户端和另一台客户端上，用OpenSSL向这个HTTPS站点发起TLS握手。</p><p><img src="https://static001.geekbang.org/resource/image/35/37/35c4d5314cd8b71f3e91c03bc8f53137.jpg?wh=1757x905" alt=""></p><p>结果我们发现了更有意思的情况：从另外一台客户端的OpenSSL去连接这个HTTPS站点，也报告certificate has expired。</p><p>这给了我们很大的信心：既然OpenSSL可以复现这个问题，那我们就可以做进一步的检查了！因为OpenSSL属于OS上的命令，虽然我们不了解如何在Node.js上做debug，但是我们对如何在OS上做排查是很有经验的。</p><p>于是，我们在OpenSSL命令前面加上 <strong>strace</strong>，以便于追踪OpenSSL在执行过程中，特别是在报告certificate has expired之前，具体发生了什么。执行这个命令：</p><pre><code class="language-clojure">strace openssl s_client -tlsextdebug -showcerts -connect abc.ebay.com:443 \n</code></pre><p>输出的关键部分如下：</p><pre><code class="language-plain">stat("/usr/lib/ssl/certs/a1b2c3d4.1", {st_mode=S_IFREG|0644, st_size=2816, ...}) =&nbsp;0\nopenat(AT_FDCWD,&nbsp;"/usr/lib/ssl/certs/a1b2c3d4.1", O_RDONLY) =&nbsp;6\n......\nwrite(2,&nbsp;"verify return:1\\n", 16verify&nbsp;return:1\n)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =&nbsp;16\n.......\nwrite(2,&nbsp;"verify error:num=10:certificate "..., 44verify error:num=10:certificate has expired\n) =&nbsp;44\nwrite(2,&nbsp;"notAfter=", 9notAfter=)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =&nbsp;9\nwrite(2,&nbsp;"Oct 14 18:45:33 2020 GMT", 24Oct&nbsp;14&nbsp;18:45:33&nbsp;2020&nbsp;GMT) =&nbsp;24\n</code></pre><p>这里的关键信息是：</p><ul>\n<li>OpenSSL读取了<code>/usr/lib/ssl/certs</code>目录下的文件 <code>a1b2c3d4.1</code>。</li>\n<li>接着，OpenSSL就报告了certificate has expired的错误，expire的日期是2020年10月24日（输出中的“24Oct&nbsp;14”）。</li>\n</ul><p>这又是一个明显的进展：很可能就是这个文件导致了错误。这是个什么文件，为什么会导致错误呢？</p><p>其实，它就是TLS客户端本地的Trust store里，存放的中间证书文件。Trust store一般用来存放根证书和中间证书文件，你可能对这几个名词还不太熟悉，我给你介绍一下TLS证书校验的原理。</p><blockquote>\n<p>补充：一般来说，证书先存入文件系统，然后通过命令或者代码，导入到应用的Trust store。</p>\n</blockquote><h3>TLS证书链</h3><p>TLS证书验证是“链式”的机制。比如，客户端存有根证书和它签发的中间证书，那么由中间证书签发的叶子证书，就可以被客户端信任了，也就是这样一条信任链：</p><center>\n<p><strong>信任根证书 -&gt; 信任中间证书 -&gt; 信任叶子证书</strong></p>\n</center><p>我画了三种不同情况下的信任链的示意图，供你参考：</p><p><img src="https://static001.geekbang.org/resource/image/32/a6/324934a90cef3531ca2b46faf70e86a6.jpg?wh=2000x1125" alt=""></p><p>场景1和3中，信任链是完整的，证书验证就可以通过。场景2中，由于中间证书既不在客户端的Trust store里，也不在服务端回复的证书链中，这就导致信任链断裂，验证就会失败。</p><p>而我们发现在这个案例里，服务端发送的证书链中包含了正确的中间证书，那为什么还会失败呢？其实这是因为，从前面strace openssl的输出里已经发现，客户端本地也有一张中间证书，而且是<strong>过期的</strong>，示意图如下：</p><p><img src="https://static001.geekbang.org/resource/image/66/f3/66bf55b67efb27ff084b9dec871acaf3.jpg?wh=1616x842" alt=""></p><p>这两张中间证书，签发机构是同一个CA，证书名称也相同，这就导致了OpenSSL在做信任链校验时，优先用了本地的中间证书，进而因为这张本地的中间证书确实已经过期，导致OpenSSL抛出了certificate has expired的错误！</p><p>这个结论你看明白了吗？你也许觉得还是有哪里不对，比如你可能会问：“照理说叶子证书是新的中间证书签发的，那用老的中间证书去验证叶子证书的签名的时候，应该会失败啊？”</p><p>你说得没错。这里最烧脑的地方在于：这两张中间证书，不仅签发机构一样，名称一样，而且<strong>私钥也一样</strong>！</p><p>如果你对TLS不熟悉，学到这里可能已经觉得有点“爆炸”了。先别急，下面有详细的解释。</p><p>这里的核心秘密在于：每次证书在更新的时候，<strong>它对应的私钥不是必须要更新的，而是可以保持不变的</strong>。</p><p>我们把本地的已经过期的中间证书，称为old_cert，新的中间证书称为new_cert。整个故事就是这样：</p><ul>\n<li>几年前，old_cert被根证书签发了出来，名称为inter-CA，并被保存在这台客户端的Trust store里。</li>\n<li>在2020年，old_cert到期，根证书机构重新签发了一张新的中间证书new_cert，它用了新的有效期，而<strong>证书名称inter-CA</strong>和对应的<strong>私钥</strong>都保持不变。</li>\n<li>CA用这张new_cert签发了这次的叶子证书。因为客户端程序没有打开证书校验机制，所以没有报错。</li>\n<li>这一天，新的代码发布上去，证书校验机制被打开了，于是客户端开始做校验。它发现这张叶子证书的签发者名称是inter-CA，而自己本地就有一张也叫inter-CA的证书，于是尝试用这张证书的公钥去解开叶子证书的签名部分，可以成功解开，于是确认old_cert就是对应的中间证书（而没有用收到的new_cert，这很关键）。但是由于old_cert已经过期了，结果客户端抛出certificate has expired的错误！</li>\n</ul><p>如果你还没有完全看明白，说明你真的在思考了，因为我确实还没有讲完。接下来介绍核心知识点：TLS证书签名。</p><h3>TLS证书签名</h3><p>你应该也知道，TLS证书都有签名部分，这个签名就是用签发者的私钥加密的。客户端为什么会相信叶子证书真的是这个CA签发的呢？就是因为，客户端的Trust store里就有这个CA的公钥（在CA证书里），它用这个公钥去尝试解开签名，能成功的话，就说明这张叶子证书确实是这个CA签发的。</p><p><img src="https://static001.geekbang.org/resource/image/97/dc/977f0b985d96d3cdc2a17d0b6a253fdc.jpg?wh=1842x862" alt=""></p><p>这里最关键的部分在于，新老中间证书用的私钥是同一把，所以这张叶子证书的签名部分，<strong>用老的中间证书的公钥也能解开</strong>，这就使得下图中的橙色的验证链条得以“打通”，不过，谁也没料到打通的是一条“死胡同”。</p><blockquote>\n<p>补充：PKI里有交叉签名的技术，就是新老根证书对同一个新的中间证书进行签名，但并不适用于这个案例。</p>\n</blockquote><p><img src="https://static001.geekbang.org/resource/image/66/f3/66bf55b67efb27ff084b9dec871acaf3.jpg?wh=1616x842" alt=""></p><p>OpenSSL报错的原因找到了，根据这个发现，我们也确认了Node.js的Trust store也存在同样的问题。我们把它的Trust store里的过期证书全部删除后，问题就被解决了。</p><p>另外在排查过程中，我们偶然发现Stack Overflow上也有人报告了类似的问题。于是<a href="https://stackoverflow.com/questions/24992976/openssl-telling-certificate-has-expired-when-it-has-not/68151948#68151948">我在Stack Overflow上也做了回复</a>，期望可以对遇到类似问题的人提供帮助。</p><blockquote>\n<p>补充：我的留言在三楼，署名VictorYang。</p>\n</blockquote><h2>小结</h2><p>这节课，我们通过两个典型案例，学习了TLS相关的知识，你可以重点关注和掌握以下知识点。</p><ul>\n<li><strong>加密算法的类型</strong></li>\n</ul><p>对称加密算法：加密和解密用同一个密钥，典型算法有AES、DES。</p><p>非对称加密算法：加密和解密用不同的密钥，典型的非对称加密算法有RSA、ECDSA。</p><ul>\n<li><strong>TLS基础</strong></li>\n</ul><p>TLS是先完成握手，然后进行加密通信。非对称算法用于交换随机数等信息，以便生成对称密钥；对称算法用于信息的加解密。</p><ul>\n<li><strong>Cipher Suite</strong></li>\n</ul><p>在握手阶段，TLS需要四类算法的参与，分别是：密钥交换算法、身份验证和签名算法、对称加密算法、消息完整性校验算法。这四类算法的组合，就形成了密码套件，英文叫Cipher Suite。这是TLS握手中的重要内容，我们的案例1就是因为无法协商出公用的密码套件，所以TLS握手失败了。</p><ul>\n<li><strong>TLS证书链</strong></li>\n</ul><p>TLS的信任是通过对证书链的验证：</p><center>\n<p><strong>信任根证书 -&gt; 信任中间证书 -&gt; 信任叶子证书</strong></p>\n</center><p>本地证书加上收到的证书，就形成了证书链，如果其中有问题，那么证书校验将会失败。我们的案例2，就是因为一些极端情况交织在一起，造成了信任链过期的问题，导致证书验证失败了。</p><ul>\n<li><strong>Trust store</strong></li>\n</ul><p>它是客户端使用的本地CA证书存储，其中的文件过期的话可能导致一些问题，在排查时可以重点关注。</p><ul>\n<li><strong>排查技巧</strong></li>\n</ul><p>在排查技巧方面，你要知道使用 <strong>curl命令</strong>，检查HTTPS交互过程的方法：</p><pre><code class="language-clojure">curl -vk https://站点名\n</code></pre><p>以及使用 <strong>OpenSSL命令</strong>来检查证书的方法，也就是：</p><pre><code class="language-clojure">openssl s_client -tlsextdebug -showcerts -connect 站点名:443 \n</code></pre><p>另外在需要分析OpenSSL为什么报错的时候，你可以在前面加上 <strong>strace</strong>，这对于排查根因有不少的帮助。</p><p>然后，我也带你学习了<strong>如何在Wireshark里导出Cipher Suite的方法</strong>，就是在TLS详情中选中Cipher Suite，右单击，选中Copy，在次级菜单中选中All Visible Selected Tree Items。这时，列表就被复制出来了。</p><p>除此之外，我们还在排查TLS Alert 40这个信息时，通过查阅<a href="https://datatracker.ietf.org/doc/html/rfc5246">RFC5246</a>得到了答案。所以，在遇到一些协议类型、定义相关的问题时，<strong>最好查阅权威的RFC文档，这样可以获得最准确的信息</strong>。</p><h2>思考题</h2><p>最后还是给你留两道思考题：</p><ul>\n<li>我们知道TCP是三次握手，那么TLS握手是几次呢？</li>\n<li>假设服务端返回的证书链是根证书+中间证书+叶子证书，客户端没有这个根证书，但是有这个中间证书。你认为客户端会信任这个证书链吗？</li>\n</ul><p>欢迎在留言区分享你的答案，也欢迎你把今天的内容分享给更多的朋友。</p>',
        article_title: "19 | TLS的各种特性：TLS握手为什么会失败？",
      },
      {
        title: "20 | TLS加解密：如何解密HTTPS流量？",
        id: 492427,
        content:
          '<p>你好，我是胜辉。</p><p>在上节课里，我们对TLS的整体的知识体系做了总览性的介绍，然后回顾了两个实际的案例，从中领略了TLS握手的奥妙。我们也知道了，TLS握手的信息量还是很大的，稍有差池就可能引发问题。我们只有对这些知识有深刻的理解，才能更准确地展开排查。</p><p>不过，也正因为这种种严苛的条件，TLS才足够安全，因为满足了这些前提条件后，真正的数据传送就令人十分放心了。除非你能调动超级计算机或者拥有三体人的智慧，要不然一个TLS连接里面的加密数据，你是真的没有办法破解的。</p><p>可话说回来，<strong>如果排查工作确实需要我们解开密文，查看应用层信息，那我们又该如何做到呢？</strong></p><p>所以在这节课里，我会带你学习TLS解密的技术要点，以及背后的技术原理，最后进行实战演练，让加密不再神秘。好了，让我们开始吧。</p><h2>TLS加密原理</h2><p>在上节课里我们已经了解到，TLS是结合了对称加密和非对称加密这两大类算法的优点，而密码套件是四种主要加密算法的组合。那么这些概念，跟我们的日常工作又有着什么样的交集呢？</p><h3>解读TLS证书</h3><p>下面这个证书，是我在访问站点<a href="https://sharkfesteurope.wireshark.org">https://sharkfesteurope.wireshark.org</a>的时候获取到的，我们来仔细读一下这里面的内容，看看哪些是跟我们学过的TLS知识相关的。</p><!-- [[[read_end]]] --><p>我把图中的很多关键信息做了标记，希望可以帮助你更好地理解。</p><p><img src="https://static001.geekbang.org/resource/image/cb/cf/cbb41d50695d62f090ec5804b6728bcf.jpg?wh=476x762" alt=""></p><p>从上到下，我们了解了这张证书所在的证书链，然后是证书名称、身份验证和签名算法、有效期。不过，看完这个证书，你可能也发现了一个小问题：站点名称跟证书名称不一致？这两个不匹配，浏览器为啥不报错呢？</p><p><img src="https://static001.geekbang.org/resource/image/e4/84/e4d8b4c388b7599a1f765b8b951abe84.jpg?wh=1684x734" alt="图片"></p><p>其实，这里的站点名称跟证书实际上是匹配的，但它匹配的不是Common Name，而是另外一个概念：SAN。</p><p>TLS证书为了支持更多的域名，设计了一个扩展选项Subject Alternative Name，简称 <strong>SAN</strong>，它就包含有多个域名。比如还是这张证书，它的SAN里的域名里就有wireshark.org、sni.cloudflaressl.com，还有跟这次访问的站点名直接相关的*.wireshark.org。这个是通配符域名，就意味着sharkfesteurope.wireshark.org也被支持了。SAN列表如下：</p><p><img src="https://static001.geekbang.org/resource/image/c8/dd/c84e5165254173fccb30e56c2dddb7dd.jpg?wh=964x1510" alt="图片"></p><p>这里也有一个小的注意点：通配符证书只能支持一级域名，比如*.wireshark.org证书可以支持以下域名：</p><ul>\n<li>a.wireshark.org</li>\n<li>b.wireshark.org</li>\n</ul><p>但不支持这样的域名：</p><ul>\n<li>a.b.wireshark.org</li>\n<li>a.b.c.wireshark.org</li>\n</ul><p>然后我们再来温习一下<strong>密码套件</strong>。在这张证书里，我们能看出它用到的密码套件是什么了吗？下面我们来解读一下。</p><p><strong>密钥交换算法是什么呢？</strong>这在证书里看不出来，需要根据握手协商的结果来判定。不过，我们也可以有个初步的判断。如果这次通信用的TLS版本是1.3，那么就是DHE或者ECDHE这样的“前向加密”的密钥交换算法了。结尾的E是Ephemeral，意思是“短时间的”，也就是密钥是每次会话临时生成的。</p><blockquote>\n<p>补充：稍后我会介绍什么是前向加密。</p>\n</blockquote><p><strong>身份验证和签名算法呢？</strong>就是证书里明确写着的ECDSA，其中EC就是Elliptic Curve的缩写，也就是椭圆曲线算法，它可以用更短的密钥达到跟RSA同样的密码强度。后面跟着的SHA-256是哈希摘要算法，证书内容用这个SHA-256算法做了哈希摘要，然后用ECDSA算法对摘要值做了签名，这样的话，客户端就可以验证这张证书的内容有没有被篡改了。</p><p><img src="https://static001.geekbang.org/resource/image/8b/e6/8b0df16302e77710e097abe770978fe6.jpg?wh=432x254" alt="图片"></p><p><strong>对称加密算法又是什么呢？</strong>在证书这里看不出来，因为它也是通过握手协商出来的。当然，用OpenSSL或者curl命令就可以观察到，我们稍后演示。</p><p><strong>最后是完整性校验算法了。</strong>其实在2里面已经提过了，是SHA-256。</p><p>我们用OpenSSL命令，可以直接观测到这次TLS里协商出来的密码套件：</p><pre><code class="language-bash">$ openssl s_client -connect sharkfesteurope.wireshark.org:443\n......\nNew, TLSv1.3, Cipher is TLS_AES_256_GCM_SHA384\n......\n</code></pre><p>原来，这里用的就是最新的TLS1.3版本，密码套件是TLS_AES_256_GCM_SHA384。你有没有发现它跟TLS1.2的那些密码套件相比还有一个区别呢？比如跟下面这个TLS1.2的密码套件比较一下：</p><center>\n<p><strong>TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA</strong></p>\n</center><p>相比之下，TLS1.3的套件<strong>TLS_AES_256_GCM_SHA384</strong>是不是少了两个算法：身份验证和签名算法，还有密钥交换算法。不过，身份验证和签名算法倒是可以从证书里看到，它是ECDSA。那密钥交换算法又到哪里去了呢？</p><p><img src="https://static001.geekbang.org/resource/image/d3/b7/d39bcc2b12f573c5866ed5198491aab7.jpg?wh=2000x844" alt=""></p><p>其实，这是因为TLS1.3只允许前向加密（PFS）的密钥交换算法了，所以使用静态密钥的RSA已经被排除了，它默认使用的是DHE和ECDHE，所以就不写在密码套件名称里了。</p><p>那么在这里，就又涉及了一个新的概念：前向加密。</p><h3>前向加密（PFS）</h3><p>前向加密又称为“完美前向加密”，它的英文就是Forward Secrecy和Perfect Forward Secrecy。</p><p>虽然我前面刚提到TLS1.3去掉了RSA，不过你不要误会了，TLS1.3只是把RSA从密钥交换算法中排除了，但证书签名算法还是可以用RSA的。</p><p><strong>为什么TLS1.3强制要求前向加密呢？</strong>这是因为，如果在密钥交换的时候用非前向加密的算法（比如RSA），那么一旦黑客取得了服务端私钥，并且抓取了历史上的TLS密文，他就可以用这个私钥和抓包文件，把这些TLS会话的对称密钥给还原出来，从而破解所有这些密文。因为可以把之前的密文都破解，RSA就不属于“前向”加密。</p><p>人们发现，要解决这个问题的关键，就要做到：<strong>每次参与协商对称密钥时的非对称密钥都不一样</strong>。这样的话，即使黑客破解了其中一次会话的密钥，也无法用这个密钥破解其他会话。</p><p>我们可以用一个例子来帮助理解“前向加密”。假设我们不断地生成一个个的保险箱，相当于一个个的TLS加密报文，如果每个箱子用同样的锁，那么一旦其中一把锁被破解，所有的保险箱都可以被打开了。用上“前向加密”锁之后，每次新的保险箱都用不同的锁，那么即使一把锁被破解，损失的只是一个保险箱，其他的箱子依旧安全。</p><h2>TLS的软件实现</h2><p>TLS只是一套协议，主要是“动动嘴皮子”，具体的活当然还是代码来干。目前应用最为广泛的SSL/TLS实现可能就是OpenSSL了，它既是一个开发库，也是一个命令行工具的名称。另外，NSS和GnuTLS也是开源的TLS实现。应用程序会基于这些TLS库来实现TLS加解密功能。</p><p><img src="https://static001.geekbang.org/resource/image/04/ec/0457ed7f7d9a730b03393ee834526aec.jpg?wh=1548x681" alt=""></p><p>有没有觉得这个很像OSI的分层模型？业务代码工作在应用层，TLS库工作在表示层和会话层，两层之间有交互也有解耦，起到了很好的协同的效果。</p><p><img src="https://static001.geekbang.org/resource/image/5d/e6/5d42c523686ffa6f5edfba03384f3fe6.jpg?wh=2000x980" alt=""></p><p>学习完TLS加密原理，我们就要进入动手环节了，也就是期待已久的TLS抓包解密，让秘密不再是秘密。</p><h2>客户端如何做TLS解密？</h2><p>这里说的客户端，包括了Chrome、Firefox等浏览器，也包括curl这样的命令行工具。我在上节课里提过，为了把TLS解密，我们需要完成几个前提条件。其实这些前提条件就是下面这三件事：</p><ul>\n<li>创建一个用来存放key信息的日志文件，然后在系统里<strong>配置一个环境变量SSLKEYLOGFILE</strong>，它的值就是这个文件的路径。</li>\n<li><strong>重启浏览器</strong>，启动抓包程序，然后访问HTTPS站点，此时TLS密钥信息将会导出到这个日志文件，而加密报文也会随着抓包，被保存到抓包文件中。</li>\n</ul><blockquote>\n<p>补充：如果是Mac又不想改动全局配置，那么你可以在terminal中的 <code>export SSLKEYLOGFILE=</code>路径，然后执行 <code>open "/Applications/Google\\ Chrome.app"</code>，这时Chrome就继承了这个shell父进程的环境变量，而terminal退出后，这个环境变量就自动卸除了。</p>\n</blockquote><ul>\n<li>在Wireshark里，打开Preferences菜单，在Protocol列表里找到TLS，然后把<strong>(Pre)-Master-Secret log filename配置为那个文件的路径</strong>。</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/0b/b8/0be2c0e7c0e6a9b2949634a24a6758b8.jpg?wh=1830x1004" alt="图片"></p><p>在做完这三件事之后，我们用Wireshark打开抓包文件，就能看到解密后的报文了，比如HTTP请求和响应，还有TLS的控制信息，都会展示为明文。</p><p>比如，在默认情况下，我们看到的会是密文：</p><p><img src="https://static001.geekbang.org/resource/image/ed/87/ed692a27ba9f3471821663abe0498187.jpg?wh=1119x299" alt="图片"></p><blockquote>\n<p>补充：抓包示例文件已经上传至<a href="https://gitee.com/steelvictor/network-analysis/blob/master/20/lesson20.pcap">Gitee</a>，建议结合抓包文件和文稿一起学习。</p>\n</blockquote><p>而配置解密步骤之后，看到的就是明文了：</p><p><img src="https://static001.geekbang.org/resource/image/fb/3c/fb4a1972d1786034c5735404950f2a3c.jpg?wh=1117x299" alt="图片"></p><p>好了，你可以看到明文了，很多应用层的信息都可以辅助你做排查了，是不是有点小小的激动？</p><p>那么这背后的原理是什么呢？</p><p>其实是这样的：浏览器在启动过程中会尝试读取SSLKEYLOGFILE这个环境变量。如果存在这个变量，而它指向的又是一个有效的文件，那么浏览器就会做最为关键的事情了：它去调用TLS库，让TLS库把访问HTTPS站点的过程中的key信息导出到SSLKEYLOGFILE文件中。我画了一张示意图供你参考：</p><p><img src="https://static001.geekbang.org/resource/image/64/6d/649e9001c4391ddec54e93fb9784bb6d.jpg?wh=2000x671" alt=""></p><p>整个过程倒是不难理解，不过你可能会好奇：为什么这个日志文件有这么强大的能力，能解密TLS？然后又不免担心，<strong>如果这个文件“被坏人利用了”，该怎么办？</strong></p><p>所以我们还需要近距离地认识一下SSLKEYLOGFILE。</p><h3>SSLKEYLOGFILE</h3><p>这个文件之所以能够解密TLS，最关键的是，TLS库把密钥交换阶段的核心信息Master secret导出到了这个文件中。基于这个信息，Wireshark就可以还原出当时的对称密钥，从而破解密文。</p><p>我们先来认识一下SSLKEYLOGFILE的格式。它是由很多条记录组成的，对于TLS1.2来说，每一行是一条记录，每一条记录由3个部分组成，中间用空格分隔，也就是下面这样：</p><ul>\n<li><code>&lt;Label1&gt; &lt;ClientRandom1&gt; &lt;Secret1&gt;</code></li>\n<li><code>&lt;Label2&gt; &lt;ClientRandom2&gt; &lt;Secret2&gt;</code></li>\n<li>……</li>\n</ul><p>这三个部分的具体含义是这样的。</p><p><strong>Label：是对这条记录的描述，对于TLS1.2来说，这个值一般是CLIENT_RANDOM</strong>。另外，RSA也是一个可能的值，但是前面说过，因为RSA算法在密钥交换方面不是前向加密的，所以已经不推荐使用了。所以如果你在日志文件里看到RSA，可能要小心一点，说明你的TLS不是前向加密的，所以并不是很安全。</p><p><strong>ClientRandom：这个部分就是客户端生成的随机数</strong>，随机数原始长度为32字节，但在这里是用16进制表示的，每个字节用16进制表示就会成为2个字符，所以就变成了64个字符的字符串。我们在抓包文件里也能看到它，因为在密钥交换算法的设计中，ClientRandom就是要在网络上公开传输的。</p><p><strong>Secret：这就是Master secret，也就是通过它可以生成对称密钥</strong>。Master secret固定是48字节，也是十六进制表示的关系，成为96个字节的字符串。你应该明白了，这个Master secret就是最为关键的信息了，也正是黑客苦苦寻求的东西。它是万万不能在网络上传输的，自然也不可能在抓包文件里看到它，只有TLS库才能导出它。</p><blockquote>\n<p>补充：TLS1.3的格式会很不一样，具体细节你可以参考这里的<a href="https://firefox-source-docs.mozilla.org/security/nss/legacy/key_log_format/index.html">链接</a>。</p>\n</blockquote><p>我们来看一个TLS1.2的KEYLOGFILE的具体的例子：</p><pre><code class="language-bash">CLIENT_RANDOM 770c2c73ef1ab58dda9360a94587e5f8b0a80c0b1abf628ddd7b55a118ec18ec bea2c01c5b6f9c577e8ba251c8f262adf33c5aa31a238d464a9c56dbd1bf30cf55cbf14e6175102fa1db9b8a0183a721\n</code></pre><blockquote>\n<p>补充：这个日志文件也已经上传至<a href="https://gitee.com/steelvictor/network-analysis/blob/master/20/lesson20sslkey.log">Gitee</a>，你可以按照前面介绍的3个步骤，结合上面的抓包文件和这个日志文件，自己来观察解密前后的区别。</p>\n</blockquote><p>在输出这个key信息的时候，我们也做了对应的抓包，现在看一下抓包文件。我们选中Client Hello报文，点开TLS详情部分，继续点开TLSv1.2 Record Layer -&gt; Handshake Protocol -&gt; Random，你看看它是不是就是前面日志文件里，第二列的值（开头是770c）？</p><p><img src="https://static001.geekbang.org/resource/image/86/a8/86979e6ba6087a4cf6a13e46fdb675a8.jpg?wh=863x359" alt="图片"></p><p>SSLKEYLOGFILE日志文件格式我们了解了，接下来了解Wireshark是怎么跟它协同工作，解开密文的。</p><h2>Wireshark是怎么解开密文的？</h2><p>在TLS1.2的SSLKEYLOGFILE中，每条记录的第一列是CLIENT_RANDOM这个字符串，第二列是这个client random的值，Wireshark就是通过它，找到对应的TLS会话（你可以理解为是TCP流）。就像上图所示，通过这个随机数，就找到了这条KEYLOG记录对应的TLS会话了。</p><p><img src="https://static001.geekbang.org/resource/image/a8/57/a89a94c72aca54686fb495d861b4c257.jpg?wh=1234x454" alt="图片"></p><p>那么接下来，Wireshark就知道真正的Master secret在哪里了：它就是前面匹配了这个客户端随机数的记录的第三列，也就是那96个字节的字符串。</p><p><img src="https://static001.geekbang.org/resource/image/44/df/443c085877yy55b4891760eeba5b3fdf.jpg?wh=1242x52" alt="图片"></p><p>由于在抓包文件里就有ECDHE密钥交换算法所需要的各种参数，结合这里的Master secret，Wireshark就可以解析出对称密钥，从而把密文解密了！</p><p><img src="https://static001.geekbang.org/resource/image/a6/9d/a6d8303e04ae39e8aa465c40e4852b9d.jpg?wh=1241x434" alt="图片"></p><h3>SYSKEYLOGFILE的安全性</h3><p>现在回答一下前面的问题：如果这个文件“被坏人利用了”，怎么办？</p><p>通过前面的学习，我们知道了：要想破解密文，既要有抓包文件，也要有SSLKEYLOGFILE日志文件，两者结合才能解密。而且不要忘了，即使你有抓包文件和日志文件，要是没抓到TLS握手阶段的报文，也还是不能解密，因为缺少了客户端随机数、加密算法参数等信息，Master secret对你也是无法下嘴的美食。</p><h2>服务端如何做TLS解密？</h2><p>其实，上面的客户端做解密的过程，网络上已经有很多资料了，但是接下来要介绍的“服务端做TLS解密”这个话题，却鲜有人讨论。要知道，TLS是双方的加密任务，但是我们一边倒地关心客户端如何解密，却对服务端的解密不闻不问，这又是什么道理呢？</p><p>我觉得可能有这么几个原因：</p><ul>\n<li>多数人接触的还是以客户端为主，能在客户端解密已经满足了大多数的需求。而服务端只有一部分专职运维或者开发工程师在维护，关注度少了很多。</li>\n<li>服务端一般有详细的日志，比如Nginx可以向日志里输出HTTP头部和性能数据，还可以输出TLS选择的密码套件等信息，这些在一般场景下也够用了。</li>\n<li>很多服务端程序并没有提供TLS解密的功能，也就是想做抓包解密也做不了。而要自己实现这个特性，难度跟简单的参数配置，不在一个等级。难度高，可能是更加关键的原因。</li>\n</ul><p>其实，在软件架构上，服务端和客户端也是类似的，也是基于TLS库来构建TLS加解密的能力的。</p><p><img src="https://static001.geekbang.org/resource/image/c9/5f/c94f7c2d6a59455ec03792288793a55f.jpg?wh=1591x673" alt=""></p><p>就我们eBay的情况来说，在我们的软件负载均衡方案中，第七层的部分是基于<a href="https://www.envoyproxy.io/docs/envoy/latest/intro/what_is_envoy">Envoy</a>实现的。我们在把一套新系统搬上生产环境之前有一系列的考量，就像体检的检查表一样逐一校验。我们发现Envoy的体检就有一项“不合格”：Envoy不提供对TLS流量进行抓包解密的功能。</p><blockquote>\n<p>补充：Envoy是硅谷的共享出行公司Lyft于2016年发起的开源项目，可以认为是云原生时代的Nginx。</p>\n</blockquote><p>在不少情况下，这个抓包解密特性对排查很有用。像一些商业产品比如Netscaler就是可以一边抓包，一边导出TLS key。就跟我们在客户端做解密类似，我们结合这两个文件，就可以在Wireshark中既观察到TCP行为，也读到应用层信息了。</p><p>你可能会问：“Envoy可以输出Web日志呀，把各种HTTP性能指标、访问头部，甚至包括用了什么Cipher，都输出到文件，这样还不香吗？”</p><p>其实，我在开篇词和<a href="https://time.geekbang.org/column/article/480068">第4讲</a>中都提到过网络排查的两大鸿沟。</p><ul>\n<li><strong>应用现象跟网络现象之间的鸿沟</strong>：你可能看得懂应用层的日志，但是不知道网络上具体发生了什么。</li>\n<li><strong>工具提示跟协议理解之间的鸿沟</strong>：你看得懂 Wireshark、tcpdump 这类工具的输出信息的含义，但就是无法真正地把它们跟你对协议的理解对应起来。</li>\n</ul><p>而包括Envoy在内的反向代理和LB软件，虽然也都提供了应用层日志，但跟实际的网络行为还有距离，这就是“应用现象跟网络现象之间的鸿沟”：日志是日志，网络是网络。如果日志里说某个HTTP请求耗时很长，你是无法知道网络上到底什么问题导致了这么长的耗时，是丢包引起了重传？还是没有丢包，纯粹是传输速度慢呢？</p><p>为了跨越第一个鸿沟，我们选择做tcpdump抓包。但是，如果抓取到的TLS密文无法被解密，就无法知道这些究竟是应用层的什么信息，这个鸿沟依然没有被跨越。</p><p><img src="https://static001.geekbang.org/resource/image/2d/9f/2d86df6591f65bba7425d10b7426819f.jpg?wh=1828x1044" alt=""></p><p>当然，我们可以选择“妥协”，采用一些灵活的策略来开展抓包分析。比如，可以把抓包分析的重心转移到TCP和TLS本身的层面，而不再关心其承载的应用层信息。但是“隔靴搔痒”总让排查工作不是特别“痛快”，我们犹豫许久之后，还是决定把这个解密的特性实现！</p><p>这并不是一件容易的事情。不过通过调研，我们发现，其实在服务端启用跟客户端类似的TLS解密功能，技术上是可行的，其中最为关键的信息就在2017年一位Wireshark开发工程师的<a href="https://sharkfesteurope.wireshark.org/assets/presentations17eu/15.pdf">演讲</a>中：</p><blockquote>\n<p>Applications using OpenSSL 1.1.1 or BoringSSL d28f59c27bac (2015-11-19) can be configured to dump keys: void SSLCTXsetkeylogcallback(SSL CTX ∗ctx, void (∗cb)(const SSL ∗ssl, const char ∗line));</p>\n</blockquote><p>也就是说，只要我们使用这个BoringSSL（是谷歌Fork自OpenSSL的项目）的 <strong>SSLCTXsetkeylogcallback()</strong> 回调函数，就可以把TLS信息导出来，于是我们信心大增。核心突破就是要把这个BoringSSL的回调函数给用起来。</p><p>具体来说，我们需要做这样的几件事：</p><ul>\n<li>在Envoy代码中增加调用SSLCTXsetkeylogcallback()函数的逻辑。</li>\n<li>增加了对外的接口，使得用户可以通过某种方式让Envoy知道，它需要去使用这个调用逻辑。</li>\n</ul><p>第二点，其实就是一种接口方式，比如SSLKEYLOGFILE环境变量就是一种，我们也可以选择API接口，或者某种别的接口。总之，只要让程序（这里是Envoy）知道：它需要去叫BoringSSL这个小弟去办点事情，整个功能就可以运作起来了。你可以参考这张示意图：</p><p><img src="https://static001.geekbang.org/resource/image/95/82/950d019fayye428c046c3d4504aa9082.jpg?wh=1707x785" alt=""></p><p>“体检通过”！我们在服务端Envoy上也可以做到方便的TLS解密了。其实，不仅实现了这个具体的需求本身，也实现了我们作为技术工作者，对“自我实现”的需求。</p><blockquote>\n<p>这个特性的主要开发者张博已经提交了PR，相信不久之后我们就能在正式版的Envoy里用上这个特性了。</p>\n</blockquote><p>实现Envoy的TLS抓包解密的具体的做法跟客户端解密的步骤差不多：</p><ul>\n<li>调用Envoy接口，启用SSL KEY导出功能。</li>\n<li>做tcpdump抓包，然后把抓包文件和KEY文件复制出来。</li>\n<li>在Wireshark里同样配置好<strong>TLS协议的(Pre)-Master-Secret log filename</strong>，打开抓包文件后，就可以跟在客户端类似，直接看到明文了。</li>\n</ul><h2>几个问题</h2><p>然后到这里，这里我们还需要搞清楚几个问题。</p><h4>问题1：我想实时查看解密信息行不行？</h4><p>一个字的答案：行。只要设置好前面提及的3件事：</p><ul>\n<li>SSLKEYLOGFILE环境变量；</li>\n<li>之后再启动浏览器，然后直接在Wireshark里开始抓包；</li>\n<li>设置Wireshark的TLS协议，配置(Pre-)Master secret logfile。</li>\n</ul><p>这时候你访问HTTPS站点时，在Wireshark里看到的就直接是解密好的信息了！因为Wireshark已经能从SSLKEYLOGFILE里读取到密钥信息，同时又在实时地抓取到TLS密文，这种解密工作是可以实时进行的。</p><p>当然，这里还有一个小的注意点，我们在第二个问题里展开。</p><h4>问题2：为什么停止抓包后再启动抓包，抓包文件又变成密文了？</h4><p>有同学就遇到这个问题：重启浏览器后，在Wireshark里马上就能看到HTTP数据包，确实能解密。但是停止抓包之后，再启动抓包，看到的又变成了TLS密文了。必须得重启浏览器才行。这是为何呢？</p><p>表面上看，这似乎又是一个“重启大法”的问题，但本质上呢？</p><p>我们知道，密文是用对称密钥加密的。而对称密钥的生成，是在TLS握手阶段完成的。我们前面提到过，Wireshark（也包括其他需要读取SSLKEYLOGFILE的程序）正是根据第二列的客户端随机数，来找到抓包文件中的TLS session，然后运用第三列的Master secret来获取到对称密钥的。</p><p>抓包停止后，新的HTTPS请求所触发的TLS握手就不会被抓取到。这也就意味着，Wireshark没有抓取到客户端随机数这个关键信息，尽管SSLKEYLOGFILE里依然在输出着一行行的key信息，但是Wireshark已经不知道用哪个Master secret了。自然，解密就无从做起。</p><p><img src="https://static001.geekbang.org/resource/image/50/50/50dbcec8fe9a381d06b6b50a8c72fd50.jpg?wh=2000x478" alt=""></p><p>而在浏览器重启后，事实上造成了TLS的重新握手，此时就又可以抓取到客户端随机数了，这样，解密工作就可以恢复。你看，这其实跟<a href="https://time.geekbang.org/column/article/479163">第3讲</a>中，没抓到TCP握手报文就无法知道Window Scale参数这个问题差不多，也是关于握手的，只不过这次是TLS握手。“技术是相通的”，这句话真不是随便说说。</p><h2>小结</h2><p>这节课，我们通过对一张真实的TLS证书的解读，复习了各个加密算法在现实场景中的实现。你也需要重点掌握以下知识点：</p><ul>\n<li>证书中的SAN列表包括了它所支持的站点域名，所以只要被访问的站点名称在这个列表里，名称匹配就不是问题了。</li>\n<li>证书中的域名通配符只支持一级域名，而不支持二级或者更多级的域名。</li>\n<li>在TLS1.3中，密钥交换算法被强制要求是前向加密算法，所以默认采用DHE和ECDHE，而RSA已经弃用。</li>\n<li>RSA依然可以作为可靠的身份验证和签名算法来使用。另外一种验证和签名算法是ECDSA，它可以用更短的密钥实现跟RSA同样的密码强度。</li>\n<li>前向加密可以防止黑客破解发生在过去的加密流量，提供了更好的安全性。</li>\n</ul><p>之后就是这节课的核心了：<strong>如何做到对抓包文件进行解密</strong>。这里又分客户端和服务端两个不同场景，你也需要重点关注。</p><p>首先，在客户端做抓包解密，需要做三件事：</p><ul>\n<li>创建一个文件，并设置为SSLKEYLOGFILE这个环境变量的值；</li>\n<li>重启浏览器，开始做抓包，此时key信息被浏览器自动导入到日志文件；</li>\n<li>在Wireshark里把该日志文件配置为TLS的(Pre)-Mater-Secret log filename。</li>\n</ul><p>这样，我们就能在Wireshark里直接读取到应用层信息了。</p><p>而在服务端抓包解密，就要依托于软件实现了，但是有些软件并没有提供这种功能，比如Envoy。借助底层BoringSSL库的接口，eBay流量管理团队实现了对这个接口的调用，我们也可以在Envoy上完成抓包解密了。</p><p>另外，你还要知道<strong>Wireshark能解读出密文的原理</strong>：</p><ul>\n<li>从抓包文件中定位到client random；</li>\n<li>从日志文件中找到同样这个client random，然后找到紧跟着的Master secret；</li>\n<li>用这个Master secret导出对称密钥，最后把密文解密。</li>\n</ul><h2>思考题</h2><p>最后，给你留两道思考题：</p><ul>\n<li>DH、DHE、ECDHE，这三者的联系和区别是什么呢？</li>\n<li>浏览器会根据SSLKEYLOGFILE这个环境变量，把key信息导出到相应的文件，那么curl也会读取这个变量并导出key信息吗？</li>\n</ul><p>欢迎你把答案分享到留言区，我们一起交流、进步。</p>',
        article_title: "20 | TLS加解密：如何解密HTTPS流量？",
      },
      {
        title: "答疑（四）| 第16~20讲思考题答案",
        id: 496875,
        content:
          '<p>你好，我是胜辉。</p><p>今天咱们的思考题解答就要进入到第16~20讲了。在第16到18讲里，我们通过几个应用层的案例，回顾了排查HTTP异常状态码的方法，也学习了偶发性问题的排查思路和技巧。在第19和20讲里，我们更是系统性地学习了TLS相关的知识，比如TLS密码套件、握手协商等细节，最后又对TLS解密这个敏感而实用的话题，进行了深入的探讨。</p><p>那么，结合了解密技能和应用层排查技能，相信你对应用层的网络难题的排查，也增加了不少把握。接下来，就让我们继续进入答题环节，先来看第16讲的思考题。</p><h2>16讲的答疑</h2><h3>思考题</h3><ol>\n<li>在 HTTP 请求里，我们用 Content-Length 表示了 HTTP 载荷，或者说 HTTP body 的长度，那有时候无法提前计算出这种长度，HTTP 是如何表示这种“动态”的长度呢？</li>\n<li>HTTP 请求的动词加 URL 部分，比如 GET /abc，它是属于 headers，还是属于 body，或者哪种都不属于，是独立的呢？</li>\n</ol><h3>答案</h3><p>第一个问题的核心知识点，是 <strong>HTTP的数据传输中，对于数据边界的约定</strong>。我们知道，在计算机世界中，数据就是由0和1表示的一连串的二进制数字，而这一连串数字的起始和结束的位置就很关键了。如果结束位置无法确定，那么显然根本无法正确读取到数据。</p><!-- [[[read_end]]] --><p>举个例子，如果我们从网络上收到一段数字01010101，而变量A在它的前部，假如A的长度为4个bit，那就是0101，十进制的值是5。假如A的长度为5个bit，那就是01010，它的十进制值就是10了，跟之前完全不同。</p><blockquote>\n<p>补充：为了简化讨论，这里略过了大小端字节序的问题。</p>\n</blockquote><p>现在网络的带宽很大，每秒钟都可能传送着几十上百兆的数据，所以宏观上看，网络数据的传输好像是大批量地“并行”进行的。但在微观尺度里，比如在纳秒这个时间尺度上看，数据依然是一个一个bit（0或者1）地进行发出和接收，依然是“顺序的”。而在网络的每一层，都有相应的头部字段规定了载荷的开始位置（offset）和长度（length）。有了这些信息，接收端就可以正确读取这些数据了。</p><p>我们可以具体到HTTP这一层来看，它的请求和响应报文，也是同样的：先头部（headers）后载荷（body）。那HTTP头部的结束位置在哪里呢？这个在<a href="https://time.geekbang.org/column/article/489700">第16讲</a>里我们就提到过，它不是用某个长度字段，而是<strong>用两个CRLF这样的字符定义了头部的结束位置</strong>。</p><p>而HTTP载荷的结束位置，又是如何定义的呢？其实有两种情况。一种就是最常见的，<strong>用Content-Length头部直接定义出载荷的长度</strong>，非常直接。接收端只要读取到这个长度的数据时，就知道已经读取到了完整数据了。</p><p><img src="https://static001.geekbang.org/resource/image/9f/38/9fe4b5ffdb3b9af4bca4b3321da8a738.jpg?wh=2000x921" alt=""></p><p>另外一种，就是<strong>针对“动态长度”的数据，HTTP用Content-Encoding: chunked这个头部，声明了块传输的方式</strong>。</p><p>这里说的数据为什么是“动态”的呢？是因为这些数据，无法在发送HTTP响应的开始阶段就确定下来，那么服务端就需要通过这种块传输的方式，一边计算出一部分数据，一边发送这些数据块（也就是chunk），这就是所谓“动态”了。也正是通过这种方式，解决了“既要传输数据，又无法在传输开始时就知道数据大小”的矛盾。</p><p>不过，chunk传输源源不断，那客户端怎么知道哪个chunk是结束呢？其实，每个chunk的开头的信息就是这个块的长度值，而<strong>如果这个长度是0，就表示这是最后一个chunk了，这也就是结束的位置</strong>。示意图如下：</p><p><img src="https://static001.geekbang.org/resource/image/3b/ab/3b341e721d56064e37e0yy4a971611ab.jpg?wh=2000x1125" alt=""></p><p>第二个问题可能是不少人忽视的一个知识点。我们知道，Server、Host等header就是明确的HTTP头部，而在这些header之前的“GET /abc HTTP/1.1”（也就是方法 + URL + 版本号），虽然形式上并不是像其他header那样的键值对，但它确实也属于HTTP头部。</p><p>其实这个知识点在这一讲的案例里就有体现。当时HAProxy把后端HTTP 400转义为前端HTTP 502，其原因就是在于：HAProxy的代码里，定义了HTTP头部大小不能超过8KB的逻辑，而GET URL正是属于这8KB中的一部分。</p><p><img src="https://static001.geekbang.org/resource/image/18/3c/1805be4b73d5dyy310cd8b2b4a377f3c.jpg?wh=2000x701" alt=""></p><h2>17讲的答疑</h2><h3>思考题</h3><ol>\n<li>如果 LB / 反向代理给客户端回复 HTTP 503，表示什么呢？如果 LB / 反向代理给客户端回复 HTTP 500，又表示什么呢？</li>\n<li>这节课里，我介绍了使用应用层的某些特殊信息，比如 uuid 来找到 LB 两侧的报文的对应关系。你有没有别的好方法也可以做到这一点呢？</li>\n</ol><h3>答案</h3><p>第一个问题，考查的是我们对HTTP 503跟500的不同语义的理解，以及这两个状态码跟LB的关系。我在课程里提过，<strong>LB是中间设备，它本身一般不会出现HTTP 500</strong>。如果后端有HTTP 500，LB就直接透传给客户端。而如果后端出现了服务不可用，比如LB到后端服务的健康检查失败，LB无法找到一个可用的后端服务器，那么LB就会回复HTTP 503给客户端。</p><p>也正是通过HTTP 503，LB表达了这层意思：“我自己是没问题的，但是后端服务器都处于不能干活的状态”。</p><p>类似的情况是HTTP 502，也就是LB后端的服务器返回了不合规的HTTP响应。你也可以再次复习一下课程里的这张图：</p><p><img src="https://static001.geekbang.org/resource/image/d0/y0/d0e50808d490yy0689379159d91dfyy0.jpg?wh=2000x588" alt=""></p><p>第二个问题也没有标准答案。其实只要在你的环境里有类似的id，能通过它区分不同的HTTP请求就可以了，无论它是叫uuid，还是traceid，或者是transaction id。然后各种contains过滤器，也就是我们在<a href="https://time.geekbang.org/column/article/495213">答疑二</a>里对<a href="https://time.geekbang.org/column/article/482610">07讲</a>做解答时候提到的这些：</p><ul>\n<li>应用层可以是 <code>http contains "abc"</code>；</li>\n<li>传输层可以是 <code>tcp contains "abc"</code>；</li>\n<li>网络层可以是 <code>ip contains "abc"</code>；</li>\n<li>数据链路层可以是 <code>frame contains "abc"</code>。</li>\n</ul><h2>18讲的答疑</h2><h3>思考题</h3><ol>\n<li>前面我介绍了使用 tshark 来找到耗时最高的 HTTP 事务的方法。关于 tshark，你自己还有哪些使用经验呢？</li>\n<li>在“是否还有其他可能？”这里，我提到了可能的重传。如果要验证是否真的存在这种重传，你觉得应该做什么呢？</li>\n</ol><h3>答案</h3><p>第一个问题，<strong>@Realm</strong> 同学做了很好的回答：</p><blockquote>\n<p><code>tshark -R "tcp.analysis.retransmission || tcp.analysis.out_of_order"</code> 通过 <code>-R</code> 指定过滤条件，抓重传和乱序的包。</p>\n</blockquote><p>tshark是随着Wireshark一起被安装到系统里的工具，它的各种过滤器等特性跟Wireshark是一样的，所以能在命令行里面做同样的分析工作。</p><blockquote>\n<p>补充：不过可能是版本的区别，如果你运行 <code>tshark -R "tcp.analysis.retransmission || tcp.analysis.out_of_order" -r file.pcap</code> 报错，把 <code>-R</code> 改为 <code>-Y</code> 就可以执行了，或者在 <code>-R</code> 前面加上-2。</p>\n</blockquote><p>像上面的tcp.analysis是一个很大的过滤器的集合。要查看tcp.analysis的各种分支，你可以这样做：</p><ul>\n<li><strong>在Wireshark的过滤器输入框里输入tcp.analysis</strong>，很多相关的过滤器就会被提示出来。</li>\n<li><strong>在命令行里执行tshark -G | grep tcp.analysis</strong>，一部分输出如下：</li>\n</ul><pre><code class="language-bash">$ tshark -G | grep tcp.analysis\nF\tSEQ/ACK analysis\ttcp.analysis\tFT_NONE\ttcp\t\t0x0\tThis frame has some of the TCP analysis shown\nF\tTCP Analysis Flags\ttcp.analysis.flags\tFT_NONE\ttcp\t\t0x0\tThis frame has some of the TCP analysis flags set\nF\tDuplicate ACK\ttcp.analysis.duplicate_ack\tFT_NONE\ttcp\t\t0x0\tThis is a duplicate ACK\nF\tDuplicate ACK #\ttcp.analysis.duplicate_ack_num\tFT_UINT32\ttcp\tBASE_DEC\t0x0\tThis is duplicate ACK number #\nF\tDuplicate to the ACK in frame\ttcp.analysis.duplicate_ack_frame\tFT_FRAMENUM\ttcp\t\t0x0\tThis is a duplicate to the ACK in frame #\nF\tThis is an ACK to the segment in frame\ttcp.analysis.acks_frame\tFT_FRAMENUM\ttcp\t\t0x0\tWhich previous segment is this an ACK for\nF\tBytes in flight\ttcp.analysis.bytes_in_flight\tFT_UINT32\ttcp\tBASE_DEC\t0x0\tHow many bytes are now in flight for this connection\nF\tBytes sent since last PSH flag\ttcp.analysis.push_bytes_sent\tFT_UINT32\ttcp\tBASE_DEC\t0x0\tHow many bytes have been sent since the last PSH flag\nF\tThe RTT to ACK the segment was\ttcp.analysis.ack_rtt\tFT_RELATIVE_TIME\ttcp\t\t0x0\tHow long time it took to ACK the segment (RTT)\nF\tiRTT\ttcp.analysis.initial_rtt\tFT_RELATIVE_TIME\ttcp\t\t0x0\tHow long it took for the SYN to ACK handshake (iRTT)\nF\tThe RTO for this segment was\ttcp.analysis.rto\tFT_RELATIVE_TIME\ttcp\t\t0x0\tHow long transmission was delayed before this segment was retransmitted (RTO)\nF\tRTO based on delta from frame\ttcp.analysis.rto_frame\tFT_FRAMENUM\ttcp\t\t0x0\tThis is the frame we measure the RTO from\nF\tThis frame is a (suspected) retransmission\ttcp.analysis.retransmission\tFT_NONE\ttcp\t\t0x0\nF\tThis frame is a (suspected) fast retransmission\ttcp.analysis.fast_retransmission\tFT_NONE\ttcp\t\t0x0\n.   .....\n</code></pre><p>tshark加上-G参数会输出所有可用的过滤器，数量多达20万个（你没看错）。这个大宝藏，值得我们去多多探索和挖掘一下。</p><p>至于第二个问题，要搞清楚服务端是否有重传，最直接的办法就是<strong>在服务端也做抓包</strong>。这样的话，有没有重传就一目了然了。当然，在客户端抓包的话，也经常能通过乱序等现象推导出对端发生了重传，但不能保证涵盖所有的重传情形。</p><h2>19讲的答疑</h2><h3>思考题</h3><ol>\n<li>我们知道 TCP 是三次握手，那么 TLS 握手是几次呢？</li>\n<li>假设服务端返回的证书链是根证书 + 中间证书 + 叶子证书，客户端没有这个根证书，但是有这个中间证书。你认为客户端会信任这个证书链吗？</li>\n</ol><h3>答案</h3><p>第一个问题是TLS握手的基本知识。不考虑TLS session复用或者TLS1.3的1RTT，甚至0RTT这种特殊情况，一般情况下TLS握手是四次。</p><ul>\n<li>第一次握手：客户端发送Client Hello。</li>\n<li>第二次握手：服务端发送Server Hello和Certificate等消息。</li>\n<li>第三次握手：客户端发送ClientKeyExchange和ChangeCipherSpec等消息。</li>\n<li>第四次握手：服务端发送ChangeCipherSec和Finished。</li>\n</ul><p>在RFC5246的<a href="https://datatracker.ietf.org/doc/html/rfc5246#section-7.3">Handshake Protocol Overview</a>的第35页，也有关于这个知识点的详述，我把其中的关键部分引用到这里，供你参考。</p><pre><code class="language-plain">      Client                                               Server\n\n      ClientHello                  --------&gt;\n                                                      ServerHello\n                                                     Certificate*\n                                               ServerKeyExchange*\n                                              CertificateRequest*\n                                   &lt;--------      ServerHelloDone\n      Certificate*\n      ClientKeyExchange\n      CertificateVerify*\n      [ChangeCipherSpec]\n      Finished                     --------&gt;\n                                               [ChangeCipherSpec]\n                                   &lt;--------             Finished\n      Application Data             &lt;-------&gt;     Application Data\n</code></pre><p>第二个问题是关于证书信任链的知识。我们知道，<strong>PKI的信任不是凭空来的，而是有一个起点，而这个起点就是客户端保存的根证书</strong>。有了它，一系列信任才能建立起来。而在这个问题里，由于客户端并没有这张根证书，而服务端返回的证书链又绑定了这张不被信任的根证书，那么信任链就无法建立了。</p><p>在这个问题里，客户端有这张中间证书却不能信任这个证书链，你可能对这一点感觉费解。其实这个题目里隐含了一个信息：<strong>这张中间证书是被两张根证书都做了签名的</strong>，否则服务端也不会把这个根证书放在证书链里一同返回。但是这个证书链强制把中间证书绑定到客户端不信任的根证书，就导致验证失败了。</p><p>为了帮助你理解，我们可以换一种场景：如果服务端返回的证书链中，只有这张中间证书和叶子证书，那么因为客户端可以把这张中间证书跟自己信任的根证书关联起来，就可以建立信任。</p><p>我们可以再看一下这两种情形下的对比示意图，就更能明白了：</p><p><img src="https://static001.geekbang.org/resource/image/2d/33/2dcd30a3cb452e269df3f156af2cb133.jpg?wh=2000x1125" alt=""></p><h2>20讲的答疑</h2><h3>思考题</h3><ol>\n<li>DH、DHE、ECDHE，这三者的联系和区别是什么呢？</li>\n<li>浏览器会根据 SSLKEYLOGFILE 这个环境变量，把 key 信息导出到相应的文件，那么 curl 也会读取这个变量并导出 key 信息吗？</li>\n</ol><h3>答案</h3><p>第一个问题，很多同学都答得很好了。我在这里引用一下<strong>@那时刻</strong>同学的回答：</p><blockquote>\n<p>DH 算法是非对称加密算法，因此它可以用于密钥交换，该算法的核心数学思想是离散对数。<br>\n&nbsp;<br>\n根据私钥生成的方式，DH 算法分为两种实现：<br>\n&nbsp;<br>\n第一种，static DH算法，这个算法实际已经被废弃了，因为它算法不具备前向安全性。<br>\n&nbsp;<br>\n第二种，DHE 算法，这是现在比较常用的，也就是让双方的私钥在每次密钥交换通信时，都是随机生成的、临时的。<br>\n&nbsp;<br>\n不过，DHE 算法由于计算性能不佳，需要做大量的乘法，所以为了提升 DHE 算法的性能，就出现了现在广泛用于密钥交换算法——ECDHE 算法。它是在 DHE 算法的基础上，利用ECC椭圆曲线特性，可以用更少的计算量计算出公钥，以及最终的会话密钥。</p>\n</blockquote><p>而第二个问题是一个实践性的问题，不少同学也去实证了。其实也花不了几分钟时间，就能收获一个不错的知识点，有没有觉得这个片刻也挺有意义呢。</p><p>好了，今天的答疑就到这里。这些答案，有没有在你的预期以内呢？如果还有新的问题，也欢迎在留言区提问，我们一起进步、成长。</p>',
        article_title: "答疑（四）| 第16~20讲思考题答案",
      },
      {
        title: "21 | 为什么用了负载均衡更加不均衡？",
        id: 493040,
        content:
          '<p>你好，我是胜辉。</p><p>咱们课程的第二个实战模块“应用层真实案例揭秘篇”已经进行到后半程了。前半程的四讲（15到18）都是围绕应用层特别是HTTP的相关问题展开排查。而在刚过去的两讲（19和20）里，我们又把TLS的知识和排查技巧学习了一遍。</p><p>基本上，无论是网络还是应用引发的问题，也无论是不加密的HTTP还是加密的HTTPS，你应该都已经掌握了一定的方法论和工具集，可以搞定不少问题了。</p><p>但是我们也要看到，现实世界里也有不少问题是混合型的，未必一定是跟网络有关。比如，你有没有遇到过类似下面这种问题：</p><ul>\n<li>ping正常，抓包看也没有丢包或者乱序现象，但是应用就是缓慢；</li>\n<li>Telnet端口能通，但应用层还是报错。</li>\n</ul><p>其实这也说明了，掌握网络排查技能固然重要，但完全脱离操作系统和架构体系方面的知识，仅根据网络知识去做排查，也有可能会面临知识不够用的窘境。所以，作为一个技术人，我们<strong>任何时候都不要限制自己的学习和成长的可能</strong>，掌握得越多，相当于手里的牌越多，我们就越可能搞定别人搞不定的问题。</p><p>所以接下来的两节课，我会集中<strong>围绕系统</strong>方面的案例展开分析，希望可以帮助你构造这方面的能力。等以后你遇到网络和系统扯不清的问题时，也不会发怵，而是可以准确定位，高效推进了。</p><!-- [[[read_end]]] --><h2>案例1：高负载和不均衡</h2><p>这也是我在公有云工作的时候处理的真实案例。当时一个客户是做垂直电商的，他们的体量还不大，所以并没有自建机房，而是放到公有云上运行。他们的架构大致是这样的：</p><p><img src="https://static001.geekbang.org/resource/image/b9/01/b9956d177cdd2034011bfb13f0d97e01.jpg?wh=2000x1125" alt=""></p><p>LB运行在第四层，设置的负载均衡策略是round robin（轮询），也就是新到达LB的请求，<strong>依次派发</strong>给后端3个Nginx服务器，使得每台机器获得的请求数相同。Nginx运行在第七层，它作为Web服务器接收HTTP请求，然后通过<a href="https://en.wikipedia.org/wiki/FastCGI">FastCGI</a>接口传递给本机的<a href="https://en.wikipedia.org/wiki/PHP#PHPFPM">php-fpm</a>进程，进行应用层面的处理。</p><p>应该说，这就是一个非常典型的负载均衡架构，平时运行也正常。不过有一天，客户忽然报告系统出问题了，网站访问越来越慢，甚至经常会抛出HTTP 504错误。如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/92/d9/92114f6352e90fd60f49a4c5f647f1d9.jpg?wh=1341x435" alt="图片"></p><p>我们借助浏览器开发者工具可以看到，这个响应耗时长达60秒，然后抛出了504错误。事实上，一般人浏览一个网站的时候，等个十来秒肯定就没耐心了，所以这次的问题确实很严重。</p><h3>初步检查</h3><p>这三台Nginx服务器的配置都是8核CPU。从服务端的监控来看，它们的系统负载，也就是CPU load出现了严重的不均衡。其中一台的负载值在7左右，一台在20左右，最后一台居然高达40左右，远远超过它们的CPU核的数量。</p><p>我们知道，<strong>uptime或者top命令输出里的CPU load值，表示的是待运行的任务队列的长度</strong>。比如8核的机器，load到8，那就是8CPU核处理8个任务，全部用满了。不过，能到用满的程度，实际已经对应用的性能产生明显的影响了，所以一般建议 <strong>load值不超过核数*0.7</strong>。也就是8核的机器，建议在load达到5.6的时候，就需要重点关注了。</p><p>回到这三台Nginx机器，我们看看具体的load。</p><p>Nginx 1的load大致在10到20之间：</p><p><img src="https://static001.geekbang.org/resource/image/87/24/87f416165a50b2bcdc208e79167c2f24.png?wh=429x94" alt="图片"></p><p>Nginx 2的load在7左右：</p><p><img src="https://static001.geekbang.org/resource/image/fc/74/fcc6ab2ac1a532e9bb471f9c65f3c374.png?wh=399x95" alt="图片"></p><p>Nginx 3就很高了，在40以上：</p><p><img src="https://static001.geekbang.org/resource/image/3b/dc/3ba65448537696061a24a0d908ec3ddc.png?wh=431x65" alt="图片"></p><p>联系前面我们提到的负载均衡，你大概也明白为什么客户找过来了：因为3台机器的负载不均衡啊，难道不是你们的LB工作不正常导致的吗？</p><p>这确实是一个形式上讲得通的逻辑，我们开工吧。</p><h3>排查LB</h3><p>首先需要检查网络状况。我们做了抓包分析，发现传输本身一切正常，没有丢包也没有特殊的延迟。</p><p>然后，我们需要确认LB工作是否正常。从直观上看，处于LB后端的3台机器的load不均，似乎也意味着LB分发请求时做得不均衡，要不然为啥后端这些机器的负载各不相同呢？</p><p><img src="https://static001.geekbang.org/resource/image/66/e3/669c1f73717b9d027e4f1c048c479ae3.jpg?wh=1595x909" alt=""></p><p>不过，这个load不均的问题，也可能只是表象，因为会影响到load的因素是比较多的。我们回到最初，既然要证明LB工作是否正常，最直接的方式，还是<strong>查证LB是否做到了round robin，也就是分发的请求数量是否均等</strong>。于是我们去查LB上的日志，看看这三台后端机器分得的请求数量。</p><p><img src="https://static001.geekbang.org/resource/image/71/c3/717817cfcbb61fb9f78552ec84cf37c3.jpg?wh=300x62" alt=""></p><p>以上就是LB上的统计，红框圈出来的那一列是请求数，3台Nginx对应这3行，可见请求数都是1364，这就说明LB的round robin机制运行正常。</p><p>接着，跟之前课程里的很多案例类似，我们做了一个简单的排除性测试：我们绕过LB，直接访问Nginx机器。结果发现：</p><ul>\n<li>load为7的那台机器，还勉强可以在10秒左右回复响应；</li>\n<li>另外两台机器（也就是load为20和40的机器）的响应要慢很多，超过了60秒。</li>\n</ul><blockquote>\n<p>补充：如果只使用load为7的那台机器，而禁用另外两台，行不行呢？当然是不行的，一台肯定撑不住这个流量，必须要三台一起服务。所以还是要把根因给找到并解决。</p>\n</blockquote><p>我们再来对比一下“通过LB”和“绕过LB”这两种场景下的问题现象：</p><ul>\n<li>通过LB，耗时60秒的时候收到HTTP 504。</li>\n<li>绕过LB，虽然收到了HTTP 200，但是耗时长达70多秒。</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/38/d5/38d479ec737cccd491fe62582383a7d5.jpg?wh=2000x902" alt=""></p><p>其实这两个场景的根本问题是一致的，都是后端服务器特别慢。这就再一次排除了LB本身的嫌疑。于是，问题就变成了“<strong>为什么机器的负载变高了？</strong>”</p><h3>排查主机</h3><p>那么，到了系统排查这一步，我们就没办法再用tcpdump结合Wireshark去排查了，因为问题跟网络报文没有关系。具体点说，我们要做下面这些事情：</p><ul>\n<li>分析所有进程，找到具体是哪个进程引起了load升高。</li>\n<li>分析进程细节，找到是什么Bug导致该进程变成了问题进程。</li>\n</ul><p>针对第一个问题，最常用的工具就是 <strong>top命令</strong>。通过top，我们很快就找到了消耗CPU资源最多的进程，发现是php-fpm进程。客户的电商程序框架本身是基于PHP开发的，所以需要PHP解释器来运行程序。又因为Nginx本身不能处理PHP，所以需要结合php-fpm才能正常工作。也就是下图这样：</p><p><img src="https://static001.geekbang.org/resource/image/cb/a3/cbb81908b9a50b951a22f1f6c7dc03a3.jpg?wh=1358x623" alt=""></p><p>既然确定了问题进程，接下来就是要排查进程导致load高的原因。排查这个问题，大致也有两个思路。</p><ul>\n<li><strong>白盒检查</strong>：检查代码本身，找到根因。</li>\n<li><strong>黑盒检查</strong>：不管代码怎么回事，我们从程序的外部表现来分析，寻求根因。</li>\n</ul><p>因为核心代码是客户的国外合作方维护的，客户自己并不十分清楚这里面所有的逻辑，所以白盒这条路，有点超出了他们的能力。那么自然的，我们要走第二条路。</p><h3>排查操作系统</h3><p>跟网络排查中有tcpdump这样强大的工具类似，进程的排查也有相关的强大工具，比如 <strong>strace</strong>。通过strace，我们可以把排查工作从进程级别，继续追查到更细的syscall（系统调用）级别。无论是系统调用读写文件时的问题，还是系统调用本身的问题，都可以在strace的帮助下现出原形。</p><p>不过，我这里需要先介绍一些系统调用相关的知识，方便你更好地理解strace。</p><h3>什么是系统调用？</h3><p>系统调用，英文叫system call，缩写是syscall。如果我们把操作系统视作一个巨大的应用程序，那么系统调用相当于什么呢？其实，就相当于 <strong>API</strong>。利用各种系统调用，应用程序就可以做到各种跟操作系统有关的任务了。</p><p>如果没有系统调用，让应用程序直接去操作文件系统、内存等资源会怎么样呢？那一定是一场灾难。我们可以想象一个场景：程序A往地址为100~300的内存段里写入数据，然后程序B往地址为200~400的内存段里写入数据，因为200~300这段内存被A和B所共用，就很容易产生错乱，这两个程序都可能因此而出错乃至崩溃。</p><p>所以，我们必须有一个“管理机构”来统筹安排，让所有的应用程序都向这个统一机构申请资源和办理业务。<strong>这个“管理机构”就是操作系统内核，而系统调用就是这个机构提供的“服务窗口”。</strong></p><h4>内核态和用户态</h4><p>实际上，从更底层的视角来说，操作系统必须要提供系统调用的另一个原因，是计算机体系结构本身的设计。为了避免前面例子里那种不合理的操作，以及让不同安全等级的程序使用不同权限的指令集，大部分现代CPU架构都做了保护环（Protection Ring）的设计。</p><p>比如，x86 CPU实现了4个级别的保护环，也就是ring 0到ring 3，其中ring 0权限最大，ring 3最小。就拿Linux来说，它的内核态就运行在ring 0，只有内核态可以操作CPU的所有指令。Linux的用户态运行在ring 3，它就没办法操作很多核心指令。</p><p><img src="https://static001.geekbang.org/resource/image/49/b1/4987376b66aa6ee0cf56a911952073b1.jpg?wh=1455x757" alt=""></p><p>那么处于ring 3的应用程序也想要运行ring 0的指令的话，该怎么办到呢？就是让内核去间接地帮忙。而这个“忙”，其实就是通过<strong>系统调用</strong>来“帮”的。</p><p>这样的话，用户空间程序就可以借系统调用之手，完成它原本没有权限完成的指令（进入内核态）。下面这张示意图就展示了用户空间、系统调用、内核空间这三者之间的关系：</p><p><img src="https://static001.geekbang.org/resource/image/fe/e4/fe8161ea9f07ec7c8b8ba87d412b47e4.jpg?wh=2000x760" alt=""></p><p>既然，系统调用要给用户空间的应用程序提供丰富而全面的接口，无论是文件和网络等IO操作，还是像申请内存等更加核心的操作，都需要通过系统调用来完成，那么系统调用的数量肯定是不少的。比如Linux的系统调用数量大约在300多个，有的操作系统则会达到500个以上。</p><h4>strace</h4><p>了解了系统调用，我们再来认识下strace。strace这个工具的s，指的就是sycall，所以strace就是对 <strong>s</strong>yscall的trace。通过这个命令，我们可以观测到一个进程访问的所有系统调用、给这些系统调用传入的参数，以及系统调用的输出。可想而知，这样充足的信息就给系统排查工作提供了极大的帮助。</p><p>你可以想象一下，没有strace的时候，你只是看到了程序的表象，也就是程序想让你看到的，你才能看到（比如通过标准输出或者日志文件）。而有了strace，程序的一举一动就全在你的视野里了，你就像有了火眼金睛，程序在明里暗里干的所有事情，都会被你知道。</p><p>夸奖了一番strace，我们来了解一下它的具体用法。strace的用法一般有两种。</p><p><strong>直接在命令之前加上strace</strong>。比如我们想知道curl www.baidu.com这个命令，在系统调用层面具体发生了什么，就可以执行strace curl www.baidu.com，然后就能看到前后的几十个系统调用，包括打开文件的openat()、关闭文件描述符的close()、建立TCP连接的connect()等等。</p><p><strong>执行strace -p PID</strong>。这样的话，你需要先找到进程的PID，然后执行这条指令来完成追踪。这比较适合对持续运行的服务（Daemon）进行追踪。比如，你可以先找到某个进程的进程号，然后执行strace -p PID，找到这个进程在系统调用方面的细节。当然，你还可以加上各种其他参数，来达到不同的追踪效果。</p><h3>使用strace</h3><p>好了，聊完了strace的强大功能，接下来看它的表现。我们用strace命令，对php-fpm的进程号进行追踪，也就是执行这个命令：</p><pre><code class="language-bash">strace -p 进程号\n</code></pre><p>果然发现一个非常奇怪的现象：整屏刷的都是gettimeofday()这个系统调用：</p><p><img src="https://static001.geekbang.org/resource/image/c9/9b/c934deec032f2442114da949d05eb89b.png?wh=338x439" alt="图片"></p><p>看起来好像只有这个调用了，那有没有别的调用呢？</p><p>我们可以对strace命令加上 <strong>-c参数</strong>，这样可以统计每个系统调用消耗的时间和次数，看看这个奇怪的gettimeofday()系统调用，占用了多少比例。我们在strace前面加上timeout 5，就可以收集5秒钟的数据了，命令如下：</p><pre><code class="language-bash">timeout 5 strace -cp 进程号\n</code></pre><p>命令输出如下：</p><p><img src="https://static001.geekbang.org/resource/image/60/be/604c6160855df236940b31b6c8dff7be.png?wh=470x498" alt="图片"></p><p>可见，gettimeofday()占用了这个进程高达97.91%的运行时间。在短短的5秒之内，gettimeofday()调用次数达到了2万多次，而其他正常的系统调用，比如poll()、read()等，只有几十上百次。也就是说，<strong>非业务操作的耗时是业务操作耗时的50倍</strong>（<code>98%:2%</code>）。难怪进程这么卡，原来全都花在执行getimeofday()，也就是收集系统时间数据上了。</p><p>那么，为什么会有这么多gettimeofday()的调用呢？这里的php-fpm有什么特殊性吗？我们按这个方向去搜索，发现有不少人也遇到了同样的问题，比如Stack Overflow上这个人的<a href="https://stackoverflow.com/questions/23454686/high-cpu-usage-in-php-fpm">“遭遇”</a>。简而言之，原因多半是启用了某些性能监控软件。</p><p>我们把这个情况告诉了客户，对方忽然想起来，最近他们的国外团队确实有做一些性能监控的事情，用的软件好像叫New Relic。果然，我们在客户服务器上找到了这个New Relic。看下图：</p><p><img src="https://static001.geekbang.org/resource/image/17/09/17d0791794e05ae3a7bd242797a5dc09.png?wh=1038x92" alt="图片"></p><p>原来，问题的根因就是他们的国外团队部署了New Relic，而这个软件发起了极为频繁的gettimeofday()系统调用。这些调用抢走了大部分的CPU时间，这就导致业务代码基本没有机会被执行。因为LB有个60秒超时的机制，所以它眼看着收不到后端Nginx服务器的返回，就不得不返回HTTP 504了。也就是下面这样：</p><p><img src="https://static001.geekbang.org/resource/image/19/70/19b070a7e8070a5a2ff14246ec37fc70.jpg?wh=2000x609" alt=""></p><p>这个根因有点令人哭笑不得，不过对见怪不怪的技术支持团队来说，心里早已云淡风轻了。接下来就是客户卸载New Relic，应用立刻恢复正常。用strace再次检查，显示系统调用也恢复正常：</p><p><img src="https://static001.geekbang.org/resource/image/05/0e/05496c48587de3f23eb542d5a3c0630e.png?wh=479x496" alt="图片"></p><p>可以看到，read()和poll()系统调用，上升到进程CPU时间的60%以上，而gettimeofday()降低到不足1%，所以已经彻底解决问题了。</p><h2>案例2：LB特性和不均衡</h2><p>我再给你讲一个案例。还有一个电商客户，也遇到了一次负载不均衡的问题。这是在双十一期间，客户发起了促销活动，随之而来的访问压力的上升也十分明显，而他们的后端服务器也出现了负载不均的现象，有时候访问十分卡顿，于是我们再次介入排查。</p><p>这次的架构是一台LB后面接了两台服务器，其中一台的CPU load高达50以上，也是远远超过了8个的CPU核数。另外一台情况要好一些，load在10左右，当请求被分配到这台服务器上的时候，还算勉强可以访问。</p><p><img src="https://static001.geekbang.org/resource/image/07/c7/07602837e2c1a6c6a203762b5b37d0c7.jpg?wh=1672x513" alt=""></p><p>因为CPU load在两台机器上有比较大的差异，从访问速度来说，大约是一半请求会非常慢，一半请求略快一些，所以客户就怀疑：LB的请求分配做得不均衡，导致其中一台处理不过来。</p><p>真的是这样吗？我们检查了LB的访问日志，发现一个有意思的现象：</p><ul>\n<li>大部分的访问请求是到了/api.php这个URL上。</li>\n<li>按HTTP请求的源IP来分开统计，某些IP的访问量远远大于其他IP。</li>\n</ul><p>我们来看一下根据源IP分开统计的访问次数：</p><p><img src="https://static001.geekbang.org/resource/image/de/aa/de51fda7a8yye48963e4bc200290ceaa.jpg?wh=500x250" alt=""></p><p>很明显，图中第一行的源IP，对/api.php这个URL有3470次访问，而其他源IP的访问量比它低了一个数量级，只有小几百。那么，这跟问题有什么关系呢？</p><p>我们通过对LB日志做进一步的分析后发现，同样是源IP的请求，要么去了Nginx 1，要么去了Nginx 2。再次检查了这台LB的配置后，我们发现客户在LB上开启了“<strong>会话保持</strong>”（Session Persistence）。这就造成了下面这种分布不均的现象：</p><p><img src="https://static001.geekbang.org/resource/image/09/b2/09c280d1644b4b9b87571c953ba767b2.jpg?wh=1625x507" alt=""></p><p>你可能也知道，会话保持是LB的常见功能，它主要是起到了维持会话状态的作用。在开启了会话保持功能后，LB会维护一张映射表，供每次分发请求之前做匹配。如果LB查到某个请求属于之前的某个会话，就会把这个请求转发给上一次会话所选择的后端服务器；否则就按默认的负载均衡算法，来选择一个后端服务器。</p><p>那么，LB怎么确定一个请求属于之前的会话呢？这就要说到会话保持的类型了。我们用的LB是HAProxy，它的会话保持有两种。</p><ul>\n<li><strong>源IP</strong>：凡是源IP相同的请求，都去同一个后端服务器。</li>\n<li><strong>Cookie</strong>：凡是HTTP Cookie值相同的请求，都去同一个后端服务器。</li>\n</ul><p>而客户启用的是第一种：基于源IP的会话保持。不巧的是，这次访问量的源IP本身的分布就很不均匀。就像前面提到的，某个IP发起了10倍于其他IP的请求量，那么这个源IP所分配到的后端服务器，就不得不服务着10倍的请求了，然后就导致了负载不均的问题出现。</p><p>所以，我们让客户关闭了会话保持，两台后端服务器的负载很快就恢复平衡了。我们也给出了进一步的建议：最好把/api.php这个服务跟其他的服务隔离开，比如使用另外的域名，做另一套负载均衡。这是因为，/api.php的访问量和作用，跟其他接口的区别很大，通过对它做独立的负载均衡，既可以隔离互相之间的干扰，也有利于提供更加稳定的访问质量。</p><p>而且，这次的问题也是无法用tcpdump来排查的，它需要我们对LB的特点很熟悉，以及对LB和应用结合的场景，都有一个全面的认识。</p><h2>实验</h2><p>我们已经学习了strace，现在做一个简单的小实验，来巩固一下学到的知识吧。我们可以这样做：</p><ol>\n<li>到<a href="https://hub.docker.com/r/moonlightysh/v_nginx">这里</a>下载并执行一个Nginx服务的Docker镜像，也就是执行：</li>\n</ol><pre><code class="language-bash">docker run --cap-add=SYS_PTRACE -p 80:80 -it docker.io/moonlightysh/v_nginx bash\n</code></pre><blockquote>\n<p>注意，这里必须加上 <code>--cap-add=SYS_PTRACE</code>，否则容器内的strace将不能正确运行。</p>\n</blockquote><ol start="2">\n<li>进入容器后，启动里面的Nginx服务，执行：</li>\n</ol><pre><code class="language-bash">systemctl start nginx\n</code></pre><ol start="3">\n<li>还是在容器内，启动strace：</li>\n</ol><pre><code class="language-bash">strace -p $(pidof nginx | awk \'{print $1}\')\n</code></pre><p>此时，strace就开始监听Nginx worker进程了。</p><blockquote>\n<p>补充：这里用 <code>awk \'{print $1}\'</code> 是为了追踪Nginx worker进程，它出现在pidof命令输出的左边，而最右边是Nginx master进程。跟踪Nginx master进程是看不到HTTP处理过程的。</p>\n</blockquote><ol start="4">\n<li>从你的本机发起HTTP请求，也就是执行 <code>curl localhost:80</code>，此时在容器内Nginx在处理这次请求时，就会发起很多系统调用，而strace就全面展示了这些调用涉及的文件和参数细节。比如下面这样的strace输出：</li>\n</ol><pre><code class="language-bash">root@48fc1221a03a:/# strace -p $(pidof nginx | awk \'{print $1}\')\nstrace: Process 14 attached\nepoll_wait(9, [{EPOLLIN, {u32=4072472592, u64=140681431285776}}], 512, -1) = 1\naccept4(6, {sa_family=AF_INET, sin_port=htons(60070), sin_addr=inet_addr("172.17.0.1")}, [112-&gt;16], SOCK_NONBLOCK) = 3\nepoll_ctl(9, EPOLL_CTL_ADD, 3, {EPOLLIN|EPOLLRDHUP|EPOLLET, {u32=4072473289, u64=140681431286473}}) = 0\nepoll_wait(9, [{EPOLLIN, {u32=4072473289, u64=140681431286473}}], 512, 60000) = 1\nrecvfrom(3, "GET / HTTP/1.1\\r\\nHost: localhost\\r"..., 1024, 0, NULL, NULL) = 73\nstat("/var/www/html/", {st_mode=S_IFDIR|0755, st_size=4096, ...}) = 0\nstat("/var/www/html/", {st_mode=S_IFDIR|0755, st_size=4096, ...}) = 0\nstat("/var/www/html/index.html", 0x7ffcb86eac80) = -1 ENOENT (No such file or directory)\nstat("/var/www/html", {st_mode=S_IFDIR|0755, st_size=4096, ...}) = 0\nstat("/var/www/html/index.htm", 0x7ffcb86eac80) = -1 ENOENT (No such file or directory)\nstat("/var/www/html/index.nginx-debian.html", {st_mode=S_IFREG|0644, st_size=612, ...}) = 0\nstat("/var/www/html/index.nginx-debian.html", {st_mode=S_IFREG|0644, st_size=612, ...}) = 0\nopenat(AT_FDCWD, "/var/www/html/index.nginx-debian.html", O_RDONLY|O_NONBLOCK) = 11\nfstat(11, {st_mode=S_IFREG|0644, st_size=612, ...}) = 0\nsetsockopt(3, SOL_TCP, TCP_CORK, [1], 4) = 0\nwritev(3, [{iov_base="HTTP/1.1 200 OK\\r\\nServer: nginx/1"..., iov_len=247}], 1) = 247\nsendfile(3, 11, [0] =&gt; [612], 612)&nbsp; &nbsp; &nbsp; = 612\nwrite(4, "172.17.0.1 - - [07/Mar/2022:14:2"..., 87) = 87\nclose(11)&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;= 0\nsetsockopt(3, SOL_TCP, TCP_CORK, [0], 4) = 0\nepoll_wait(9, [{EPOLLIN|EPOLLRDHUP, {u32=4072473289, u64=140681431286473}}], 512, 65000) = 1\nrecvfrom(3, "", 1024, 0, NULL, NULL)&nbsp; &nbsp; = 0\nclose(3)&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; = 0\nepoll_wait(9,\n</code></pre><h2>小结</h2><p>我们的课程主旨是网络排查，但因为网络跟系统又是密不可分的关系，所以在掌握Wireshark等技能以外，我们最好要熟悉操作系统排查的常规步骤。</p><p>在这节课里，我们通过一个典型的系统问题导致服务慢的案例，学习了内核空间（内核态）、用户空间（用户态）、系统调用这些操作系统的概念。另外我们还重点学习了strace这个工具，知道它有两种使用方式：<strong>直接在命令之前加上strace；执行strace -p PID。</strong></p><p>那么在案例中，我们也用 <strong>strace-cp PID中的-c参数</strong>，对进程发起的各种系统调用进行了执行次数和执行时间的统计，这对于分析进程耗时的去向会很有帮助。</p><p>在这里，我们也再一次温习下HTTP 504的概念。在后端服务不能在LB的时限内回复HTTP响应的时候，LB就会用HTTP 504来回复给客户端，告诉它是后端服务超时（潜台词：我LB可没问题）。</p><p>而在第二个案例里，我们了解了<strong>会话保持在某些情况下会“放大”负载不均衡</strong>的问题。所以你要知道，在启用会话保持功能时，你需要根据实际情况，做通盘的考虑。</p><h2>思考题</h2><p>给你留两道思考题：</p><ul>\n<li>在案例1里面，LB回复了HTTP 504。那在什么情况下，这个LB会回复HTTP 503呢？</li>\n<li>除了strace，你还知道哪些trace类的工具可以帮助排查呢？</li>\n</ul><p>欢迎你在留言区分享自己的经验，我们一同成长。</p>',
        article_title: "21 | 为什么用了负载均衡更加不均衡？",
      },
      {
        title: "22 | 为什么压力测试TPS总是上不去？",
        id: 493633,
        content:
          '<p>你好，我是胜辉。</p><p>在上一讲里，我们排查了一个跟操作系统紧密相关的性能问题。我们结合top和strace这两个工具，抓住了关键点，从而解决了问题。性能问题，确实也是我们日常技术工作中的一个重要话题。在出现性能问题以后，我们要有能力搞定它；而在出现性能问题之前，最好能提前预见到它。而要“预见”性能瓶颈，最好的方法就是做<strong>压力测试</strong>。</p><p>但是，我们在做压力测试的过程中也时常出现预料不到的情况。比如在离预期的瓶颈值还很远的时候，系统就出现了各种意外，影响到压力测试的继续进行。</p><p>那么在这节课里，我们会回顾几个典型的压力测试场景中的网络问题，一起来学习其中的关键要点。同时，我们还会学习一系列跟网络性能相关的压测工具和检测工具，这样以后你遇到类似的问题时，就有所准备了。</p><h2>压测要做什么？</h2><p>压力测试的诉求实际上多种多样，不过大体上可以分为这几种。</p><p><strong>应用的承受能力</strong>：这主要在第七层应用层，比如发起了压测，把服务端的CPU打到95%甚至100%，观察这时候的请求的TPS、请求耗时、并发量等等。而这些对于不同的业务场景，又会有不同的侧重点。比如：</p><ul>\n<li>对于时间敏感型业务来说，请求耗时（Latency）这个指标就是关键了。</li>\n<li>对于经常做秒杀的电商来说，并发处理量TPS（Transaction Per Second）就是一个核心关注点了。</li>\n</ul><!-- [[[read_end]]] --><p><strong>LB的连接处理能力</strong>：这主要在第四层TCP，看LB能最大支持的TCP并发连接数。这时候，发起压测的客户端一般会指定比较大的并发数，这样就可以发起尽量多的TCP连接。</p><p><strong>网络的承受能力</strong>：这可能主要在第三层IP层了，比如测试上行和下行带宽能否跑满、是否有丢包和额外的延迟，等等。特别是对于一些流量比较大的场景，很可能服务端计算能力都还在，但带宽已经不够用了，所以我们要提前发现这些隐患。</p><h2>案例1：压测TPS上不去</h2><p>我们有个客户是传统企业转型做电商，有一次准备搞大促。为了确保大促顺利，他们要提前对网站进行压测。</p><p>客户用的压测工具比较简单，是Apache ab。ab是Apache Benchmark的缩写，它的用途就是对HTTP服务端发起测试，以获得性能指标（Benchmark）。ab本身不是独立安装的，而是在apache2-utils工具包里，所以你可以这样来安装它：</p><pre><code class="language-bash">apt install apache2-utils\n</code></pre><p>ab是一个轻量级的工具，因为相对其他重量级的工具比如LoadRunner或者JMeter来说，ab只要一行命令就可以发起压测了，是不是很省事？比如你可以这样：</p><pre><code class="language-bash">ab -c 100 -n 10000 目标URL\n</code></pre><p>通过上面的命令，你就用-c 100这个参数，让ab发起了100个并发的请求，而-n 10000指定了总共发送的请求量。</p><blockquote>\n<p>补充：这里有一个小的注意点。如果目标URL只是站点名本身，还是需要在结尾处加上“<code>&gt;/</code>”，要不然ab会报这个错误：<code>ab: invalid URL</code></p>\n</blockquote><p>比如我用下面这条ab命令，对一个著名网站发起“压测”，当然我的参数选择的很小，只有10个并发，一共100次请求，尽量避免打扰到这个网站。我们可以看一下输出：</p><pre><code class="language-bash">$ ab -c 10 -n 100 https://www.baidu.com/abc\nThis is ApacheBench, Version 2.3 &lt;$Revision: 1843412 $&gt;\nCopyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/\nLicensed to The Apache Software Foundation, http://www.apache.org/\n\nBenchmarking www.baidu.com (be patient).....done\n\n\nServer Software:&nbsp; &nbsp; &nbsp; &nbsp; Apache\nServer Hostname:&nbsp; &nbsp; &nbsp; &nbsp; www.baidu.com\nServer Port:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 443\nSSL/TLS Protocol:&nbsp; &nbsp; &nbsp; &nbsp;TLSv1.2,ECDHE-RSA-AES128-GCM-SHA256,2048,128\nServer Temp Key:&nbsp; &nbsp; &nbsp; &nbsp; ECDH P-256 256 bits\nTLS Server Name:&nbsp; &nbsp; &nbsp; &nbsp; www.baidu.com\n\nDocument Path:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; /abc\nDocument Length:&nbsp; &nbsp; &nbsp; &nbsp; 201 bytes\n\nConcurrency Level:&nbsp; &nbsp; &nbsp; 10\nTime taken for tests:&nbsp; &nbsp;9.091 seconds\nComplete requests:&nbsp; &nbsp; &nbsp; 100\nFailed requests:&nbsp; &nbsp; &nbsp; &nbsp; 0\nNon-2xx responses:&nbsp; &nbsp; &nbsp; 100\nTotal transferred:&nbsp; &nbsp; &nbsp; 34600 bytes\nHTML transferred:&nbsp; &nbsp; &nbsp; &nbsp;20100 bytes\nRequests per second:&nbsp; &nbsp; 11.00 [#/sec] (mean)\nTime per request:&nbsp; &nbsp; &nbsp; &nbsp;909.051 [ms] (mean)\nTime per request:&nbsp; &nbsp; &nbsp; &nbsp;90.905 [ms] (mean, across all concurrent requests)\nTransfer rate:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 3.72 [Kbytes/sec] received\n\nConnection Times (ms)\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; min&nbsp; mean[+/-sd] median&nbsp; &nbsp;max\nConnect:&nbsp; &nbsp; &nbsp; 141&nbsp; 799 429.9&nbsp; &nbsp; 738&nbsp; &nbsp; 4645\nProcessing:&nbsp; &nbsp; 19&nbsp; &nbsp;67 102.0&nbsp; &nbsp; &nbsp;23&nbsp; &nbsp; &nbsp;416\nWaiting:&nbsp; &nbsp; &nbsp; &nbsp;17&nbsp; &nbsp;67 101.8&nbsp; &nbsp; &nbsp;23&nbsp; &nbsp; &nbsp;416\nTotal:&nbsp; &nbsp; &nbsp; &nbsp; 162&nbsp; 866 439.7&nbsp; &nbsp; 796&nbsp; &nbsp; 4666\n\nPercentage of the requests served within a certain time (ms)\n&nbsp; 50%&nbsp; &nbsp; 796\n&nbsp; 66%&nbsp; &nbsp; 877\n&nbsp; 75%&nbsp; &nbsp; 944\n&nbsp; 80%&nbsp; &nbsp;1035\n&nbsp; 90%&nbsp; &nbsp;1093\n&nbsp; 95%&nbsp; &nbsp;1339\n&nbsp; 98%&nbsp; &nbsp;1530\n&nbsp; 99%&nbsp; &nbsp;4666\n&nbsp;100%&nbsp; &nbsp;4666 (longest request)\n</code></pre><p>结尾部分的多个Percentage，确切地说就是Percentile，也就是百分位。比如50% 796这一行的意思是：第50个请求（因为总数是100个）的耗时小于等于796毫秒，另外50个请求大于796毫秒。那么我们也可以知道，耗时最长的那个就是排最后一名的4666，它的耗时是4666毫秒。</p><p>这次，客户用ab时指定的具体参数是这样的：</p><pre><code class="language-bash">ab -k -c 500 -n 100000 http://site.name.com/path\n</code></pre><p>也就是并发数为500，总次数为10万，而-k参数是启用长连接。然后就是查看以下两个主要指标：</p><ul>\n<li>ab这个客户端的耗时分布，也就是前面刚介绍过的各个百分位的耗时数值。</li>\n<li>服务端的性能指标，也就是在这个压力下面，服务端的CPU、内存、网络丢包率等的统计数值。</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/8c/9d/8ce9cb87baccc568020963321f3ff39d.jpg?wh=1575x445" alt=""></p><p>压测过程中，客户发现测试端的带宽用到400Mbps后，TPS就上不去了，无论把并发量或者总量的数值进行怎样的调整，TPS都会维持在一个稳定的数值。</p><pre><code class="language-bash">Transfer rate:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 52087.82 [Kbytes/sec] received\n</code></pre><p>那究竟是不是购买的服务端主机的性能不够的原因呢？要知道，客户端和服务端都是1Gbps的网卡，客户是想压满100%的带宽，现在只用到了40%的带宽，TPS就上不去了。</p><p>为什么会这样呢？</p><p>我们就跟客户配合，一边做ab压测，一边做了4秒钟的tcpdump抓包，这对采样分析来说也够用了。然后打开抓包文件，查看专家信息（Expert Information），如下：</p><blockquote>\n<p>补充：抓包示例文件已经上传至<a href="https://gitee.com/steelvictor/network-analysis/tree/master/22">Gitee</a>，建议结合抓包文件和文稿一起学习。</p>\n</blockquote><p><img src="https://static001.geekbang.org/resource/image/d6/00/d64b27b7a79ce3edcd702d0fcffdb900.jpg?wh=1822x592" alt="图片"></p><p>这次我就不做标记了，你自己先找找看，能否找到一些问题？</p><p>你有没有发现，GET有1603次（倒数第三行），而RST有1982次（第三行），比GET这种HTTP请求的次数还更多。也就是说，平摊的话，每次GET请求对应了一次以上的RST。这个现象会不会跟TPS上不去的问题有关系呢？</p><p>我们选一个RST来看一下情况。比如14031号报文：</p><p><img src="https://static001.geekbang.org/resource/image/46/28/4610b0fd0352d03523f448480c256428.jpg?wh=1920x847" alt="图片"></p><p>然后Follow -&gt; TCP Stream，就来到了这条TCP流：</p><p><img src="https://static001.geekbang.org/resource/image/fa/d0/fa6d59359e1044a4844b6a29c3a260d0.jpg?wh=1920x782" alt="图片"></p><p>在这里，你有没有发现两个重传报文？一个是6504号报文，一个是7340号报文。我们再进一步看一下这两个的重传的原因是什么。一般来说，判断一个报文是否是重传，最方便的就是借助Wireshark本身提供的提示，Wireshark说是Retransmission，那就是了。</p><p>那么假设世界上还没有Wireshark，你又<strong>如何判断一个报文是否是重传呢？</strong>其实可以根据两个关键信息：<strong>序列号、载荷长度</strong>。</p><p>我在之前的课程里提到过，序列号本身反映的就是字节位置，载荷的长度就表示这段报文的实际字节长度，这两个信息就确定了信息的<strong>起点和终点</strong>，也就是决定了一个报文是否是之前报文的重传。</p><p>在上面的截图里，我们根据序列号（Sequence Number列）和载荷长度（TCP Seglen列），判断出下面两次重传：</p><p><img src="https://static001.geekbang.org/resource/image/10/fa/10e8909974fd01608f6f7a976949b7fa.jpg?wh=1966x550" alt=""></p><p>可见，7340是3155报文的重传，因为它们的序列号都是9593，载荷长度都是1061。同样的道理，6504和3156也是一对重传关系。</p><p>当然，我们也可以直接在Wireshark里选中7340号报文，它的基础报文也就是3155的<strong>前面会出现一个小圆点</strong>，就像下图这样：</p><p><img src="https://static001.geekbang.org/resource/image/e4/57/e460a19638fb3b633e1315cc02b07857.jpg?wh=1920x354" alt="图片"></p><p>总之，我们确认TCP传输中是有丢包和重传现象的，我们可以回头从专家信息那里得到证实，这里显示有184个Suprious重传和2274个（超时）重传。</p><p><img src="https://static001.geekbang.org/resource/image/2d/9d/2d44ff9320c8bf3e72ff8e907a9eyy9d.jpg?wh=1794x78" alt="图片"></p><blockquote>\n<p>补充：而出现RST的原因，是客户端在已经没有了这个TCP连接的情况下，收到了服务端的ACK报文。从现象来看，客户端应该是做了TIME_WAIT优化的设置，我们后面的实验里也会带到。</p>\n</blockquote><p>考虑到这是在压测场景中出现的丢包现象，我们根据经验，想到了<strong>网卡包量</strong>的问题。</p><p>一般说到网络性能，我们会讨论的就是带宽、时延、网速等等这些指标。实际上，另外一个性能指标常常在达到带宽极限之前就已经触顶了，它就是包量。</p><p><strong>包量是对PPS（Packet Per Second）的简称，一般用来衡量一台主机的网络处理能力。</strong>那么，这次是不是触及了服务端主机的包量上限了呢？我们又如何检查包量指标呢？</p><p>我们需要用的工具是sar。</p><h3>性能工具sar</h3><p>sar是sysstat工具集的一部分。这个工具集包含了一些很有用的工具，除了sar，还有mpstat、iostat等等。如果你平时经常做操作系统维护方面的工作，应该对它们比较熟悉了。这些工具还有一个共同的特点，运行的时候一般都是加上“间隔 次数”这样的参数的。也就是下面这样：</p><pre><code class="language-bash">sar -n DEV 1 10   #查看网卡性能\niostat 1 10       #查看IO性能\nmpstat 2 5        #查看CPU性能\n</code></pre><p>这些工具会按指定的时间间隔，运行指定的次数后再退出。这样我们就可以观察到这段时间内的多次输出了，很适合通过这种多次的观察，得到比较准确的性能数据趋势。</p><p>这次我们也在被压测的服务端云主机上运行了 <code>sar -n DEV</code>，得到了以下的性能数据：</p><p><img src="https://static001.geekbang.org/resource/image/55/be/5523ee0f7eba2cce7979620e4abccabe.jpg?wh=749x338" alt="图片"></p><p>前面说的包量，就是体现在两个pck/s指标中，一个是 <strong>rxpck/s</strong>，就是接收方向的包量；一个是 <strong>txpck/s</strong>，就是发送方向的包量。显然，图中的数值已经在5万左右，这个正是当时我们云主机性能的上限。难怪服务端已经无法提供更高的TPS了，因为网络包的处理都来不及做了。</p><p>那你可能会问了：“网络包也有大有小，这个包量指标说的是大包还是小包呢？”</p><p>其实，一般的包量测试，不是随便什么大小的报文都可以测试，而是普遍使用64字节长度的IP报文。另外，我们也要认识到，包的大小对包量性能的影响也不是很大。这是因为，对于网络处理来说，主要的开销在包的头部的处理上，而载荷本身的处理是很快的。</p><p>那么，对于客户遇到的这种包量达到上限的情况，我们可以选择的应对办法是这样的：</p><ul>\n<li>选择更高网络性能的主机，比如硬件的RSS、软件的RPS等特性，都会大幅提升包量处理性能。</li>\n<li>对服务集群进行水平扩展，也就是在LB后面增加服务器，这样VIP作为一个整体提供的包，处理能力也就提升了。</li>\n</ul><h2>案例2：LoadRunner压测发现部分失败</h2><p>这是另外一个客户，他们没有用ab这样的简单工具，而是用了LoadRunner这样一个企业级的测试软件。LoadRunner的厉害之处在于，不仅可以发起巨大的请求量，而且可以模拟用户的复杂行为，比如登录、浏览、加入购物车等等。这一系列事务有前后状态关系，这就不是简单的ab可以做到的了。</p><p>但是测试结果中的小几十个Failed和Error引起了客户的疑虑。他们怀疑：是不是我们公有云的机器或者网络质量不行，所以才导致了这些失败呢？</p><p><img src="https://static001.geekbang.org/resource/image/91/cb/9185cd1c33463aa7594bf290547fb1cb.jpg?wh=450x248" alt=""></p><p>我们就尝试复现这个压测场景，同时也对测试机上的TCP资源情况做检查，其中最主要的就是源端口了。</p><p>为什么会想到这个呢？因为压测发起的请求，都是依托于TCP连接的。我们在<a href="https://time.geekbang.org/column/article/477510">第1讲</a>里就提到过：<strong>TCP连接是基于五元组的</strong>。那么对于客户端来说，源IP、目的IP、目的端口、协议，这四个元素都不会变化，<strong>唯一会变的就是自己的源端口</strong>了。那么我们来看看，当时测试机的源端口情况。</p><p>这是一台Window机器，跟Linux类似，我们执行这条命令：</p><pre><code class="language-bash">netstat -ant\n</code></pre><p>输出如下图：</p><p><img src="https://static001.geekbang.org/resource/image/ce/a4/cee3b9c775369dba1cfd2aaa2c6d8fa4.jpg?wh=550x328" alt=""></p><p>原来，确实有大量的TCP连接都在TIME_WAIT状态，尤其是源端口已经用到了<strong>65534</strong>。所以可以肯定，这次压测中出现失败的原因，就在于源端口耗尽。</p><p>那要如何处理呢？显然我们也变不出更多的端口来。这个时候，我们可以<strong>调整压测软件的设置，比如从短连接改成长连接</strong>。这样就可以避免源端口耗尽的情况，因为所有的请求是在长连接里完成的，只要连接池本身设置合理，源端口就不会被用完。</p><h2>案例3：压测报cannot assign requested address错误</h2><p>我们内部有一个团队在做业务压力测试，结果遇到了一个奇怪的报错：cannot assign requested address。这次的场景是这样的：这个团队从多台客户端机器向LB上的一个VIP，发送大量的请求，而LB的后面就是很多的服务器。</p><p><img src="https://static001.geekbang.org/resource/image/6d/fb/6d0449c9c02b508eaa5bc7956c0194fb.jpg?wh=1858x711" alt=""></p><p>但是，还没等到这些服务器的负载跑起来，客户端那边提前报错了。这是一个Go语言的程序，具体的报错信息如下：</p><pre><code class="language-plain">https://test.vip/a": dial tcp 10.123.123.12:443: connect: cannot assign requested address\nhttp post Post \\"https://test.vip/b": dial tcp 10.123.123.12:443: connect: cannot assign requested address retry 1 times\n</code></pre><p>于是他们就怀疑：既然报错是连接出错，那是不是LB有什么问题呢？比如，是否LB本身处理能力不够，不能服务这么大量的连接请求？否则为什么客户端会报connect的错误呢？要知道，用来发起TCP连接的系统调用（syscall）就是connect。</p><p>确实，这个报错信息“dial tcp 10.123.123.12:443: connect: cannot assign requested address”说得不太清楚。表面上看，就是客户端往10.123.123.12:443这个VIP发起连接请求（用connect）然后遇到了报错，也难怪测试团队会找到我们来查看LB的问题。</p><p>我们检查了LB，发现一切正常。于是把排查方向换到客户端。在这次测试中，有一个参数是关于HTTP Keep-alive的。如果你对课程前面的内容还有印象的话，应该还记得我们在第7讲“<a href="https://time.geekbang.org/column/article/482610">保活机制</a>”里，深入讨论过这个问题。</p><p>简单来说，这次的测试配置里，HTTP Keep-alive没有打开，导致这些TCP连接被视作短连接来处理了，也就是一次HTTP请求和响应完成后，这条连接就关闭了。由于发起关闭的是客户端自己，于是这条连接也就进入了TIME_WAIT状态。</p><p>而要查看TIME_WAIT状态的连接数量，我们可以用 <strong>netstat命令</strong>，配合管道和awk来完成统计。比如，这次我在一台客户端机器上执行了下面的命令：</p><pre><code class="language-bash">$ netstat -ant |awk \'{++a[$6]} END{for (i in a) print i, a[i]}\'\nTIME_WAIT 28231\n</code></pre><p>或者用下面的 <strong>ss命令</strong>会更快：</p><pre><code class="language-bash">ss -ant | awk \'{++s[$1]}END{for(k in s) print k,s[k]}\'\nTIME-WAIT 28231\n</code></pre><p>可见，处于TIME_WAIT状态的连接数接近3万个，差不多就是Linux的本地动态端口的范围了。我们随后检查这台Linux机器的本地源端口范围，执行了下面的命令：</p><pre><code class="language-plain">$ cat /proc/sys/net/ipv4/ip_local_port_range\n32768\t60999\n</code></pre><p>于是发现它的下限是32768，上限是60999，范围正好就是28231，跟TIME_WAIT的数量一致，显然也是一次源端口耗尽导致的压测问题。</p><p>当然，用sysctl也一样可以查看这个范围：</p><pre><code class="language-bash">sysctl net.ipv4.ip_local_port_range\n</code></pre><p>不过你可能会问：“难道压测中的网络排查，除了包量和源端口，就没有别的问题了吗？”</p><p>我们来看一个不一样的案例。</p><h2>案例4：压测遇到connection reset by peer</h2><p>这次的场景是一个应用团队用netty http client去调用一个VIP做压测。具体来说是9台客户端机器，向同一个LB VIP发起大量的请求。</p><p><img src="https://static001.geekbang.org/resource/image/c4/9e/c42770f906ab7yyf9b964757d120069e.jpg?wh=1866x711" alt=""></p><p>结果遇到了connection reset by peer。蹊跷的是，这个报错是零零星星出现的。而像之前的例子，如果真的源端口用尽了，那么接下来一段时间内，这些请求都会因为本地没有源端口可用而宣告失败，也就是报错会是大面积出现，而不会零星出现。</p><p><img src="https://static001.geekbang.org/resource/image/a0/72/a086cf71e10b0c1f41f79978f6bd1a72.png?wh=1920x1120" alt="图片"></p><p>从图上看，报错数量不超过50个，主要集中在11点20分到11点35分这个时间段，这正是压测的时段。因为报错只有50来个，这相比于这次压测发起的成千上万的请求来说，是很小的比例了。</p><blockquote>\n<p>补充：Y轴就是报错的个数，最高值是10，我们把几个柱体的高度加起来，就是报错的总数量了。</p>\n</blockquote><p>我们了解到，压测期间每个客户端的请求频率是700TPS，所以9台客户端一共会发起6300TPS的请求量。这个问题诡异的地方就是，大部分的请求都能得到正确及时的回复，但是隔了两三分钟，就会出现几次这种connection reset by peer的问题。</p><p>那我们需要理解一下，<strong>为什么会reset</strong>。</p><p>我们在做网络排查的时候，如果在Wireshark里看到TCP RST，往往会觉得它不是一个好的征兆。确实，有时候是RST引起了故障，有时候又是网络故障迫使TCP用RST来结束连接。无论RST是因还是果，它总是跟问题本身逃不脱关系。</p><p>那干脆在TCP里取消RST，是不是很多问题就会被解决呢？当然不是，RST在TCP里面是一个非常必要的组成部分。没有RST，其实就没有“坏”的结束，也就没有“好”的开始。</p><p>大体上，TCP RST的原因可以分为这么几个大类：</p><ul>\n<li>找不到相关连接，那么接收端可以放心地直接发送RST。</li>\n<li>找到了相关连接，但收到的报文不符合TCP规范，那么接收端也可以发送RST。</li>\n<li>找到了相关连接，但传输状况恶劣，内核选择及时“止损”，发送RST。</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/de/d6/de7ec3cb7ccdc5c7da395f271dc935d6.jpg?wh=1861x890" alt=""></p><p>而这次案例里面的情况，就符合第一种。这是因为：</p><ul>\n<li>我们的LB上的VIP有一个idle timeout的设置，如果客户端在一定时限内不发任何报文，那么这条连接将被回收。这个时限是180秒，而且回收时不向客户端发送FIN或者RST报文。</li>\n<li>这次的压测客户端框架里，有一个设置值也是关于idle timeout的。不过这个值设置的是360秒。</li>\n</ul><p>你有没有发现问题？这两个idle timeout值一边大一边小，配合不当，就会出现下面这样的问题：</p><p><img src="https://static001.geekbang.org/resource/image/02/9d/02372ef7e9f63a76aa9f1c8e82957d9d.jpg?wh=2000x1125" alt=""></p><p>某条TCP连接中完成一次HTTP请求和响应之后，连接没有被关闭。过了180秒，LB这一侧的连接被回收了，但客户端那边还没到360秒，所以还认为这条连接是活着的，于是在180秒之后发起了一次请求。报文到达LB，后者在连接表里没有查到这条连接，于是回复了RST。</p><p>根因清楚了，解决起来异常简单：把客户端的Idle timeout参数，从原先的360秒，改成比LB的180秒更低的值就好了。这次改到了120秒，RST就完全消失了。</p><p>你看，虽然在案例2里，我们知道了压测最好使用长连接，这样可以避免源端口耗尽的问题。但是不等于你设置了长连接就一劳永逸了，还需要考虑idle timeout的问题。这次压测的关键在于：要确保长连接本身是真正有效的，你需要<strong>确保客户端的idle timeout小于服务端的idle timeout，这样才能避免连接失效导致的RST</strong>。</p><blockquote>\n<p>补充：这个库是Ractor-Netty，idle timeout对应的是<a href="https://projectreactor.io/docs/netty/release/api/reactor/netty/resources/ConnectionProvider.ConnectionPoolSpec.html#evictInBackground-java.time.Duration-">evictInBackground</a> + <a href="https://projectreactor.io/docs/netty/release/api/reactor/netty/resources/ConnectionProvider.ConnectionPoolSpec.html#maxLifeTime-java.time.Duration-">maxLifeTime</a>。</p>\n</blockquote><h2>实验</h2><p>回顾完这些压测案例，你可能会发现其实难度并不太大，是不是也跃跃欲试了呢？接下来，我们就用ab来做个小实验，这次还捎带一个小任务：<strong>控制TIME_WAIT状态的连接的数量</strong>。</p><p>在案例2和3里面，源端口耗尽的问题，本质是TIME_WAIT需要停留2MSL的时长。要解决TIME_WAIT连接过多的问题，方法有很多。这次我们只实验其中的一种方法，就是修改 <code>net.ipv4.tcp_max_tw_buckets</code> 这个内核参数。</p><p>它的默认值为16384，改为更小的值后，超过这个数值的TIME_WAIT连接会被清除，这样，TIME_WAIT连接数就被控制住了。</p><p>这个实验在本地就可以完成，比如在你的笔记本上安装VirtualBox，然后在这个虚拟机里面完成实验。</p><p>步骤一：安装Nginx和Apache ab。</p><pre><code class="language-bash">apt install nginx\napt install apache2-utils\n</code></pre><p>步骤二：启动ab测试，执行下面的命令。</p><pre><code class="language-bash">ab -c 500 -n 10000 http://localhost/\n</code></pre><p>步骤三：观察TCP连接的各种状态的统计数。</p><pre><code class="language-bash">ss -ant | awk \'{++s[$1]}END{for(k in s) print k,s[k]}\'\n</code></pre><p>此时你应该会看到1万多个TIME_WAIT的连接，然后等待2分钟后继续步骤四。</p><p>步骤四：修改内核参数 <code>tcp_max_tw_buckets</code> 为100。</p><pre><code class="language-bash">sysctl net.ipv4.tcp_max_tw_buckets=100\n</code></pre><p>步骤五：再次ab测试。</p><pre><code class="language-bash">ab -c 500 -n 10000 http://localhost/\n</code></pre><p>步骤六：再次观察TCP连接的各种状态的统计数。</p><pre><code class="language-bash">ss -ant | awk \'{++s[$1]}END{for(k in s) print k,s[k]}\'\n</code></pre><p>此时TIME_WAIT应该只有100了。</p><p>当然，TIME_WAIT本身被设计出来是有原因的，建议你在改动它之前，把自己的场景想清楚，然后再改不迟。</p><h2>小结</h2><p>这节课，我们回顾了几个典型的压力测试场景中的网络问题，你需要重点掌握以下这些知识点。</p><p><strong>关于压测指标和工具。</strong>轻量级的工具是ab，它十分方便，而且也可以发起大量的请求，在简单的压测场景下很有用。比如下面的命令：</p><pre><code class="language-bash">ab -k -c 100 -n 10000 目标URL\n</code></pre><p>重量级工具是LoadRunner和JMeter，它们可以实现复杂的交互行为，也提供更加详细的报告。</p><p>在网络性能领域，<strong>包量是对PPS的简称，一般用来衡量一台主机的网络处理能力。</strong>而评估包量的工具是sar，我们可以运行下面的命令获取到实时的包量值：</p><pre><code class="language-bash">sar -n DEV 1 10\n</code></pre><p><strong>关于TCP的知识。</strong>这节课里我们也再次温习了TCP相关的知识，包括：</p><ul>\n<li>判断一个报文是否是之前报文的重传，可以根据两个关键信息：<strong>序列号、载荷长度</strong>。这两个值分别相同的多个报文，互相就是重传关系了。</li>\n<li>压测数据起不来的一个常见原因是<strong>源端口耗尽</strong>，要验证这一点，可以执行以下命令，来查看TIME_WAIT或者ESTABLISHED状态的连接数是否到达了上限：</li>\n</ul><pre><code class="language-bash">ss -ant | awk \'{++s[$1]}END{for(k in s) print k,s[k]}\'\n</code></pre><p>或者：</p><pre><code class="language-bash">netstat -ant |awk \'{++a[$6]} END{for (i in a) print i, a[i]}\'\n</code></pre><ul>\n<li>而连接数到达上限值的问题，往往跟可用的<strong>本地源端口范围</strong>有关。要查看端口范围，我们可以执行这条命令：</li>\n</ul><pre><code class="language-bash">sysctl net.ipv4.ip_local_port_range\n</code></pre><ul>\n<li>压测中遇到RST的原因，可能跟两端的idle timeout不合理有关，建议把客户端的idle timeout设置为比服务端idle timeout更低的值。</li>\n</ul><h2>思考题</h2><p>最后还是再给你留两道思考题：</p><ul>\n<li>测试HTTP性能经常用ab，那测试带宽本身，一般用什么工具呢？</li>\n<li>要把TIME_WAIT停留的时间2MSL改小，你知道用什么方法吗？</li>\n</ul><p>欢迎在留言区分享你的答案，也欢迎你把今天的内容分享给更多的朋友。</p><h2>附录</h2><p>抓包示例文件：<a href="https://gitee.com/steelvictor/network-analysis/tree/master/22">https://gitee.com/steelvictor/network-analysis/tree/master/22</a></p>',
        article_title: "22 | 为什么压力测试TPS总是上不去？",
      },
    ],
  },
  {
    chapterTitle: "实战三：不用抓包就能做的网络排查篇",
    children: [
      {
        title: "23 | 路径排查：没有网络设备权限要如何做排查？",
        id: 498475,
        content:
          '<p>你好，我是胜辉。</p><p>有一位同事问我：“大V，你的课程会教我们排查家庭网络问题的技巧吗？”这是一个有意思的问题。我们课程里用到了大量的抓包分析，虽然这套方法功效甚大，但是在很多场景下，也有比它更加合适的方法。在网络路径的排查方面，这一点体现得尤为明显。比如一些小工具往往能起到意想不到的大作用。</p><p>不过，一个现实的问题是，我们一般不是专职的网络工程师，也没有相关的网络设备的查看权限，那要如何在这种条件下，尽可能做一些网络层排查的工作呢？这就需要我们对<strong>协议</strong>有深入的理解，对工具能做到灵活地运用。</p><p>在接下来的实战三模块“不用抓包就能做的网络排查篇”，我们就来学习一下网络层的案例和排查技巧。这样你以后遇到跟网络路径异常、丢包、时通时不通等问题的时候，不仅有抓包分析这样的“重型武器”，也有几把趁手的“瑞士军刀”，可以精确快速地搞定这些问题。</p><p>好，我们还是从案例开始。</p><h2>为何TCP连接时常失败？</h2><p>有一次，我们的一个内部客户团队报告了一个TCP连接失常的问题。这是一个MySQL的服务，它有两台服务器，都在同一个LB VIP的后面。这个团队发现，从他们的客户端到这个LB VIP的TCP连接，时常有失败的情况发生，于是我们介入排查。</p><!-- [[[read_end]]] --><p>这个服务的具体架构是这样的：客户团队从他们的客户端发起TCP连接到这台LB上的VIP，这个VIP后面是两台MySQL主机，LB把请求转发给其中的一台。这里我们使用了LB的一个特性，就是可以平时只启用一台后端主机，只有当这台主机离线时，LB才会自动把流量切换到另外一台上。</p><p><img src="https://static001.geekbang.org/resource/image/94/00/94b9dfe3ceb99f06126faf85eb028500.jpg?wh=2000x731" alt=""></p><blockquote>\n<p>补充：那为什么不用MySQL双主 + Keep alive的方案呢？其实，在当时我们的云网络设计里，漂移IP是不可以配置在主机上的，只有LB可以这么配置，所以就采用了这样的方案。</p>\n</blockquote><p>在前面很多次课里，我都提到过用“排除法”或者说是“逐段验证法”来开展排查。这次也不例外，我们就从客户端直接去连接MySQL主机，重试了很多次，TCP连接都可以正常建立。于是，MySQL主机的嫌疑基本可以排除，我们又集中到对LB的排查上来。</p><p><img src="https://static001.geekbang.org/resource/image/5d/0e/5d65cc33e0dcac560610779f92c2070e.jpg?wh=2000x435" alt=""></p><p>在客户端上测试TCP连接的工具有很多，其中<strong>nc</strong>是比较好用的一个。我们就选了一台客户端，<strong>用nc命令对LB VIP发起了多次TCP连接</strong>。我们看看当时的输出：</p><pre><code class="language-bash">root@client:~# while true;do nc -zv 10.111.1.111 3306;sleep 1;done\nConnection to 10.111.1.111 3306 port [tcp/mysql] succeeded!\nConnection to 10.111.1.111 3306 port [tcp/mysql] succeeded!\nnc: connect to 10.111.1.111 port 3306 (tcp)&nbsp;failed: Connection timed out\nConnection to 10.111.1.111 3306 port [tcp/mysql] succeeded!\nConnection to 10.111.1.111 3306 port [tcp/mysql] succeeded!\nConnection to 10.111.1.111 3306 port [tcp/mysql] succeeded!\nnc: connect to 10.111.1.111 port 3306 (tcp)&nbsp;failed: Connection timed out\nConnection to 10.111.1.111 3306 port [tcp/mysql] succeeded!\nConnection to 10.111.1.111 3306 port [tcp/mysql] succeeded!\nConnection to 10.111.1.111 3306 port [tcp/mysql] succeeded!\nnc: connect to 10.111.1.111 port 3306 (tcp)&nbsp;failed: Connection timed out\nConnection to 10.111.1.111 3306 port [tcp/mysql] succeeded!\n</code></pre><p>差不多每四次里就有一次的连接请求会超时失败，确实是有问题。而在LB那边发生了什么呢？</p><p>我们做了第二次nc测试。这次，我们还在客户端和LB上都分别做了tcpdump抓包，这样就可以观察到具体的握手阶段的情况了。客户端做nc的结果还是跟上面这个类似。我们再来看看客户端的抓包的情况，下面是tcpdump命令的输出：</p><pre><code class="language-bash">23:32:37.084634 IP (tos 0x0, ttl 64, id 33781, offset 0, flags [DF], proto TCP (6), length 60)\n&nbsp;&nbsp;&nbsp; 10.222.22.22.47849 &gt; 10.111.1.111.3306: Flags&nbsp;[S], cksum 0x2eed (incorrect -&gt; 0x7045), seq 3882715171, win 14600, options [mss 1460,sackOK,TS val 750324694 ecr 0,nop,wscale 7], length 0\n23:32:38.082468 IP (tos 0x0, ttl 64, id 33782, offset 0, flags [DF], proto TCP (6), length 60)\n&nbsp;&nbsp;&nbsp; 10.222.22.22.47849 &gt; 10.111.1.111.3306: Flags&nbsp;[S], cksum 0x2eed (incorrect -&gt; 0x6f4b), seq 3882715171, win 14600, options [mss 1460,sackOK,TS val 750324944 ecr 0,nop,wscale 7], length 0\n23:32:40.086484 IP (tos 0x0, ttl 64, id 33783, offset 0, flags [DF], proto TCP (6), length 60)\n&nbsp;&nbsp;&nbsp; 10.222.22.22.47849 &gt; 10.111.1.111.3306: Flags&nbsp;[S], cksum 0x6d56 (correct), seq 3882715171, win 14600, options [mss 1460,sackOK,TS val 750325445 ecr 0,nop,wscale 7], length 0\n23:32:44.098519 IP (tos 0x0, ttl 64, id 33784, offset 0, flags [DF], proto TCP (6), length 60)\n&nbsp;&nbsp;&nbsp; 10.222.22.22.47849 &gt; 10.111.1.111.3306: Flags&nbsp;[S], cksum 0x696b (correct), seq 3882715171, win 14600, options [mss 1460,sackOK,TS val 750326448 ecr 0,nop,wscale 7], length 0\n</code></pre><p>这里的Flags [S]表示，这个报文的TCP标志位为SYN。显然，客户端连续发送了4个SYN，但都没有收到SYN+ACK。</p><p>那么在LB这边的tcpdump情况又如何呢？我们来看一下：</p><pre><code class="language-bash">23:31:30.210092 IP 10.222.22.22.47837 &gt; 10.111.1.111.3306: Flags&nbsp;[S], seq 1376708963, win 14600, options [mss 1460,sackOK,TS val 750308151 ecr 0,nop,wscale 7], length 0\n23:31:30.210099 IP 10.111.1.111.3306 &gt; 10.222.22.22.47837: Flags&nbsp;[S.], seq 2920814195, ack 1376708964, win 8190, options [mss 1460], length 0\n23:31:32.214334 IP 10.222.22.22.47837 &gt; 10.111.1.111.3306: Flags&nbsp;[S], seq 1376708963, win 14600, options [mss 1460,sackOK,TS val 750308652 ecr 0,nop,wscale 7], length 0\n23:31:32.214339 IP 10.111.1.111.3306 &gt; 10.222.22.22.47837: Flags&nbsp;[S.], seq 2920814195, ack 1376708964, win 8190, options [mss 1460], length 0\n23:31:36.611535 IP 10.222.22.22.47837 &gt; 10.111.1.111.3306: Flags&nbsp;[S], seq 1376708963, win 14600, options [mss 1460,sackOK,TS val 750309654 ecr 0,nop,wscale 7], length 0\n23:31:36.611542 IP 10.111.1.111.3306 &gt; 10.222.22.22.47837: Flags&nbsp;[S.], seq 2532603288, ack 1376708964, win 8190, options [mss 1460], length 0\n23:31:44.112084 IP 10.222.22.22.47837 &gt; 10.111.1.111.3306: Flags&nbsp;[S], seq 1376708963, win 14600, options [mss 1460,sackOK,TS val 750311656 ecr 0,nop,wscale 7], length 0\n23:31:44.112091 IP 10.111.1.111.3306 &gt; 10.222.22.22.47837: Flags&nbsp;[S.], seq 2532603288, ack 1376708964, win 8190, options [mss 1460], length 0\n</code></pre><p>这里的[S.]就是SYN+ACK，点号[.]就是ACK。显然，<strong>在LB这里既收到了客户端发来的全部4个SYN，自己也发出了4个SYN+ACK，可是并没有收到任何一个[.]</strong>，也就是握手的第三个包。顺便提一下，[F.]是指FIN+ACK，[P.]是指PUSH+ACK，[R.]就是RST+ACK了。</p><p>到这里，我们能拼出问题的全貌了：客户端发出了SYN，LB回复了SYN+ACK，但是却没到达客户端。显然，SYN+ACK是在网络上丢失了。</p><p><img src="https://static001.geekbang.org/resource/image/81/37/81665c475afe7969aeb00f3e86edae37.jpg?wh=2000x986" alt=""></p><p>那会不会是这个链路的偶发性的丢包导致的呢？比如，是否这个链路存在一定比例的丢包，那么也可能在我们测试TCP握手的时候，正好赶上了丢包时段，所以就会观察到这样的现象。</p><p>于是我们就做了<strong>长ping测试</strong>。结果却发现，长ping是100%成功的，一点丢包都没有。这就奇怪了，难道丢包还专门盯着SYN+ACK丢，而ICMP echo reply报文就一点都不丢？这好像是“玄学”。</p><p><img src="https://static001.geekbang.org/resource/image/6d/35/6d0ed37dc0199de99495d20b2fb5a335.jpg?wh=1894x878" alt=""></p><p>所以，为了搞清楚真相，我们需要回答下面两个问题：</p><ul>\n<li>为什么ICMP测试一直能通呢？</li>\n<li>为什么TCP有时候连续丢包，有时候又可以呢？</li>\n</ul><p>我们需要到网络层寻找原因，这里要进入一个新的知识点：ECMP。</p><h3>什么是ECMP？</h3><p>我们先复习一下三层网络的知识。现在随着网络上传输的数据量越来越大，网络基础架构要提供的传输带宽也迅速增大。这一方面需要提升“单兵作战能力”，就是增加网络接口和网线的带宽，另一方面也需要提升“多兵协同作战能力”，也就是把多个链路的带宽充分利用起来。</p><p>那么，假设有这样一个网络场景：一个起点连接到路由器r1，它有两条10Gbps的线路连接到后面的两个节点，分别是cost为10的r2和cost为20的r3，而r2和r3都连向终点10.10.10.0/24这个网段。那么实际上从起点到终点的路径有2条，也就是下图中的路径a和路径b。</p><p><img src="https://static001.geekbang.org/resource/image/fd/fc/fd6ec614561039d62727078d75822bfc.jpg?wh=2000x856" alt=""></p><p>一般来说，因为cost不一样，路由器r1会选择cost更低的那条，也就是路径a。当某些情况导致路径b的cost更低时，流量就切换到路径b。这样的话，这两条路径平时只能用一条，带宽是10Gbps。</p><p>而如果把两个cost设置为相同的值，并启用路由器的某个特性，<strong>这两条路径就可以被全部利用起来。</strong>这个时候，因为两条线路同时使用，所以带宽有20Gbps。也就是下图这样：</p><p><img src="https://static001.geekbang.org/resource/image/f3/b1/f37yy3c70d794d0235ac32b02ffef8b1.jpg?wh=2000x863" alt=""></p><p>这个特性就是 <strong>ECMP</strong>，全称是equal-cost multi-path，它可以让路由器同时使用多条链路，这样就使得通往同一个网段的带宽，变成了原先的好几倍。我们来看看下面的示意图：</p><p><img src="https://static001.geekbang.org/resource/image/c3/f2/c35baefeeyyeeb114612b0a4ddd066f2.jpg?wh=1872x705" alt=""></p><p>如果你在Linux上做过网卡bonding，那你可能对这个“并行使用带宽”的做法并不陌生。在Linux服务器上做多网卡的bonding，也可以达到带宽合并使用的效果。</p><blockquote>\n<p>补充：bonding有多种模式，其中的一部分模式需要交换机配合。</p>\n</blockquote><p>实际上，稍大一点的数据中心都已经使用了ECMP。这样做，一方面极大地提升了网络带宽，而另一方面也带来了新的挑战，也就是网络路径的数量上升了一个数量级，排错的复杂度也明显上升。</p><p>比如，原先每一跳只有一个可用的next-hop，从起点到终点的路径可能只有一条。而<strong>启用了ECMP后，路径条数就是各跳的next-hop数量的乘积</strong>。以下图的拓扑为例，这里一共是6跳，从第1跳到第4跳都各有4个next hop，那么就有 <code>1*4*4*4*1*1=64</code> 种可能的路径。</p><p><img src="https://static001.geekbang.org/resource/image/76/a4/765cd53b0de055a00b3626cdd72866a4.jpg?wh=2000x1125" alt=""></p><p>如果网络报文也有思想，面对ECMP这个场景可能会很兴奋了：“我终于不用每次都走一样的路径了，总算有多种选择了！”就像下图展示的这样，报文可能会走绿色的路线a，也可能是走蓝色的路线b，或者任何别的可行的路线。</p><p><img src="https://static001.geekbang.org/resource/image/c3/bd/c3a89fc66ee588d87477cb39443dd2bd.jpg?wh=2000x1125" alt=""></p><p>不过看到现在，这个ECMP跟我们的案例有什么关系呢？其实，了解了ECMP的核心特点，我们就能回答前面的两个问题：为什么ping可以成功；为什么TCP连接就时常不行。</p><p>这个核心特点就是 <strong>ECMP的路径选择策略</strong>。</p><h3>ECMP的路径选择策略</h3><p>路由器面对多条等价的路径，是不是随便选一个转发策略就行了呢？比如用轮询策略，第一个报文转发给第一条路径，第二个报文给第二条路径，以此类推。</p><p>我们假设就用轮询策略。当ECMP场景下的某一个节点（比如r8）出现故障的时候，经过这个节点的路径就都会受到影响。对于TCP来说，可能前一个报文经过的路径都是好的节点，就可以成功传送，而下一个报文因为轮询策略的关系，选择的路径里有r8这个故障节点，这个报文就会丢失了。</p><p>那么同样地，任何一个TCP流，都可能会被这个故障节点影响到。仅仅一个节点出现问题就可能影响到所有的流量，这是不合理的。</p><p><img src="https://static001.geekbang.org/resource/image/fa/c0/fa2f59256fa17ae64841beef8ce22fc0.jpg?wh=2000x1125" alt=""></p><p>所以我们需要“立规矩”，让同一个流走固定的链路，这样可以限定影响的范围。比如，启用基于哈希的转发机制（也就是三层的负载均衡机制）就可以做到这一点。比较常见的哈希算法是基于报文的五元组，也就是源IP、目的IP、源端口、目的端口、协议，这样就可以确定TCP流。</p><p>有了哈希转发机制，即使有一个节点出现问题，它所影响的就只是分配到这个节点上的TCP流，而没有分配到这个节点的TCP流就不会被影响到，这就起到了风险“隔离”的效果。</p><blockquote>\n<p>这段特殊时期，我们对“隔离”的认识肯定更加深刻了。</p>\n</blockquote><p>到这里，我们就终于可以回答前面两个问题了。</p><h3>根因推理</h3><p>先看第一个问题：<strong>为什么ICMP测试一直能通呢？</strong></p><p>ECMP用的哈希算法多数是基于五元组的哈希，那么ICMP报文的五元组就会是下面这样：</p><p><img src="https://static001.geekbang.org/resource/image/3c/32/3cf7e1e44ac1de96bbb9dce9yy9f8132.jpg?wh=2000x539" alt=""></p><p>对于同一个客户端和服务端，因为源IP、目的IP和协议都不会变，而源端口和目的端口是空缺的，那么哈希值就不会变。这意味着，每次选择的路径都是一样的。那么<strong>如果选到的路径是正常的，做多少次ping也都会是正常的</strong>！这就是第一个问题的答案。</p><p>其实，这时候你可能已经想到第二个问题的答案了：<strong>为什么TCP有时候连续丢包，有时候又可以呢？</strong>我们每次发起新的TCP连接的时候，用的源端口一般跟前一次连接是不同的，所以就会造成它跟ICMP的明显区别：</p><p><img src="https://static001.geekbang.org/resource/image/3a/d4/3a7cc7f59dfaf70e4324e659f8e81dd4.jpg?wh=2000x540" alt=""></p><p>可见，不同的TCP连接，哈希值也不同，所以选择的路径也可能不同。如果某条路径有问题，某一次TCP连接选到这条路径的话就会发生故障！</p><p>我们可以用下面这张示意图来解释在测试中发现的现象。</p><p><img src="https://static001.geekbang.org/resource/image/5d/76/5d8d754537bd32d8b70b887d9b00fb76.jpg?wh=2000x940" alt=""></p><blockquote>\n<p>补充：图中我去掉了各个路由节点，只保留了一条正常路径和一条故障路径。</p>\n</blockquote><p>那还有一个问题：为什么客户端直接访问MySQL主机是正常的呢？其实也是因为ECMP，直接访问MySQL主机时候的路径跟访问LB的路径不同，正好绕开了有问题的节点。</p><p>这就能完美解释我们遇到的所有现象了。玄学也不玄，只要你愿意深究，玄学就能变成科学。</p><p>不过，理论都解释通了，但没有证据还是不行的，让我们完成最后一击。</p><h3>落实证据</h3><p>既然源端口不同，ECMP选择的路线就不同，那如果我们能模拟不同的源端口来测试，应该就能复现问题现象了。有什么工具可以指定源端口做测试呢？</p><p>我们前面用过的 <strong>nc命令</strong>其实就可以，给它加上 <strong>-p参数指定源端口</strong>就行。要知道，像其他工具，比如telnet、curl等，都无法指定源端口，也就没法在当前这个特定场景下帮上忙了。</p><p>所以，我们用下面这个命令，就可以指定用源端口30000来连接服务端了：</p><pre><code class="language-bash">nc -p 30000 -w 5 -vz 10.111.1.111 3306\n</code></pre><p>于是，我们用nc配合不同的源端口（比如从30000到30010）做了多次测试，果然发现其中几个源端口可以稳定复现丢包现象。这其实就证实了我们的推测：路径中有一个节点出现了丢包，如果选择的路径中包含该节点，就一定会遇到问题。</p><pre><code class="language-bash">nc -p 30000 -w 5 -vz 10.111.1.111 3306   #正常\nnc -p 30001 -w 5 -vz 10.111.1.111 3306   #正常\nnc -p 30002 -w 5 -vz 10.111.1.111 3306   #正常\nnc -p 30003 -w 5 -vz 10.111.1.111 3306   #不正常，可以稳定重现\nnc -p 30004 -w 5 -vz 10.111.1.111 3306   #正常\nnc -p 30005 -w 5 -vz 10.111.1.111 3306   #正常\nnc -p 30006 -w 5 -vz 10.111.1.111 3306   #正常\nnc -p 30007 -w 5 -vz 10.111.1.111 3306   #不正常，可以稳定重现\nnc -p 30008 -w 5 -vz 10.111.1.111 3306   #正常\nnc -p 30009 -w 5 -vz 10.111.1.111 3306   #正常\nnc -p 30010 -w 5 -vz 10.111.1.111 3306   #正常\n</code></pre><p>根据这个信息，网络团队定位到了有问题的节点并做了修复，这个TCP连接时通时不通的问题终于被解决了。</p><p>有了对ECMP的充分理解和对nc这个工具的灵活运用，我们即使没有网络设备的权限，也依然发现了某些路径存在丢包的现象，从而推动相关团队更加高效地解决问题。可以说，这又是一次知识上的胜利。</p><h2>拓展：ECMP的其他使用场景</h2><p>ECMP大部分时候用来增加网络骨干环节的带宽，不过它还有一个重要的使用场景，就是实现多活的负载均衡（LB）。我们可以先看一个传统的一主一备的负载均衡架构：</p><p><img src="https://static001.geekbang.org/resource/image/34/e8/34052832f09307d48bf84891fed121e8.jpg?wh=2000x562" alt=""></p><p>而现在大型网站的负载均衡一般都选择做<strong>多活架构</strong>，也就是所有的LB节点都提供服务，这跟前面的一主一备就非常不同了。而要实现多活，就必须让同一个VIP能在多个LB节点上都提供服务。</p><p>我们知道，IP一般是通过ARP广播，让广播域内的机器和交换机，都学习到这个IP和MAC的对应关系，从而“占据”这个IP。交换机学习到这条ARP记录后，就会把去往这个IP的报文都转发给这台机器了。</p><p>那多个LB配置同一个IP，岂不是要“打架”了吗？如果大家都争先恐后地发起ARP，那只有最后一个发ARP的节点可以真正的拿到这个IP，其他节点是无法用这个IP来服务的。</p><p>所以ARP是肯定行不通的，而ECMP配合BGP/OSPF就可以发挥作用了。这些LB节点都会启用BGP或者OSPF这样的路由协议，而不是ARP协议，并跟上联网关进行BGP配对。这样的话，网关收到流量后，会认为这多个LB节点不再是终端设备，而是跟它一样的next-hop路由器，再加上ECMP的加持，流量就能相对均匀地分发到这些LB上。这就实现多活了。我们看一下示意图：</p><p><img src="https://static001.geekbang.org/resource/image/cf/d0/cf6d0cc1ae25ce849507efb42709c1d0.jpg?wh=2000x729" alt=""></p><h2>小结</h2><p>这节课，我们通过案例，学习了网络层的一个重要概念ECMP。<strong>ECMP的最大作用是用多条链路实现更大的传输带宽</strong>。而为了提升可用性，ECMP一般会启用基于<strong>哈希</strong>的转发策略，实现网络流量在多个链路间的有状态的转发。</p><p>因为ECMP多路径的存在，网络排查变得更加复杂。而了解它的哈希转发策略，对我们的排查工作很有帮助。特别是ICMP报文会走固定线路，而TCP/UDP的不同连接会因为五元组的变化而走不同的线路。</p><p>在排查技巧方面，我们学习了一个实用的小工具nc。如果你怀疑ECMP的某个节点有问题，可以尝试<strong>用nc命令来发起不同哈希值的连接，也就是使用-p参数指定源端口做测试</strong>。如果发现某些源端口的表现跟其他端口不同，那么很可能就是ECMP的节点问题。</p><p>另外，我们也学习了一个tcpdump的小知识点。在tcpdump的命令行输出中：</p><ul>\n<li>[S]代表SYN；</li>\n<li>[.]代表ACK；</li>\n<li>[F.]代表FIN+ACK；</li>\n<li>[P.]代表PUSH+ACK；</li>\n<li>[R.]代表RST+ACK。</li>\n</ul><p>网络路径问题的排查是比较复杂的。当路径中某个节点的问题比较严重时，用nc工具相对容易复现出问题。当问题比较隐蔽，比如出错率比较低的时候，可能单纯用nc就不够方便了，我们需要另外一个工具mtr。它是加强版的traceroute，可以做持续的三层可达性的探测。关于mtr的更多细节，我们留到在下一讲里做详细的探讨。</p><h2>思考题</h2><p>最后再给你留两道思考题：</p><ul>\n<li>ECMP除了用五元组做哈希，还有哪些算法呢？</li>\n<li>这个案例中的问题，用traceroute可以定位出来吗？为什么呢？</li>\n</ul><p>欢迎你在留言区分享你的答案，我们一同进步，成长。</p>',
        article_title: "23 | 路径排查：没有网络设备权限要如何做排查？",
      },
      {
        title: "24 | 丢包：如何确定丢包的存在及其程度？",
        id: 499777,
        content:
          '<p>你好，我是胜辉。</p><p>在上一讲里，我们回顾了一个网络路径排查的典型案例。我们是通过 <strong>nc工具</strong>发起不同源端口的连接，从而定位了ECMP路径中的问题。这个排查方法的背后，其实是我们对三层网络的深入理解和灵活应对。在这一类的网络排查中，我们都未必要上抓包分析这样的“重型武器”，只要场景合适，我们就可以用小工具达到大效果。</p><p>现在我们也知道了，这个案例的根因是ECMP路径中某个节点存在<strong>丢包</strong>。而丢包，也是网络排查中特别普遍的现象，特别是下面这三个问题：</p><ul>\n<li>有没有丢包？</li>\n<li>在哪里丢包？</li>\n<li>丢包程度多严重？</li>\n</ul><p>这三个问题的组合，就使得很多故障场景变得复杂。特别是当丢包情况不太明显的时候，问题表象就变得更加“神出鬼没”了。</p><p>所以在这一讲里，我们将会对“丢包”这个十分典型的问题场景进行一次深入的探讨。这样，下次你遇到丢包等问题的时候，就有很多种“兵器”，也知道在什么场景下使用它们，从而真正突破“丢包”这个难点了。</p><p>那么首先，在讨论丢包之前，我们要先对网络排查工具做一下总体的审视。</p><h2>路径排查工具概览</h2><p>网络路径排查的工具有挺多，大体上可以分为两大类：探测类工具和统计类工具。</p><p>为什么要这么分呢？在我看来，网络信息的获取方式，大体上有动态和静态之分：</p><!-- [[[read_end]]] --><ul>\n<li><strong>动态</strong>的信息在传输过程中体现，传输结束后就没有了。它获得的是通信两端在动态变化中的网络状况，所以我们需要做实时的探测，才能把这个动态的状态抓取下来。</li>\n<li><strong>静态</strong>的信息在传输结束后依然存在，继续保存在通信端的系统中。它是通信端的网络信息的历史统计。我们只要通过运行统计类工具，就能把这种累计的信息读取出来了。</li>\n</ul><p>所以下面，我们就先来看看这两类工具具体都包括了什么。</p><h3>探测类工具</h3><p>探测类工具主要包括ping、traceroute、mtr、nc、telnet等，它们都是从一端发起，对另外一端发送探测报文，然后观测报文的丢失、乱序、时延等情况。</p><p>其中，ping、traceroute、mtr主要是利用ICMP或者UDP的特性，实现了<strong>对网络路径状况的检测</strong>。在接下来的课程中，我们会深入探讨traceroute和mtr的工作原理。</p><p>而nc和telnet，则主要是测试<strong>传输层连通性</strong>的，比较典型的场景就是探测TCP握手能否成功。有了这两个工具，我们不需要做tcpdump抓包，就可以检测出TCP端口是否可以连通了。</p><h3>统计类工具</h3><p>统计类工具包括netstat、ss，这些工具都是在一端读取自己的历史统计值，而并不发送出探测报文。比如，我们可以通过netstat -s命令，看到很详细的传输层、网络层、数据链路层等的质量状况，包括报文丢失、重传、重置（RST）等情况的统计。比如下面这样：</p><pre><code class="language-bash"># netstat -s\nIp:\n&nbsp; &nbsp; Forwarding: 1\n&nbsp; &nbsp; 41406 total packets received\n&nbsp; &nbsp; 0 forwarded\n&nbsp; &nbsp; 0 incoming packets discarded\n&nbsp; &nbsp; 41406 incoming packets delivered\n&nbsp; &nbsp; 30976 requests sent out\nIcmp:\n&nbsp; &nbsp; 16 ICMP messages received\n&nbsp; &nbsp; 0 input ICMP message failed\n&nbsp; &nbsp; ICMP input histogram:\n&nbsp; &nbsp; &nbsp; &nbsp; echo replies: 16\n&nbsp; &nbsp; 16 ICMP messages sent\n&nbsp; &nbsp; 0 ICMP messages failed\n&nbsp; &nbsp; ICMP output histogram:\n&nbsp; &nbsp; &nbsp; &nbsp; echo requests: 16\nIcmpMsg:\n&nbsp; &nbsp; &nbsp; &nbsp; InType0: 16\n&nbsp; &nbsp; &nbsp; &nbsp; OutType8: 16\nTcp:\n&nbsp; &nbsp; 18 active connection openings\n&nbsp; &nbsp; 0 passive connection openings\n&nbsp; &nbsp; 0 failed connection attempts\n&nbsp; &nbsp; 1 connection resets received\n&nbsp; &nbsp; 2 connections established\n&nbsp; &nbsp; 41353 segments received\n&nbsp; &nbsp; 30924 segments sent out\n&nbsp; &nbsp; 0 segments retransmitted\n&nbsp; &nbsp; 0 bad segments received\n&nbsp; &nbsp; 0 resets sent\nUdp:\n&nbsp; &nbsp; 37 packets received\n&nbsp; &nbsp; 0 packets to unknown port received\n&nbsp; &nbsp; 0 packet receive errors\n&nbsp; &nbsp; 37 packets sent\n&nbsp; &nbsp; 0 receive buffer errors\n&nbsp; &nbsp; 0 send buffer errors\n......\n</code></pre><p>其中，跟丢包直接相关的是 <strong>segments retransmitted</strong> 这个指标，<strong>如果它一直在增长，那一般意味着网络上存在丢包</strong>，我们需要多加注意了。</p><p>当然，netstat命令是相对传统的命令，属于早已停止维护的net-tools工具集。新的工具集是iproute2，其中包括了ip、ss、nstat等命令。这些命令也可以读取到类似的网络统计信息。</p><p>不过，既然新老命令的功能类似，那为什么我们还要重复造这个轮子呢？</p><p>这其实是因为，netstat主要是通过/proc文件系统收集信息的，而ss主要通过netlink内核接口获取数据（有些信息也依然要从/proc中读取），这个netlink接口的效率要比/proc接口更高，所以ss能更快地返回数据。</p><p>另外，ss能获得的信息要比netstat更丰富。比如，ss就可以获取到socket option的信息，但netstat就做不到。</p><p>我们可以看一个例子。我在自己的Ubuntu容器里运行了这条命令：</p><pre><code class="language-bash">$ ss -eit\n</code></pre><p>就可以获取到很多netstat无法获取的信息：</p><p><img src="https://static001.geekbang.org/resource/image/07/8b/07c095c22467de0d73b982184c0db98b.jpg?wh=1464x244" alt="图片"></p><p>我们可以挑几个信息展开一下。</p><ul>\n<li><code>cubic</code>：这条TCP连接用的拥塞控制算法是cubic。</li>\n<li><code>wscale:2,7</code>：这条TCP连接的两端的Window Scale分别是2和7。</li>\n<li><code>cwnd:10</code>：这条TCP连接的拥塞窗口为10个MSS。</li>\n</ul><p>如果你感到好奇：“ss是如何获取到这么多有用的信息的呢？”其实这个也不难。一个简单的办法就是使用<a href="https://time.geekbang.org/column/article/493040">第21讲</a>我们介绍的strace，然后就能观察到ss -eit拿到这些信息的具体过程了。</p><blockquote>\n<p>其实，类似这样的细微的探究过程，都能不经意间不断深化我们的技术能力。</p>\n</blockquote><h4>nstat工具</h4><p>我们还可以了解一下nstat这个工具。它跟ss一样，也是iproute2工具包里面的。nstat的一个特点是，如果不加参数，<strong>它每次运行时输出的数值，是从上一次被执行以来这些计数器的变化值</strong>。这就带来了一个挺明显的优势：我们不用像使用netstat那样，在两次输出值中找出变化量了，nstat输出的直接就是变化量。</p><p>首次运行nstat时，输出的就是全部计数器的值。第二次运行时，就只是发生变化的数值了，比如下面的nstat输出的就是变化的值：</p><pre><code class="language-bash">root@08d984197cfb:/# nstat\n#kernel\nIpInReceives&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 5&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.0\nIpInDelivers&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 5&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.0\nIpOutRequests&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;4&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.0\nTcpEstabResets&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 1&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.0\nTcpInSegs&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;5&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.0\nTcpOutSegs&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 4&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.0\nTcpOutRsts&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 3&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.0\nIpExtInOctets&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;336&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.0\nIpExtOutOctets&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 160&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.0\nIpExtInNoECTPkts&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 5&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.0\n</code></pre><p>当然，要查看全部值也是可以的，执行这条命令即可：</p><pre><code class="language-bash">nstat -a\n</code></pre><p>nstat还有个功能是按JSON格式做输出，这有助于我们做自动化运维。做法是nstat命令加上<code>--json</code>参数：</p><pre><code class="language-bash">nstat --json\n</code></pre><p>而既然netstat和ss都能读取到计数器的值，那么<strong>这些计数器本身又是如何产生的呢？</strong></p><h4>内核的SNMP计数器</h4><p>这些计数器的统计和更新其实都是Linux内核的行为。内核根据<a href="https://datatracker.ietf.org/doc/html/rfc1213">RFC1213</a>的标准，定义了IP、ICMP、TCP、UDP等协议相关的<a href="https://www.kernel.org/doc/html/latest/networking/snmp_counter.html">SNMP计数器</a>。在每次报文发送、接收、重传等行为发生时，都会往相应的计数器中更新数值。具体来说，这些计数器的定义在内核代码的include/linux/snmp.h文件中。</p><blockquote>\n<p>补充：SNMP名义上是“简单网络管理协议”，其实名字叫“简单”的技术往往并不简单。事实上SNMP还是很庞大的，这里就不展开了。</p>\n</blockquote><p>所以说，这些数值都不是netstat、nstat这些命令去“生成”的，而是内核早就准备好这些数据了，工具去完成读取就好了。</p><p><img src="https://static001.geekbang.org/resource/image/e7/b1/e77af0a0d27bfcf2a1790844bf4408b1.jpg?wh=2000x740" alt=""></p><p>好了，讨论完各种工具，接下来我们就进入丢包这个核心话题。</p><h2>如何确定丢包位置和丢包率？</h2><p>在丢包这个问题上，我们最关心的可能就是<strong>丢包在哪里、丢包率是多少</strong>这两个问题。这里我们重点使用的工具，是traceroute和mtr。</p><h3>traceroute和mtr的工作原理</h3><p>我们先说traceroute。其实在<a href="https://time.geekbang.org/column/article/477510">第1讲</a>，我就提到过traceroute的两种模式：UDP模式和ICMP模式。但是，<strong>traceroute为什么可以做到探测网络路径中的节点的呢？</strong>要知道，ping也是用ICMP，它咋就看不到中间节点呢？</p><p>你可以找一个工程师问这个问题：“traceroute是如何显示每一个节点的？”很多人可能都会卡住，或者会给一个似是而非的答案，比如说：“可能是有什么协议吧，可以展示每一个节点的”。</p><blockquote>\n<p>所以我们不要小看一些表面上不起眼的小技术点，也许里面藏着的，还正是你的盲区，也是我们可以提高的地方。</p>\n</blockquote><p>实际上，traceroute的工作原理，是巧妙地利用了IP报文的 <strong>TTL属性</strong>。具体过程是这样的：</p><ul>\n<li>发送端首次探测时把UDP源端口为33434，TTL设置为1。这个报文经过第一个路由器时TTL会被减1，也就是变为0，因此包被丢弃，这个路由器向源地址发回一个ICMP超时通知（ICMP Time Exceeded Message），于是第一跳就被探测出来了。</li>\n<li>发送端把下一次发送的包的UDP源端口也加一，变为33435，TTL也在原来的基础加一，变为2，这样就可以多前进一步。跟上面的步骤类似，第二跳也被探测出来了。</li>\n<li>发送端一直重复TTL加一和UDP源端口加一的操作，直到报文最终到达目的地，后者回复一个端口不可达的ICMP错误信息（ICMP Port Unreachable）。当源地址收到这个消息时，就停止traceroute。</li>\n</ul><blockquote>\n<p>补充：实际工作中，traceroute会批量发送多个不同TTL值的报文然后等待回应，这样效率更高。</p>\n</blockquote><p>当然，在探测过程中，任何一个探测报文如果在限定时间内没有收到，traceroute就会认为超时，在输出中这一跳就显示为一个星号。</p><p>另外，目的地也可能不回复ICMP Port unreachable，那么traceroute的输出就会出现从某一个位置开始的后续跳数全部是星号，直到你按Ctrl + C终止探测，或者是直到默认的64跳检测全部完成。对于这种情况，你可以<strong>加上-I参数</strong>，让traceroute用ICMP协议的echo request消息进行探测，一般来说，<strong>目的地总能对UDP和ICMP中的一种进行响应</strong>。</p><p>我们看一下原理示意图：</p><p><img src="https://static001.geekbang.org/resource/image/4a/89/4a7cf207be80c4f0c628f86766ed3889.jpg?wh=2000x1125" alt=""></p><p>Linux的traceroute命令默认用的就是UDP模式，而Windows的tracert默认用了ICMP模式。有时候我们会看到每一跳有2个或者3个节点IP，其实就表明这几个路径是启用了ECMP的，所以有多个节点会被选到。比如下面这个例子：</p><pre><code class="language-bash">$ traceroute www.ebay.com\ntraceroute to e9428.a.akamaiedge.net (23.45.61.92), 64 hops max\n&nbsp; 1&nbsp; &nbsp;10.0.2.2&nbsp; 0.308ms&nbsp; 0.192ms&nbsp; 0.353ms\n&nbsp; 2&nbsp; &nbsp;192.168.1.1&nbsp; 4.284ms&nbsp; 1.379ms&nbsp; 1.360ms\n&nbsp; 3&nbsp; &nbsp;100.65.0.1&nbsp; 5.916ms&nbsp; 5.464ms&nbsp; 5.520ms\n&nbsp; 4&nbsp; &nbsp;61.152.53.149&nbsp; 148.051ms&nbsp; 146.005ms&nbsp; 4.126ms\n&nbsp; 5&nbsp; &nbsp;61.152.25.2&nbsp; 16.216ms&nbsp; 61.152.25.101  14.618ms&nbsp; 144.713ms\n&nbsp; 6&nbsp; &nbsp;*&nbsp; 202.97.83.13&nbsp; 8.385ms&nbsp; *\n&nbsp; 7&nbsp; &nbsp;202.97.12.186&nbsp; 5.112ms&nbsp; 13.341ms&nbsp; 14.592ms\n&nbsp; 8&nbsp; &nbsp;202.97.41.142&nbsp; 49.747ms&nbsp; 54.797ms&nbsp; 202.97.41.130  58.370ms\n&nbsp; 9&nbsp; &nbsp;*&nbsp; *&nbsp; *\n&nbsp;10&nbsp; &nbsp;*&nbsp; *&nbsp; *\n&nbsp;11&nbsp; &nbsp;*&nbsp; *&nbsp; *\n&nbsp;12&nbsp; &nbsp;*&nbsp; *&nbsp; *\n&nbsp;13&nbsp; &nbsp;*&nbsp; *&nbsp; *\n</code></pre><p>当你改成用ICMP模式后，每一跳就变成只有一个IP了。原因我们在上一讲讨论过，就是因为五元组是一致的，所以路径也不变。比如我们还是对www.ebay.com进行探测，这次用ICMP模式，每跳就只有一个IP了，而且目的地也返回了有效的响应。</p><pre><code class="language-bash">$ traceroute -I www.ebay.com\ntraceroute to e9428.a.akamaiedge.net (23.45.61.92), 64 hops max\n&nbsp; 1&nbsp; &nbsp;10.0.2.2&nbsp; 0.003ms&nbsp; 0.002ms&nbsp; 0.003ms\n&nbsp; 2&nbsp; &nbsp;192.168.1.1&nbsp; 7.616ms&nbsp; 1.315ms&nbsp; 6.206ms\n&nbsp; 3&nbsp; &nbsp;*&nbsp; 100.65.0.1&nbsp; 11.057ms&nbsp; 7.746ms\n&nbsp; 4&nbsp; &nbsp;*&nbsp; 61.152.53.149&nbsp; 18.436ms&nbsp; 9.598ms\n&nbsp; 5&nbsp; &nbsp;*&nbsp; 61.152.25.158&nbsp; 12.918ms&nbsp; *\n&nbsp; 6&nbsp; &nbsp;*&nbsp; *&nbsp; *\n&nbsp; 7&nbsp; &nbsp;*&nbsp; *&nbsp; *\n&nbsp; 8&nbsp; &nbsp;202.97.41.142&nbsp; 50.746ms&nbsp; 58.052ms&nbsp; 59.546ms\n&nbsp; 9&nbsp; &nbsp;203.215.237.22&nbsp; 62.385ms&nbsp; 66.607ms&nbsp; 54.561ms\n&nbsp;10&nbsp; &nbsp;23.45.61.92&nbsp; 49.609ms&nbsp; 46.586ms&nbsp; 45.567ms\n</code></pre><h3>用mtr定位丢包点</h3><p>在上一讲的案例中，我们是用nc加上-p参数指定多个源端口来测试，定位到了问题路径的存在。这对于问题节点丢包率比较高时，作用比较明显。</p><p>但是，当问题节点的丢包率不高的时候，我们用nc做多次测试，未必正巧能抓到现场。我们最好还要有<strong>持续监控</strong>的手段，运行一段时间或者发送一定数量（比如1000次以上）的报文，通过足够的探测数，来抓到偶发的故障现象。</p><p>这里呢，我们就要用到预习篇提到过的工具：<strong>mtr</strong>。mtr相当于traceroute的加强版，特别是它可以指定任意次数的探测并生成详细的探测报告，而traceroute只能对每个节点发起3次探测，数据量就不够了。</p><blockquote>\n<p>你也可以理解为mtr = traceroute + ping，因为ping也可以发送很多次，也有最终的报告。</p>\n</blockquote><p>比如，我们用下面这条命令，就可以对目标IP完成指定次数的探测并生成详细的报告，报告里包含了每一跳的丢包率，很有帮助。</p><pre><code class="language-bash">$ mtr -c 10 -r 8.8.8.8\nStart: 2022-03-27T00:44:36+0000\nHOST: victorebpf&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Loss%&nbsp; &nbsp;Snt&nbsp; &nbsp;Last&nbsp; &nbsp;Avg&nbsp; Best&nbsp; Wrst StDev\n&nbsp; 1.|-- _gateway&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0.0%&nbsp; &nbsp; 10&nbsp; &nbsp; 0.2&nbsp; &nbsp;0.3&nbsp; &nbsp;0.2&nbsp; &nbsp;0.5&nbsp; &nbsp;0.1\n&nbsp; 2.|-- 192.168.1.1&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.0%&nbsp; &nbsp; 10&nbsp; &nbsp; 1.9&nbsp; 14.9&nbsp; &nbsp;1.6 119.8&nbsp; 37.0\n&nbsp; 3.|-- 100.65.0.1&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0.0%&nbsp; &nbsp; 10&nbsp; &nbsp;11.9&nbsp; 27.8&nbsp; &nbsp;6.6 157.0&nbsp; 46.6\n&nbsp; 4.|-- 61.152.53.149&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;20.0%&nbsp; &nbsp; 10&nbsp; &nbsp; 4.3&nbsp; 29.9&nbsp; &nbsp;3.2 128.5&nbsp; 45.4\n&nbsp; 5.|-- 61.152.24.226&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;20.0%&nbsp; &nbsp; 10&nbsp; &nbsp; 4.0&nbsp; 10.1&nbsp; &nbsp;3.9&nbsp; 44.2&nbsp; 13.8\n&nbsp; 6.|-- 202.97.94.237&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;10.0%&nbsp; &nbsp; 10&nbsp; &nbsp;28.1&nbsp; 41.8&nbsp; 22.2 157.4&nbsp; 43.5\n&nbsp; 7.|-- 202.97.12.182&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;60.0%&nbsp; &nbsp; 10&nbsp; &nbsp; 4.9&nbsp; 27.9&nbsp; &nbsp;4.9&nbsp; 57.5&nbsp; 25.7\n&nbsp; 8.|-- 202.97.93.158&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;10.0%&nbsp; &nbsp; 10&nbsp; &nbsp;89.4 111.8&nbsp; 80.8 217.6&nbsp; 55.0\n&nbsp; 9.|-- 72.14.211.144&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;20.0%&nbsp; &nbsp; 10&nbsp; &nbsp;79.7 107.9&nbsp; 79.6 231.9&nbsp; 51.6\n&nbsp;10.|-- 108.170.240.225&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;10.0%&nbsp; &nbsp; 10&nbsp; &nbsp;89.8&nbsp; 91.5&nbsp; 81.0 148.2&nbsp; 21.5\n&nbsp;11.|-- 74.125.251.207&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0.0%&nbsp; &nbsp; 10&nbsp; &nbsp;87.7&nbsp; 95.4&nbsp; 81.0 193.3&nbsp; 34.5\n&nbsp;12.|-- dns.google&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 10.0%&nbsp; &nbsp; 10&nbsp; &nbsp;81.0 106.2&nbsp; 80.1 286.9&nbsp; 67.9\n</code></pre><p>可见，终点处的丢包率为10.0%。不过，你也可能注意到了：为什么中间有好几个节点的丢包达到了20.0%甚至60.0%，但终点丢包率反而更低？这是怎么回事？</p><p><img src="https://static001.geekbang.org/resource/image/f8/8c/f8555257dbfbe05d766a9199cefc2c8c.jpg?wh=629x268" alt="图片"></p><h3>为何中间节点丢包率更高？</h3><p>首先，终点的丢包率（在这里是10.0%）肯定是正确的，因为这就是ICMP报文到达终点后的情况。按常规的理解，中间节点的丢包率应该比终点的丢包率低或者持平，但第4跳的20.0%和第7跳的60.0%，都比终点丢包率更高，这就令人费解了。另外，Last等列的响应耗时指标，也出现了中间节点和终点倒挂的情况。</p><p>其实，这个问题需要我们结合两个知识点来理解：</p><ul>\n<li><strong>mtr探测路径节点的工作机制；</strong></li>\n<li><strong>网络设备回复ICMP报文的工作机制。</strong></li>\n</ul><p>第一个知识点，mtr跟traceroute一样，也是通过递增TTL的方式，使得ICMP或者UDP报文终止在中间节点，从而获取到这些节点回复的ICMP time exceeded消息。也正是通过这个消息，我们知道了这个节点的IP、时延，以及我们关心的丢包率。</p><p>第二个知识点，是网络设备在回复ICMP报文时候的工作机制。其实，网络设备（交换机和路由器）在“转发”和 “回复”这两个任务上的工作机制是不同的：</p><ul>\n<li>对于“转发”，大部分工作是卸载到数据面的硬件芯片完成的，这也是实现高速转发的底层基础。</li>\n<li>对于“回复”，因为需要自己生成一个ICMP响应报文，那就需要动用自己的CPU资源了，速度就会慢一些。</li>\n</ul><p>我们可以通过下面这个图来理解这个知识点：</p><p><img src="https://static001.geekbang.org/resource/image/5c/da/5cca38e739b33185140f6cca05366fda.jpg?wh=2000x1006" alt=""></p><p>另外还有一些原因会导致这个现象，比如，<strong>不少网络设备对自己的ICMP响应报文设置了限速</strong>（rate limit），这也会加剧这种中间节点丢包的情况。</p><h3>如何解读中间节点的丢包率？</h3><p>那么，这个中间节点的丢包率是不是就毫无价值了呢？我们再看另外一个例子：</p><p><img src="https://static001.geekbang.org/resource/image/55/20/5538d1517190bdf2cbe1fc73b7e97120.jpg?wh=810x271" alt="图片"></p><p>我们从这里可以解读出很多的信息，包括：</p><ul>\n<li>从第5跳开始到最后一跳都有丢包。其中的第8跳到最后一跳的丢包率接近（都在20%左右），那么这几跳需要重点关注。</li>\n<li>从第9到第11跳的节点名称解析来看，这些是日本东京（tokyjp）的路由节点，运营商是NTT。</li>\n<li>第11跳的名称里带fastly，这是一个CDN厂商的名称，所以这是fastly CDN的节点。</li>\n<li>Last这一列是最近一次响应耗时，第7和第8跳的耗时突增了70ms。我们知道，时延跟地理距离成正比，而70ms是一个比较长的耗时了，所以我们可以判断，这两跳就是中日之间的海底光缆的两端。</li>\n</ul><p>根据这些信息，我们大致可以认为，丢包（本质是链路拥塞）主要发生在中日海底光缆这个位置。</p><p>当然，这是一个粗略的推断。<strong>一般来说，mtr最好在两端都做</strong>，然后结合两边的mtr的报告来综合分析，结论会更加准确。</p><p>为什么要在两端都做呢？这是因为，网络路由一般是“不对称的”，也就是发送是一条路径，回来往往是另外一条路径了。那么从一端发起mtr，只能看到这个方向的网络状况，另外一个方向的网络状况，就需要另外一端运行mtr来获取了。</p><p>比如我们借用一下上一讲的示意图。假设蓝色路径是起点到终点的路径，而紫色的是返回的路径，那么两条路径在r1，r3，r14这节点上是重合的，而在其他节点上“各走各的”。</p><p><img src="https://static001.geekbang.org/resource/image/f8/90/f8f50362fyyeabbb5be8c6ca056e7590.jpg?wh=2000x1125" alt=""></p><p>你看，这跟我们抓包经常要在两端同时抓，背后的逻辑是不是差不多的？大道相通，我们积累得越多，越有可能打通这些知识之间的，达成更深的掌握。</p><h2>丢包与MTU</h2><p>在<a href="https://time.geekbang.org/column/article/484667">第8讲</a>里，我们学习了MTU相关的概念。MTU是最大传输单元，而PMTU就是路径上的瓶颈MTU（最小MTU）。一旦报文设置了DF=1并超出PMTU的大小，就会被丢弃。特别是对于<strong>相对较大尺寸的TCP报文（比如超过1400字节）总是传输失败的情况，可以优先排查PMTU</strong>。</p><p>那么，我们有什么方法可以很方便地就找到PMTU呢？</p><h3>快速探测PMTU的方法</h3><p>其实，我们可以用一个十分简单的命令：ping。给 <strong>ping命令加上“-s 尺寸”</strong>这个参数，就可以发送自定义载荷尺寸的报文。比如你可以试试这个命令：</p><pre><code class="language-bash">ping www.baidu.com -s 1472\n</code></pre><p>然后再运行：</p><pre><code class="language-bash">ping www.baidu.com -s 1473\n</code></pre><p>看看两者的返回有没有区别？</p><p>ping报文属于网络层的ICMP控制报文，所以整体的IP报文大小是载荷（1472或者1473）加上IP头部的20字节和ICMP头部的8字节。显然，指定IP报文载荷为1473后，整个IP报文就达到1501字节，正好超过了1500字节，所以 <code>ping www.baidu.com -s 1473</code> 就在发送的途中丢失，也就无法收到ICMP响应了。</p><p>不过，也许你的网络环境跟我不同，特别是如果有隧道的存在，那-s 1472也无法通过。那么你可以不断调整这个值，直到试出一个刚刚能通过的尺寸，然后在它基础上加上28字节，就是你的网络环境的PMTU了。</p><blockquote>\n<p>补充：这里还隐含了另外一个知识点，就是IP分片。关于它的细节，你可以回顾第8讲的内容。</p>\n</blockquote><h2>如何统计丢包率？</h2><p>最后，我们来学习一下丢包率的统计方法。要统计丢包率，众所周知的方法应该就是用ping了，或者说“长ping”。比如下面的例子里，丢包率为1%：</p><pre><code class="language-bash">--- turner-tls.map.fastly.net ping statistics ---\n100 packets transmitted, 99 received, 1% packet loss, time 99204ms\nrtt min/avg/max/mdev = 122.012/134.749/301.225/27.648 ms\n</code></pre><p>不过ping还是有可能无法探测出网络的真实状况。其实在上一讲里，我们就发现，如果路径中有ECMP，那么因为ECMP哈希转发策略的存在，ping的网络路径可能就跟TCP的网络路径不同。这样可能就会造成这样的状况：</p><ul>\n<li>长ping没丢包的话，不等于应用也不丢包；</li>\n<li>长ping丢包的话，应用丢包的可能性也很大。</li>\n</ul><p>所以，既然ping也未必准，那<strong>不如直接一边跑应用一边抓包，然后对抓包文件进行分析</strong>，从而得出丢包率。而这里，又分了两种途径：图形界面和命令行。</p><h3>图形界面方法</h3><p>在Wireshark中我们能看到TCP retransmission、Out-of-Order等信息提示，那么我们就可以借助这些信息，计算出丢包率来。</p><p>我们看一个抓包文件的Expert Information：</p><p><img src="https://static001.geekbang.org/resource/image/d1/83/d13cd47bf1209eb8e007921b71d4fc83.jpg?wh=1804x326" alt="图片"></p><p>在<a href="https://time.geekbang.org/column/article/480068">第4讲</a>中，我详细介绍过解读Expert Information的方法，这里就不赘述了，相信你也已经比较熟悉了。这里我们最关心的指标是retransmission，它有100个。</p><p>这些数量算多吗？没有基数就不好说了，所以让我们看一下整体包量。打开Wireshark的Statistics下拉菜单，选中第一个选项即Capture File Properties：</p><p><img src="https://static001.geekbang.org/resource/image/74/04/74450c5ce46afe58c42f485d03ec3604.png?wh=512x162" alt="图片"></p><p>在弹出的界面中我们就能看到具体的数据了，比如这个抓包的整体包量是750个。</p><p><img src="https://static001.geekbang.org/resource/image/22/6a/22ac21a51273525d01421646037b9a6a.jpg?wh=1564x374" alt="图片"></p><p>然后<strong>丢包率可以大致认为是重传率，也就是重传报文数/整体报文数</strong>。在这里就是100/750 = 13.3%。</p><blockquote>\n<p>补充：当然，为了方便讨论，这里略过了重复重传的报文被重复计数的情况。</p>\n</blockquote><h3>命令行方法</h3><p>另外一个办法是用 <strong>capinfos命令</strong>获取总的报文数：</p><pre><code class="language-plain">$ capinfos viaLB.pcap\nFile name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;viaLB.pcap\nFile type:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Wireshark/tcpdump/... - pcap\nFile encapsulation:&nbsp; Ethernet\nFile timestamp precision:&nbsp; microseconds (6)\nPacket size limit:&nbsp; &nbsp;file hdr: 65535 bytes\nPacket size limit:&nbsp; &nbsp;inferred: 84 bytes\nNumber of packets:&nbsp; &nbsp;750\nFile size:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;68 kB\nData size:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;315 kB\nCapture duration:&nbsp; &nbsp; 2.076944 seconds\nFirst packet time:&nbsp; &nbsp;2016-12-09 20:06:00.629223\nLast packet time:&nbsp; &nbsp; 2016-12-09 20:06:02.706167\nData byte rate:&nbsp; &nbsp; &nbsp; 151 kBps\nData bit rate:&nbsp; &nbsp; &nbsp; &nbsp;1215 kbps\nAverage packet size: 420.67 bytes\nAverage packet rate: 361 packets/s\nSHA256:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 9c34dc15bcf69b419c0e3eb0c37fa851485adbe3953018b957b14de330fa0882\nRIPEMD160:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;c353fdd8d634dd38ac395980b4824751f667908f\nSHA1:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; fd86e4f6969cc8a27ff1a29cf2de88ab6d57db7d\nStrict time order:&nbsp; &nbsp;True\nNumber of interfaces in file: 1\nInterface #0 info:\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Encapsulation = Ethernet (1 - ether)\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Capture length = 65535\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Time precision = microseconds (6)\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Time ticks per second = 1000000\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Number of stat entries = 0\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Number of packets = 750\n</code></pre><p>结尾处就是总的报文数量，即Number of packets = 750。那么重传个数如何在命令行里获取呢？</p><p>你是否还记得之前我们学习过tshark这个命令？这里也是用 <strong>tshark</strong>，执行下面的命令：</p><pre><code class="language-bash">$ tshark -n -q -r viaLB.pcap -z "io,stat,0,tcp.analysis.retransmission"\n\n======================================\n| IO Statistics&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |\n|&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |\n| Duration: 2.077 secs&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;|\n| Interval: 2.077 secs&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;|\n|&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |\n| Col 1: tcp.analysis.retransmission |\n|------------------------------------|\n|&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |1&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | |\n| Interval&nbsp; &nbsp; &nbsp; &nbsp;| Frames |&nbsp; Bytes | |\n|----------------------------------| |\n| 0.000 &lt;&gt; 2.077 |&nbsp; &nbsp; 100 | 145400 | |\n======================================\n</code></pre><p>最后一行的Frames 100就是指重传报文个数100。同样的，把两个数字相除就得出了丢包率。</p><h2>丢包多少算严重？</h2><p>回到重传数量占整体比例的讨论。前面的例子里丢包率是13.3%。直觉上看，这个比例也高了。那如果降低一个数量级，比如1.33%，这算高还是低呢？</p><p>这个问题可能也没有标准答案，毕竟每个应用对于丢包和重传的敏感度也有不同。而且由于公网情况复杂，本身就有一定的丢包率存在，比如像前面的海底光缆拥塞造成的丢包，也难以避免。</p><p><strong>实际上，公网丢包率在1%左右是一个可以接受的范围</strong>。如果明显超过1%，比如达到了5%以上，那对应用的影响就会比较明显了，此时应该通过节点修复或者链路调整，来解决丢包的问题，把丢包率控制在1%左右，最好是1%以下。</p><p>而内网网络会比公网稳定很多。一般来说，<strong>一个正常的内网也有万分之一左右的丢包率</strong>。如果明显超过了这个比率，比如达到了千分之一的话，尽管依然比公网丢包率低一个数量级，但也需要认真对待并解决。</p><h2>小结</h2><p>这节课，我们一起探讨了丢包这个关键话题的工具、原理，还有方法。我们学习了多种工具，包括：</p><ul>\n<li>探测类工具：ping、mtr、traceroute、nc、telnet等。</li>\n<li>统计类工具：netstat、ss、nstat。</li>\n</ul><p>其中，netstat和nstat都会展示 <strong>TCP retransmission的数值，如果它随时间递增，那就说明这台主机的对外通信存在丢包的现象</strong>。我们也了解了Linux内核本身实现了SNMP计数器，这些计数器记录了系统启动后的各个网络指标的数值，而netstat等工具正是读取内核中这些计数器，获得了相应指标的数值。</p><p>另外我们也要清楚一点，ss能提供比netstat更多的网络信息，特别是TCP socket的各种属性。这些信息对TCP方面的排查工作很有帮助，比如运行：</p><pre><code class="language-bash">ss -iet\n</code></pre><p>而对于路径排查的重要工具 <strong>mtr</strong> 的原理和使用方法，你也要好好掌握。在mtr的输出中，要注意中间节点的丢包率。如果终点的丢包率比前面节点的低，那前面节点的丢包率只能作为有限的参考。如果节点丢包率出现越往后越高的情况，这样的丢包率的参考价值就高很多了。</p><p>而且，最好<strong>在通信两端都做mtr，然后结合两边的探测结果做综合分析</strong>。</p><p>另外，在丢包这个主题中，<strong>MTU</strong> 也是重要角色。因为路径MTU的不一致，某些情况下就会发生超出MTU而丢包的现象。由于ICMP消息不一定会回到发送方，就会导致发送方的PMTU机制不能正常工作，也就是无法感知到这个MTU超限的事实，导致连续发送失败。</p><p>一般我们会建议对自己网络情况了解清楚后，再对主机设置合适的MTU，这样可以最大程度上确保不发生MTU超限引发问题。另外呢，我们也可以<strong>用ping来探测PMTU</strong>，也就是运行：</p><pre><code class="language-bash">ping -s 1472   #或者其他数值\n</code></pre><p>最后，我们还学习了如何根据抓包文件，计算出丢包率，也就是<strong>丢包率=重传个数/总报文数</strong>。另外公网丢包率1%和内网丢包率万分之一，可以认为是正常范围。</p><h2>思考题</h2><p>最后，再给你留两道思考题：</p><ul>\n<li>Linux中运行ifconfig -a看到的网卡接口的dropped指标，是指我们这里讨论的丢包吗？为什么？</li>\n<li>TCP是如何在发生丢包的情况下保证传输可靠性的呢？</li>\n</ul><p>欢迎你在留言区分享你的答案，我们一同进步、成长。</p>',
        article_title: "24 | 丢包：如何确定丢包的存在及其程度？",
      },
    ],
  },
  {
    chapterTitle: "不定期加餐",
    children: [
      {
        title: "不定期加餐（一） | 八仙过海，各显神通：透传真实源IP的各种方法",
        id: 497864,
        content:
          '<p>你好，我是胜辉。这节加餐课，我们来聊聊透传真实源IP的各种方法。</p><p>在互联网世界里，真实源IP作为一个比较关键的信息，在很多场合里都会被服务端程序使用到。比如以下这几个场景：</p><ul>\n<li><strong>安全控制</strong>：服务端程序根据源IP进行验证，比如查看其是否在白名单中。使用IP验证，再结合TLS层面和应用层面的安全机制，就形成了连续几道安全门，可以说是越发坚固了。</li>\n<li><strong>进行日志记录</strong>：记下这个事务是从哪个源IP发起的，方便后期的问题排查和分析，乃至进行用户行为的大数据分析。比如根据源IP所在城市的用户的消费特点，制定针对性的商业策略。</li>\n<li><strong>进行客户个性化展现</strong>：根据源IP的地理位置的不同，展现出不同的页面。以eBay为例，如果判断到访问的源IP来自中国，那就给你展现一个海淘页面，而且还会根据中国客户的特点，贴心地给你推荐流行爆款。</li>\n</ul><p>虽然源IP信息有这么多用处，但是现实情况中，这个源IP信息还不是那么好拿。这个原因有很多，最主要的还是跟负载均衡（LB）的设计有关系。</p><p>一般来说，用户发起HTTP请求到网站VIP，VIP所在的LB会把请求转发给后端，一前一后分别有两个TCP连接。</p><ul>\n<li>前一个TCP连接的客户端IP是CIP，服务端IP是VIP。</li>\n<li>后一个TCP连接的客户端IP是LB的SNAT IP，服务端IP是SIP。</li>\n</ul><!-- [[[read_end]]] --><p>由此，我们可以得到以下示意图：</p><p><img src="https://static001.geekbang.org/resource/image/9b/51/9b6101727ca01dbdb2c16b67c1440251.jpg?wh=2000x722" alt=""></p><p>在这个过程中，LB把这两个表面上没有任何联系的TCP连接“映射”了起来，所以也只有LB知道，从哪个真实源IP（这里的CIP）来的请求被转发到了哪一个后端的连接上去了。</p><p>在这种设计之下，可怜的服务端（SIP）却<strong>只能看到LB的SNAT IP，对CIP是一无所知</strong>，就导致了上面说的好几个功能一个都用不上。</p><p>不过别急，我们有这么几种方法来解决这个难题。我会按网络层级来一一介绍，分别是应用层方法、传输层方法，还有网络层方法。</p><p>我们先看应用层方法。</p><h2>应用层方法</h2><p>在这一层，Web协议的制定者们想到了一个巧妙的办法：既然HTTP协议比较灵活，那就可以<strong>设计一个新的header，用来传递真实源IP，它就是X-Forwarded-For</strong>。这个标准最初是Squid的开发工程师提出的，很快受到了业界的支持，各种web服务器都早已支持了这个header。</p><blockquote>\n<p>补充：Squid是应用最为广泛的代理和缓存软件之一。</p>\n</blockquote><p>X-Forwarded-For的形式跟其他HTTP header一样，也是key: value的形式。key是X-Forwarded-For这个字符串，value是一个IP或者用逗号分隔开的多个IP，也就是下面这样：</p><pre><code class="language-bash">X-Forwarded-For: ip1,ip2,ip3\n</code></pre><p>那为什么会有多个IP的情形呢？因为一个HTTP请求，可能会被多个HTTP代理等系统转发，每一级代理都可能会把上一个代理的IP，附加到这个X-Forwarded-For头部的值里面。最左边的IP就是真实源IP，后面跟着的多个IP就是依次经过的各个代理或者LB的IP。</p><p>我们来看个例子。下面是截取的某个抓包文件的HTTP请求的部分，能看到X-Forwarded-For头部，它的值为真实源IP。同时也看到还有另外一个头部X-Forwarded-Proto，它的值为真实客户端跟这个代理之间通信的协议，此处为HTTP，当然也可以是HTTPS。</p><p><img src="https://static001.geekbang.org/resource/image/dc/8e/dc6f69f0f2dc03be38ec5f6135a8c58e.png?wh=754x76" alt="图片"></p><p>不过，X-Forwarded-For这个标准，虽然用一种相对低的成本解决了“服务器不能获取真实源IP”的问题，但它本身还是有一些不足的，我们来看一下。</p><ul>\n<li><strong>源IP信息的伪造问题</strong></li>\n</ul><p>这也是它最大的问题，因为这个头部本身没有任何安全保障机制，攻击者完全可以任意构造X-Forwarded-For信息来欺骗服务端。</p><p>比如，如果攻击者知道服务端对某个IP段来的请求进行特殊处理（比如会提供更大力度的优惠券），那么攻击者就可以在发送请求时候，构造一个X-Forwarded-For头部，它的值就是这个段内的某个IP。</p><p>当服务端收到请求时，认为X-Forwarded-For里排在最左边的IP是真实IP，而事实上这个是伪造出来的，所以可想而知，这个请求就可以获取它原本不应该得到的特权了。</p><ul>\n<li><strong>重复的X-Forwarded-For头部</strong></li>\n</ul><p>HTTP协议本身并不严格要求header是唯一的，所以有些情况下，HTTP请求可能会携带两个或者更多的X-Forwarded-For头部。</p><p>造成这个现象的原因是，某些代理或者LB并不是严格按照协议规定的，把IP附加到已有的X-Forwarded-For头部，而是自己另起一个X-Forwarded-For头部，那么这样就导致了重复的X-Forwarded-For。</p><p>对于服务端来说，在收到这种请求的时候，可能会导致信息识别上的错乱。比如某些服务端的逻辑是读取第一个X-Forwarded-For，而另外一些服务端程序可能是读取最后一个，并无定法。</p><ul>\n<li><strong>不能解决HTTP和邮件协议以外的真实源IP获取的需求</strong></li>\n</ul><p>X-Forwarded-For解决了HTTP的透传真实源IP的需求，但是事实上，很多应用并不是基于HTTP协议工作的，比如数据库、FTP、syslog等等，这些场景也需要“获取真实源IP”这个功能。但是前面说的<strong>X-Forwarded-For，只能为HTTP/邮件协议所用</strong>，那其他这么多协议和应用难道就成了没妈的孩子，永远不能获取到真实源IP了吗？</p><p>这时候，传输层的方法就上场了。</p><h2>传输层方法</h2><p>在传输层这一层，有不止一种办法可以实现真实源IP透传，让我来逐一介绍。</p><h3>TOA和TCP Options</h3><p>TOA全称是TCP Option Address，它是<strong>利用TCP Options的字段来承载真实源IP信息</strong>，这个是目前比较常见的第四层方案。不过，这并非是TCP标准所支持的，所以需要通信双方都进行改造。也就是：</p><ul>\n<li>对于发送方来说，需要有能力把真实源IP插入到TCP Options里面。</li>\n<li>对于接收方来说，需要有能力把TCP Options里面的IP地址读取出来。</li>\n</ul><p>这里，我们先来看一下TCP Options在TCP header里面的位置：</p><p><img src="https://static001.geekbang.org/resource/image/13/c9/134aa8498fda4d2a212eb58a7705a7c9.png?wh=1259x502" alt="图片"></p><blockquote>\n<p><a href="https://en.wikipedia.org/wiki/Transmission_Control_Protocol">图片来源</a></p>\n</blockquote><p>可见，TCP Options是可变长的，最长为40字节（第一列的偏移量20到60字节之差）。每个Option项由三部分组成：</p><ul>\n<li>op-kind；</li>\n<li>op-length；</li>\n<li>op-data。</li>\n</ul><p>TOA采用的kind是254，长度为6个字节（用于IPv4）。我们来看一下TOA的工作原理示意图：</p><p><img src="https://static001.geekbang.org/resource/image/b2/d6/b216a4941885e549ce69f07b932d93d6.jpg?wh=2000x671" alt=""></p><p>我们可以到Github上<a href="https://xn--19g">TOA的repo</a>了解到更多的实现细节。比如，我们可以看一下TOA源码中toa_data的数据结构：</p><p><img src="https://static001.geekbang.org/resource/image/73/73/73fa81424e7b50412225eef89334b273.png?wh=346x218" alt="图片"></p><p>可见，opcode（op-kind）是一个字节，opsize（op-length）是1个字节，端口（客户端的）是2个字节，ip地址是4个字节，也就是TOA传递了真实源IP和真实源端口的信息。</p><p>TOA具体的工作原理是，TOA模块hook了内核网络中的结构体inet_stream_ops的inet_getname函数，替换成了自定义函数。这个自定义函数会判断TCP header中是否有op-kind为254的部分。如果有，就取出其中的IP和端口值，作为返回值。</p><p>这样的话，当来自用户空间的程序调用getpeername()这个系统调用时，拿到的IP就不再是IP报文的源IP，而是TCP Options里面携带的真实源IP了。比如服务器加载TOA后（当然LB也要支持TOA），那么在access log里面的remote IP一列，就会是真实源IP；而不加载TOA模块的话，就只是LB的SNAT IP了。</p><h3>Proxy Protocol</h3><p>这个方案是HAProxy（另外一个广泛应用的反向代理软件）工程师提出的。它的实现原理是这样的：</p><ul>\n<li>客户端在TCP握手完成之后，在应用层数据发送之前，插入一个包，这个包的payload就是真实源IP。也就是说，在三次握手后，第四个包不是应用层请求，而是一个包含了真实源IP信息的TCP包，这样应用层请求会延后一个包，从第五个包开始。</li>\n<li>服务端也需要支持Proxy Protocol，以此来识别三次握手后的这个额外的数据包，提取出真实源IP。</li>\n</ul><p>我们可以看一下它具体的工作原理：</p><p><img src="https://static001.geekbang.org/resource/image/a8/cf/a87712ec596bb9d0a34a9fb44yy918cf.jpg?wh=2000x1125" alt=""></p><p>那么目前，除了HAProxy以外，其实也有不少软件已经支持了Proxy Protocol，比如Nginx，以及各大公有云的服务，比如AWS（亚马逊云）和GCP（谷歌云）。我们还是拿鲜活的抓包信息来展示一下。测试环境是：client -&gt; HAProxy (enabled with proxy protocol as proxy) -&gt; nginx (enabled with proxy protocol as server)。</p><p><img src="https://static001.geekbang.org/resource/image/cc/d3/cc09cac24dbbcc8643c140c1034627d3.jpg?wh=2000x345" alt=""></p><p>首先，我们从客户端发起HTTP请求，然后在HAProxy上抓包，获取信息如下：</p><p><img src="https://static001.geekbang.org/resource/image/7c/0a/7c204094338a08564669f0a33f0fb30a.png?wh=1864x1284" alt="图片"></p><p>可见，整个抓包文件中第9个包（也就是服务端连接的第四个包），就是那个关键的携带了真实源IP信息的包，我们可以直接在Wireshark下方的报文详情里看到它的文本格式的内容：</p><pre><code class="language-bash">PROXY TCP 10.0.2.2 10.0.2.15 51866 80\n</code></pre><p>其中，10.0.2.2就是真实源IP，10.0.2.15是VIP，51866是真实源端口，80是VIP端口。</p><p>而这里你要知道，默认的HAProxy和Nginx配置都是不启用Proxy Protocol的，所以需要额外进行这些配置。</p><p>另外，如果中间LB（这个例子里是HAProxy）启用了Proxy Protocol，而后端服务器（这个例子里是Nginx）没启用，那么客户端会收到HTTP 400 bad request。究其原因，是因为不启用Proxy Protocol的Nginx，会认为握手后的第一个包并没有遵循HTTP协议规范，所以给出了HTTP 400的报错回复。</p><p><img src="https://static001.geekbang.org/resource/image/23/76/237bfc77513710f7b7e06cec5bff3876.jpg?wh=2000x374" alt=""></p><h3>NetScaler的TCP IP header</h3><p>这是Citrix（也就是NetScaler的厂商）提供的自家的方案。它的原理跟Proxy Protocol是类似的，也是在握手之后，立即发送一个包含真实源IP信息的TCP包，而差别仅仅在于<strong>数据格式不同</strong>。也就是说，这个方式的原理也可以借用Proxy Protocol的那张图来说明：</p><p><img src="https://static001.geekbang.org/resource/image/0f/11/0f11c82b347902868af12312d737e811.jpg?wh=2000x1125" alt=""></p><p>然后，后端服务器也需要进行适当改造以支持这个行为，也就是需要读取相应字段，提取出源IP信息。</p><p>我们可以来看一下<a href="https://support.citrix.com/article/CTX205670">Citrix官网文档</a>中的例子：</p><p><img src="https://static001.geekbang.org/resource/image/eb/9c/eb64f4043cc1f8998dc51cabf289749c.png?wh=491x103" alt="图片"></p><p>可见，在握手的三个包之后，第四个包里面包含了真实源IP信息。也就是图中黄色高亮的部分：0a 67 06 1e。换算成十进制就是10.103.6.30。</p><p>这种算是私有协议了，支持场景会比Proxy Protocol更少一些，所以需要服务端开发人员对此进行代码改造，来让应用程序能够识别这个包里面的信息。</p><h2>网络层方法</h2><p>不过，既然事关IP信息的传递，怎么IP层自己反而没有办法呢？事实上，在这一层确实也有办法，比如利用IPIP这样的隧道技术。简单来说，就是<strong>用“三角模式”来实现直接的源IP信息的透传</strong>。但它的实现原理，跟前面介绍的几个就有比较明显的区别了。</p><ul>\n<li>传输层和应用层：把真实源IP当做header的一部分，传输到后端。</li>\n<li>网络层：直接把真实源IP传输到后端。</li>\n</ul><p>让我们看一下三角模式示意图：</p><p><img src="https://static001.geekbang.org/resource/image/80/4c/80b810923431ac3c307e1b6392c8874c.jpg?wh=1967x827" alt=""></p><p>具体的IPIP隧道加三角模式的配置细节网上很容易搜到，这里就不赘述了。显而易见，这种模式里，客户端地址（CIP）是被服务端直接可见的，看起来貌似最为直接，也不需要任何应用层和传输层的改造。</p><p>不过，这种方式的缺点也比较明显。</p><ul>\n<li><strong>配置繁琐，扩展性不佳</strong>：IPIP隧道（或者其他隧道技术）需要在LB和服务端都进行配置，VIP也需要在服务端上配置。我们知道，步骤越多，出错概率就越大，在系统架构选型的时候，我们要注意控制这些变量的数目，使得系统易于维护。</li>\n<li><strong>LB无法处理回包</strong>：因为回包不再经过LB，那么对应用回复的处理就无从实现了，比如对HTTP Response的改写，就没办法在LB环节做了。如果需要有这些逻辑，那么我们要把这部分逻辑回撤到服务器本身来处理。</li>\n</ul><blockquote>\n<p>补充：当然如果LB跟后端服务器在同一个二层网络里，可以把LB配置为服务器的网关，使得HTTP响应报文也经过LB，不过这个前提条件相对苛刻。</p>\n</blockquote><h2>小结</h2><p>这节课，我们主要学习了几种透传真实源IP的方法。其中，应用层透传真实源IP的方法，是利用X-Forwarded-For这个头部，把真实源IP传递给后端服务器。这个场景对HTTP应用有效，但是对其他应用就不行了，所以还要看另外两大类方法。</p><p>那么，针对传输层主要是有三种方法：</p><ul>\n<li>扩展SYN报文的<strong>TCP Options</strong>，让它携带真实源IP信息。这个需要对中间的LB和后端服务器都进行小幅的配置改造。</li>\n<li>利用<strong>Proxy Protocol</strong>。这是一个逐步被各种反向代理和HTTP Server软件接纳的方案，可以在不改动代码或者内核配置的情况下，只修改反向代理和HTTP Server软件的配置就能做到。</li>\n<li>利用<strong>特定厂商的方案</strong>，如果你用的也是NetScaler，可以利用它的相关特性来实现TCP层面的真实源IP透传。不过这也需要你修改应用代码来读取这个信息。</li>\n</ul><p>而在网络层，我们可以用<strong>隧道+DSR模式</strong>的方法，让真实源IP直接跟服务端“对话”。这个方案的配置稍多，另外LB也可能无法处理返回报文，所以你需要评估自己的需求后再决定是否采用这一方案。</p><p>最后，学完了这节课，你也要清楚，在实际的工作中，其实并没有一个普适于一切场景的获取真实源IP的方案，而是<strong>应该根据不同的需求和基础架构特点，来选取最适合自己的那一个</strong>。我想这个原则，无论对于获取真实源IP这个场景，还是其他任何技术选型，都应该是我们遵守的法则。就算是衣服的均码，也有人穿着不合身呢。要想展现你的身材，恐怕只有量身定做，才最为靓丽。当然，前提是你知道这些选项的存在。</p><h2>思考题</h2><p>今天的加餐就到这里，最后也给你留一道思考题：假设你的应用是一个自己开发的基于TCP的应用，部署在LB后面，那你会选择用上面介绍的那种方法来透传真实源IP信息呢？</p><p>欢迎在留言区分享你的答案，也欢迎你把今天的内容分享给更多的朋友。</p>',
        article_title:
          "不定期加餐（一） | 八仙过海，各显神通：透传真实源IP的各种方法",
      },
      {
        title: "用户故事 | 小S：学习是人生路上生生不息的活泉",
        id: 500377,
        content:
          "<p>你好，我是小S，来自北方二线城市。我做了七八年的开发，最近一份工作是做安防音视频流媒体方向，其中部分工作内容是负责实际项目的对接任务，所以会涉及到很多的网络相关的问题。</p><h2>我与极客时间的不解之缘</h2><p>我是非科班出身的，最早只是在图书馆借借书看，后来从事了开发工作之后，我就开始买网课学习，可以说，我的整个计算机知识体系都是通过网课+图书构建起来的。</p><p>不过那个时候，我在学习的过程中就发现，这些资料要么是大部头的书、要么是很长的视频，学起来不是很方便，很少有大段的时间能专门坐下来看长视频并仔细分析、记笔记，而且也没太多时间去搜集额外的资料。还有一个问题就是这些资料很多都是良莠不齐、鱼龙混杂的，早期我还会在一些音频App上找，但因为这些App并不是专门做技术内容，所以也不太合适。</p><p>机缘巧合下，我发现了极客时间这个平台，课程上新很快，内容也一直在更新迭代。虽说我大部分计算机基础课之前都补上了，但是现在有更上一层楼的机会，何乐而不为呢，毕竟这种音频+图文的讲解模式还是很适合充实基础知识的。</p><p>并且，极客时间的大部分课程也都是从实际应用的角度出发，拓宽了我们来自二三线以下城市开发者的视野。说实在话，小地方没什么大厂，小公司也不太能用得上新技术，但是我想，<strong>有追求的人是不会想困在一潭死水中的，而是会付诸实际行动去学习、去实践，这才是取得进步的活泉</strong>。</p><!-- [[[read_end]]] --><p>另外，我也比较喜欢极客时间划分的学习路径，像我这样非科班出身的技术人，其实打心底是想要把内功补上的，而即使是科班出身，我想大部分人内心深处也是向往着有一天把这些知识好好串一串吧。</p><h2>我的学习方法</h2><p>说说我用极客时间的方法，希望也能给你带来启发或共鸣。我在极客时间上初步规划了三个学习进阶方向：计算机基础知识、C++工程师（目前的方向）、Go工程师（希望以后兼顾的方向）。</p><p>首先就是<strong>常规刷课</strong>。比如平时通勤、下班休息后，甚至跑步时都可以听一听。然后我一般会在周日下午固定几个小时，用来整理当周学过的内容。当然，实践性强的内容这么学当然不行，需要动手做实验，为此我还买了一个阿里云ECS。另外有些课程老师会推荐一些延伸资料，我平时也会看一下。</p><p>再者就是<strong>善用搜索</strong>。极客时间的课程都是很体系化的，涵盖的技术范围也比较广泛，可以帮我们省去不少埋头苦苦摸索的时间，所以我推荐根据自己平时工作中用到的一些技术来进行扩展。</p><p>比如我之前的公司有做中台的打算，我就去看了《说透中台》这门课。看完后感觉公司并不适合做。这就像当年大学时候的数学建模一样，我分析完数据之后，得出的结论就是它是假设的，压根不成立。但是这样也没关系，因为我们能从中获取很多实践经验，也可以拓宽视野。</p><p>不过，我们也要时刻提醒自己，不要像刘姥姥进大观园，眉毛胡子一把抓，最后可能收效甚微，一定要把握好“量”和“度”。这个其实和健身跑步一个道理，得一步步来，毕竟我们总不能直接练习十项全能。</p><p>最后，还有一个方法，就是<strong>学习网络</strong>，我在工作中经常碰到很多奇奇怪怪的问题，而我一直觉得，网络排查不是一个网络工程师或者一个纯开发就能单独搞定的事，所以我就想深入做一下网络编程、网络原理、网络排查。所以自然而然地，我就开始学习胜辉老师的《网络排查案例课》，收获颇多。</p><h2>我是怎么学习这门课的？</h2><p>要我来说的话，这门《网络排查案例课》，广大运维同胞可太值得看了。毕竟，如果系统出了问题，我们总不能只会“重启大法”吧。胜辉老师讲解的各种案例真可谓是惊涛骇浪，如果这些我们都见过了，那后面我们还能害怕遇到啥问题呢。</p><p>我在工作中出的问题最多、用的最多的，其实还是UDP的问题，平时因为涉及到音视频传输，会遇到各种流媒体协议导致的传输问题。而TCP的问题相对少一些，不过我为了拓宽视野就来看胜辉老师的课了。胜辉老师真的是苦口婆心，手把手从基本的抓包入手，帮我们大大降低了学习门槛，即使是新手小白理解起来也没什么问题。而且后半部分的课程，也都是我现在项目最常用的内容，比如Nginx问题、HTTP状态码问题、重传问题、丢包问题、负载均衡等等。</p><p>在学习这门课的过程中，也让我在实际项目上有了新的收获。比如，我们之前项目碰到过下级服务和上级服务之间使用SIP协议交互（基于UDP的SIP）。下级服务给上级服务的消息一般正常，但是上级服务回复的消息会不定期消失。当时网络的同事说两级服务之间是双线，一条联通、一条电信。后来禁用了一条就恢复正常了。这和老师在第23讲当中说的情况就有相似之处，多链路时某些特有的链路出问题，导致了表面上看似无规则的出错，而我们根据详尽的链路分析，其实就会发现问题所在。</p><h2>写在最后</h2><p>实际上，对于一个日常要为项目排查问题的开发来说，<strong>学习新知识应该是一种必备素质和要求</strong>。现在的技术发展日新月异，很多公司的程序也从普通机房迁到了云上。那我们就只管使用程序吗？遇到问题只管让现场重启吗？其实不太可能。日常的抓包排查网络、看日志、查代码、查接口、分析问题，如果单靠工作上的点滴积累，恐怕会跟不上时代的变迁了。</p><p>实际项目出问题就要解决问题，但不是什么时候和我们配合的团队都是专业团队，也不是什么时候和我们配合的人都是负责任的人，也不是什么时候负责协调的人什么都懂。那么需要的就是，我们有更专业的知识和更丰富的经历。而丰富的知识和经历，我们在这里就可以找到答案。行百里者半九十，希望我们一起加油、一起学习、一起进步！</p>",
        article_title: "用户故事 | 小S：学习是人生路上生生不息的活泉",
      },
      {
        title: "用户故事 | 王未：网络排查能力是一名合格运维工程师的必备技能",
        id: 503087,
        content:
          '<p>你好，我是王未，目前在上海一家互联网公司从事运维工作，有7年左右的工作经验。</p><p>运维是一项非常杂乱、技术支持性要求比较高的工作，分类也非常多，比如有应用运维、网络运维、系统运维、安全运维、监控运维、运维开发等等。就以应用运维为例，我们需要对各个运维中间件、监控系统了解精通，同时也要对排查故障做到得心应手。</p><p>那么，<strong>作为一个运维工程师，维护系统稳定是天职，而这就需要我们能够在出现问题的时候，迅速解决故障，把损失降低到最小。</strong>至于如何排障，运维的基本功——网络排查，就是一名合格运维工程师的必备技能了。但往往，很多人在这方面掌握得并不是很好。</p><h2>为什么要学习这门课程？</h2><p>日常排查工作中遇到的问题，我们大多是通过日志给出来的报错，来顺藤摸瓜式地排查定位，可一旦遇到网络问题，我们可能就找不到方向了。网络世界太复杂，七层协议，每一层都有很多概念和原理需要理解精通，就比如：</p><ul>\n<li>网络层，有TTL、RTT、MTU、hops、Router等。</li>\n<li>传输层，包括TCP Sequence Number、AckNumber、MSS、Receive Window、Congestion Window，latency、DupAck、SACK、Retransmission、Packet loss等。</li>\n<li>应用层，有HTTP协议的header、body等。</li>\n</ul><!-- [[[read_end]]] --><p>而除了这些之外，我们还需要结合Linux的操作性，来设置合理的TCP层的内核参数等等，这样就使得我们对TCP协议的理解和掌握非常混乱，很难系统性地去学习。</p><p>我在刚入行时看过一本书，是机械工业出版社的<a href="https://book.douban.com/subject/1088054/">《TCP/IP详解 卷1：协议》</a>，来来回回看了几次，书里的理论知识比较多，但当时道行比较浅，工作实践比较少，另外也没有结合实战，所以理解起来比较费劲。</p><p>可以说，网络排查一直是我一知半解的领域，每次遇到类似的问题，我都很难灵活地去排查问题，对Wireshark的使用也非常不熟悉，那些数据包抓出来后在Wireshark里的展示，对我来说基本上就是“天书”了。</p><p>所以，胜辉老师的《网络排查案例课》在极客时间推出后，一下子就吸引了我，终于有个全面讲解TCP/IP协议、使用Wireshark的课程上线了。</p><p>我们知道，TCP/IP协议是运维知识体系的基石，就像房子的地基一样，必须要牢固。所以基于这个普遍认知，也让我对自己的网络排查能力有了一定的要求。就我个人而言，我想要通过这门课程，达成以下三个学习目标：</p><ul>\n<li>学习并理解二、三、四层的工作原理；</li>\n<li>系统地实战学习故障排查方法，积累经验；</li>\n<li>拓展自己的技术思维。</li>\n</ul><h2>学习课程的心得体会</h2><p>因为之前我对TCP/IP协议并不熟悉，没有全面、系统性地学习过，所以在解决问题方面并不灵通，对各种概念也是一知半解的。不过，胜辉老师的运维经验非常丰富，并且老师生动的讲解方式，也非常易于我们理解。随着课程的推进和学习，也让我对之前工作中遇到的一些网络排查问题的思路渐渐清晰了。</p><p>举个例子，Nginx 499的问题我之前在工作中也遇到过。我在公有云工作的时候，在某个区域的客户请求有一定比例的499，当时因为只是极个别的客户，也没去抓包，觉得反正是客户主动断开的，大概率就是网络不稳定导致客户端超时断开了，而且客户没有报障，也就不管了，最起码不是我们服务端的问题。不过，具体为什么会断开、为什么reset，其实并不了解。</p><p>现在想想，之前的工作态度其实会比较草率，没有做到刨根问底地去分析、解决问题，或许我当时深入去研究一下，早就能够提升自己的网络排查能力了。HTTP 400也是一样，也只是知道客户端请求格式不正确，具体HTTP的请求格式也并没有了然于胸，还只是非常肤浅地理解一些概念而已，知其然而不知其所以然。</p><p>还有<a href="https://time.geekbang.org/column/article/491674">第19讲</a>的案例中，关于TLS握手失败的问题，我在实际工作中也遇到过，但是很难解决。当时是不同服务器上的curl客户端，有些TLS失败，有些成功。那时候，我们的处理是将curl升级到最新版本才解决，但最后也并不了解问题根因在哪、为什么这样做就OK了，直到学习了课程才明白，原来这种故障大概率是因为Cipher Suite问题导致的。</p><h2>我的高效学习方法</h2><p>我想，能来这里学习的人，大多数都是希望提升自己的小伙伴，我也一样。所以这里，我也想分享一下我的学习方法，希望能给你带来一些参考。其实这些方法也很简单。</p><ul>\n<li><strong>温故而知新</strong></li>\n</ul><p>我一般会在课程更新的第一时间，先打开电脑学习一遍，用正常的倍速去听讲，遇到没理解的地方暂停，重复学习，同时拿着抓包文件对着Wireshark同步操作。</p><p>然后，我也会利用上班时的地铁通勤时间，或者晚上睡觉前的时间，用手机App学习。用手机主要是为了复习，因为这个时间很难高度集中注意力，比较适合复习刚刚学完的知识，尤其是像这种碎片化的知识，需要不停地复习加强记忆。</p><ul>\n<li><strong>做好笔记</strong></li>\n</ul><p>通过整理笔记，能帮我们做到高度归纳、快速梳理知识点，也非常有利于我们后期温习。</p><p>其实，做笔记这个方法对于我们在几个月甚至更长的时间后去复习内容非常有帮助，这样我们一般就不需要重新学习一次课程了。</p><ul>\n<li><strong>重视思考题，以及留言区其他小伙伴的提问</strong></li>\n</ul><p>毕竟，带着问题去学习会事半功倍。而且，留言也是输出的一种，而输出是学习的最佳路径。</p><p>在学到一些概念的时候，自己其实会有不清楚的地方，或者是一些看似自己已经明白的概念，如果用自己的话再说一次，也会发现其中的很多细节我其实根本没搞懂，我要不停地查资料，把不懂的地方搞明白，那么这时候的留言，当然就很有存在价值了，这样在充分搞清楚这些概念的前提下，再去学习课程也会更容易理解。理解，才能更好地掌握知识点。</p><p>另外，通过留言，我们还可以根据其他小伙伴的提问以及交流讨论，再次巩固所学知识，让不同的思想碰撞，从而更接近学习的本质。</p><h2>总结</h2><p>目前课程已经基本接近尾声，我也想再回过头来，总结一下自己的学习过程。</p><p>首先，很明显，整个课程学起来还是比较轻松的，因为之前对TCP/IP的了解比较模糊，而通过学习课程，我的网络排查能力有了很大的提升，以后遇到抓包问题不会怵了，基本上能够熟练使用Wireshark去分析了。</p><p>其次，课程里非常全面地介绍了各种情况下的网络问题，在后面的工作过程中再遇到网络问题，我想也不会超出课程里的案例情况。并且，老师介绍的各种排查方法和抓包技巧，也能让我在工作中及时、灵活地排障。</p><p>最后，我想说的是，无论是学习网络排查还是其他的运维知识，我们都必须要把基本功夯实好，<strong>多学多练</strong>才能提升自己的能力，坚持久了自然而然就水到渠成了，我们的技术能力就会像武侠的内功一样深不可测。</p>',
        article_title:
          "用户故事 | 王未：网络排查能力是一名合格运维工程师的必备技能",
      },
    ],
  },
  {
    chapterTitle: "总结篇",
    children: [
      {
        title: "25 | 抓包分析的回顾、拾遗，和提高",
        id: 501696,
        content:
          '<p>你好，我是胜辉。</p><p>在完成了三个实战模块之后，我们已经把三层以上，也就是网络层、传输层、应用层等的排查技术都比较完整地学习了一遍。在二十多节课里，我们学习了对TCP的各种行为，比如握手、挥手、拥塞、重传等行为进行排查的技巧，也对tcpdump和Wireshark进行了深度使用。</p><p>除了这些技巧以外，我们更是在TLS加解密和应用层HTTP协议等理论知识方面，做了不少深入的研究。我想，很多工具的使用技巧、案例的排查思路已经在我们的脑海中回荡，那么如果我们来一次系统性的总结和提高，对我们的学习效果一定是一种更大的促进。</p><p>所以在这节课里，我们就来对过去的二十多讲进行一次回顾、总结和提高吧。</p><h2>tcpdump和tshark等命令行工具的技巧</h2><p>tcpdump是我们用得最多的抓包工具，其中的基本技巧可以汇总为以下几个方面。</p><h3>tcpdump</h3><p>我们做抓包，一般都需要指定过滤条件，这样才能保证对系统的CPU、内存、硬盘等资源不产生过大的影响。这里说的过滤条件又分基础参数和真正的过滤条件，我们分别来看一下。</p><h4>基础参数</h4><p>这类基础参数可能不算是用来过滤报文本身的参数，它们一般用于指定抓包的网口、报文长度等等。我们来逐个看一下。</p><p>要限定网络接口，可以用-i参数。比如：</p><!-- [[[read_end]]] --><pre><code class="language-bash">tcpdump -i eth0\n</code></pre><blockquote>\n<p>补充：这里还有个小的注意点。当我们用 <code>-i any</code> 的时候，抓到的报文就不是标准以太网帧头格式了，这一点你需要知道一下。</p>\n</blockquote><p>如果我们要查看抓包过程中的详细信息，下面这三种参数任你选择：</p><pre><code class="language-bash">tcpdump -v         #提供TTL等信息\ntcpdump -vv        #提供更多信息\ntcpdump -vvv       #提供最详细的信息\n</code></pre><blockquote>\n<p>补充：如果你还是用 <code>-w file.pcap</code> 存入了文件，加上 <code>-v</code> 参数可以每10秒打印一次抓取的报文数量。</p>\n</blockquote><p>默认情况下，tcpdump会对IP进行反向域名查询等操作，这些DNS查询等行为可能影响抓包的效率。这时候就可以加上 <code>-n</code> 参数：</p><pre><code class="language-bash">tcpdump -n\n</code></pre><p>为了节约抓包成本，我们经常会指定要抓取的每个报文的长度。比如，假如TCP没有启用扩展头部，那么指定54字节即可抓取到从二层帧头部到TCP头部的这些信息了。</p><pre><code class="language-bash">tcpdump -s 54\n</code></pre><h4>tcpdump过滤条件</h4><p>抓包的过滤条件当然是重中之重的技巧了，这里我们再复习一下。</p><p>要限定IP，可以用下面的过滤器：</p><pre><code class="language-bash">tcpdump host 10.10.10.10\ntcpdump dst host 10.10.10.10\ntcpdump src host 10.10.10.11\n</code></pre><p>要限定端口，可以用这个：</p><pre><code class="language-bash">tcpdump port 22\n</code></pre><p>我们有时候也需要从已有的抓包文件中过滤出报文，然后存入一个新的文件。比如下面的例子：</p><pre><code class="language-bash">tcpdump -r file.pcap \'tcp[tcpflags] &amp; (tcp-rst) != 0\' -w rst.pcap\n</code></pre><h3>tshark</h3><p>tshark是一个跟随Wireshark一起安装进系统的工具，类似情况的工具还有editcap、mergecap、capinfos等等，算是Wireshark大礼包了。这些工具都有很强大的功能，这个大礼包真是物超所值。</p><p>我们先看tshark。在课程中，我们主要用tshark实现了三个不同的需求场景。</p><p>第一个场景是<strong>统计文件中的HTTP状态码</strong>。我们可以到<a href="https://gitee.com/steelvictor/network-analysis/tree/master/17">Gitee</a>里找到相关的抓包示例文件，然后运行下面的命令：</p><pre><code class="language-bash">tshark -r lesson17-in-shorten.pcap -T fields -e http.response.code | grep -v ^$ | sort | uniq -c | sort -r\n</code></pre><p>输出如下：</p><pre><code class="language-bash">2931 200\n&nbsp;704 502\n&nbsp;227 304\n&nbsp;141 400\n&nbsp; 45 301\n&nbsp; 41 302\n&nbsp; 16 408\n&nbsp; 14 403\n&nbsp; &nbsp;6 503\n&nbsp; &nbsp;6 404\n&nbsp; &nbsp;2 206\n</code></pre><p>可见，HTTP状态码都非常整体地统计展现出来了。所以我们做抓包分析，不仅要熟练掌握Wireshark，同时也应该多了解这些命令行工具，它们经常可以提供Wireshark以外的一种良好的补充。</p><p>第二个场景是<strong>解析文件中的HTTP事务的耗时</strong>。比如我们可以用下面的命令：</p><pre><code class="language-bash">tshark -r 文件名 -T fields -e http.time | grep -v ^$\n</code></pre><p>第三个场景，是在找到HTTP耗时最高的事务，然后找这个<strong>事务所在的整个TCP流的所有报文</strong>，我们可以用下面的命令：</p><pre><code class="language-bash">tshark -r 文件名 -T fields -e frame.number -e http.time -e tcp.stream | sort -k2 -r | head -1 | awk \'{print $3}\' | xargs -n 1 -I {} tshark -r captured.pcap -Y "tcp.stream eq {}"\n</code></pre><p>这个命令比较长，用到了多次管道符，特别是最后一个管道符后面，使用了 <code>tshark -Y "tcp.stream eq xx"</code>，而这个过滤器就是把某个特定的TCP流给完整地展示出来，这样的话我们不仅可以找到HTTP耗时最高的事务，而且这个事务的完整的TCP流里的报文都展示了出来，方便我们做进一步的分析。</p><p>tshark的功能有很多，比如它还可以查看TCP初始往返时间：</p><pre><code class="language-bash">tshark -r file.cap -T fields -e tcp.analysis.initial_rtt | grep -v ^$\n</code></pre><p>也可以统计TCP重传包的数量：</p><pre><code class="language-bash">tshark -n -q -r file.pcap -z "io,stat,0,tcp.analysis.retransmission"\n</code></pre><p>最后，tshark其实也经常用来直接抓包，这时候它跟tcpdump是差不多的。比如下面这样：</p><pre><code class="language-bash"> tshark -i "Wireless Network Connection" -w file.pcap host 123.45.66.77\n</code></pre><h3>capinfos</h3><p>这个工具主要是用来获取抓包文件的整体信息，比如我们使用下面这个命令，就获取到了抓包文件的报文数、文件大小、抓包时长等丰富的信息。</p><pre><code class="language-bash">$ capinfos lesson17-in-shorten.pcap\nFile name:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;lesson17-in-shorten.pcap\nFile type:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Wireshark/tcpdump/... - pcap\nFile encapsulation:&nbsp; Ethernet\nFile timestamp precision:&nbsp; microseconds (6)\nPacket size limit:&nbsp; &nbsp;file hdr: 65535 bytes\nPacket size limit:&nbsp; &nbsp;inferred: 150 bytes\nNumber of packets:&nbsp; &nbsp;88 k\nFile size:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;9769 kB\nData size:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;115 MB\nCapture duration:&nbsp; &nbsp; 297.564908 seconds\n......\n</code></pre><p>它的功能跟Wireshark里Statistics下拉菜单里的Capture File Properties是类似的：</p><p><img src="https://static001.geekbang.org/resource/image/1f/65/1ffa6ed19402ed396364c828b38bf065.png?wh=298x177" alt="图片"></p><h3>editcap</h3><p>有时候我们需要传递抓包文件，同时又不想透露应用层数据等敏感信息，那么一个简便的做法，就是直接修改抓包文件中每个报文的长度。前面说过，<code>tcpdump -s</code> 是可以指定抓包大小的，而即使文件已经生成了，我们还是可以<strong>通过editcap名把它改小</strong>。是不是挺方便的？</p><p>比如要把原始文件 <code>old.pcap</code> 的每个报文截短为54字节，可以执行下面的命令：</p><pre><code class="language-bash">editcap -s 54 old.pcap new.pcap\n</code></pre><p>另外，editcap也可以用来把TLS key文件跟抓包文件合并在一起，这样你在分享文件的时候，就不需要传送两个文件了，而是一个合并过的文件就可以。比如下面的命令：</p><pre><code class="language-bash">editcap --inject-secrets tls,key.log in.pcap out.pcap\n</code></pre><blockquote>\n<p>注意：命令中的 <code>tls</code> 跟后面的 <code>key.log</code> 文件名之间有一个逗号，虽然略有点奇怪，但这就是它的格式。</p>\n</blockquote><h2>Wireshark的技巧</h2><p>Wireshark可以算是我们课程中最闪亮的明珠了。不过很多同学在学习这门课之前，可能对Wireshark的了解很少，一打开文件就晕了，心里只有一个念头“不明觉厉”。当然，通过课程的学习，相信你对这方面已经积累了相当不错的经验。如果你现在看到Wireshark窗口甚至有“亲切”的感觉，那就对了，说明你有了长足的进步，已经真正跟Wireshark成为密友了。</p><p>这里我们再来简单回顾一下相关的技巧。</p><h3>Wireshark的解读技巧</h3><p>在解读抓包文件时，第一步一般可以查看Expert Information。这个信息的解读方法是这样的：</p><ul>\n<li>Warning条目的底色是黄色，意味着可能有问题，应重点关注。</li>\n<li>Note条目的底色是浅蓝色，是在允许范围内的小问题，也要关注。</li>\n<li>Chat条目的底色是正常蓝色，属于TCP/UDP的正常行为，比如这次通信里TCP握手和挥手分别有多少次等，可以作为参考。</li>\n<li>一般来说，乱序可以重点关注。</li>\n</ul><p>在解读完Expert Information之后，我们会找到可疑的报文，展开进一步的分析。此时我们用得最多的技巧，可能就是<strong>Follow TCP Stream</strong>了。我们可以选中一个报文后右单击，然后选择Follow，在二级菜单中选择TCP Stream，这样就来到了这个报文所在的TCP流。基本上每次做抓包分析都会用到这一步的操作。</p><p><img src="https://static001.geekbang.org/resource/image/7f/yy/7f82e0a848203641b591cbb7e1de76yy.png?wh=747x519" alt="图片"></p><p>另外，Wireshark默认展示的列经常不能满足我们的需求，所以我们也要学会<strong>添加自定义列</strong>的方法。简单来说，就是选择一个报文后，进入它的详情部分（界面下方），然后选中某一个属性，右单击，选择Apply as Column即可。比如下面这样：</p><p><img src="https://static001.geekbang.org/resource/image/5d/4a/5d3f3de2c419fd0ae2be8fc4085b754a.jpg?wh=819x416" alt="图片"></p><p>上图中，我们通过添加TTL自定义列，就可以让每个报文的TTL值都在主视图中展现，极大地方便了对这些TTL的比较。所以我们除了掌握协议知识以外，也要挖掘各类工具的使用技巧。所谓“工欲善其事，必先利其器”也。</p><p>另外，在排查中，<strong>时间</strong>这个信息也是十分重要的。根据场景的不同，我们需要不同的Time列的表示方式。比如：</p><ul>\n<li>当我们关注<strong>前后报文间的间隔时间</strong>时，我们会选择Delta time或者Delta time displayed。</li>\n<li>当我们关注<strong>跨度比较大的报文间隔</strong>时，最好选择Absolute time或者另外两个Absolute date的形式。</li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/bc/d9/bce1f5aa452d84c742dc6d9a119699d9.jpg?wh=602x195" alt="图片"></p><h3>对TCP提示的解读</h3><p>Wireshark很方便也很强大，但如果我们脱离了对网络协议的理解，那其实还是无从下手。比如我们做得最多的TCP排查，就需要结合自己对TCP的掌握才能做好解读。</p><p>举个例子。同样是Wireshark关于TCP窗口方面的提示，TCP Window Full跟TCP Zero Window都是什么含义，有什么差别呢？像这样的例子很多，如果你真正地从Wireshark给你的众多信息中获取了有效的线索，那你的排查能力必定有一个明显的提升了。</p><h4>TCP Window相关的信息</h4><p>就TCP Window Full而言，它的意思是<strong>一端的在途字节数等于另一端的接收窗口</strong>。Wireshark会根据发送出去的数据量和已经确认的数量，计算出在途字节数，然后跟另一端的接收窗口进行比较。如果两者相等，说明另一端的接收窗口已经被用满了。这个对于排查TCP传输速度相关的问题是很有帮助的。</p><p>然后是TCP Zero Window，这个比较简单，就是<strong>接收窗口为零</strong>。这是主动通告对端“自己的接收窗口已经为零，不要再发送数据了”。所以你能看出来它跟TCP Window Full的区别了吗？</p><p>它们的<strong>区别</strong>是：TCP Zero Window是一个体现在TCP报文里的信息，算是主动“投降”。而TCP Window Full不是在报文中的信息，它是Wireshark自己分析出来的结论。知道了两者的区别，相信你也知道在相应的场景下，该如何结合这种解读来帮助排查了。</p><h4>TCP传输状况信息</h4><p>关于TCP传输速度也是一个热门话题，我们在第9~11讲都做了比较深入的探讨。在Wireshark里，我们可以利用下面几个图表来帮助排查。</p><ul>\n<li>I/O Graph：这个可以看到IP报文级别的传输速度。</li>\n<li>TCP Stream Graphs：这里又分了几种子图表，分别是：\n<ul>\n<li>Time Sequence (Stevens)：查看TCP序列号随着时间的变化趋势。</li>\n<li>Throughput：查看传输数据量的变化趋势。</li>\n<li>Round Trip Time：查看传输中RTT的变化。</li>\n<li>Window Scaling：查看接收窗口在传输中的变化，还可以直观地找到TCP Window Full事件。</li>\n</ul>\n</li>\n</ul><p>你可以在Statistics下拉菜单中找到上面的这些工具。</p><p><img src="https://static001.geekbang.org/resource/image/e9/b3/e94dd4yydeb31d9f1f19511fdc1a3ab3.jpg?wh=457x101" alt="图片"></p><h3>Wireshark过滤器</h3><p>过滤器这一块也是很丰富且很有价值的内容。有时候，能否写出一个高效准确的过滤器，几乎可以当做是区分我们抓包分析水准的量尺，所以对过滤器再重视也不为过。下面我们来回顾几个典型的过滤器。</p><p>在<strong>匹配字符串</strong>的时候，我们可以有以下这样的好几种选择，分别在不同的分层上实现了文本过滤。</p><pre><code class="language-bash">tcp contains "abc"\nip contains "abc"\nhttp contains "abc"\n</code></pre><p>那如果要<strong>匹配二进制数据</strong>该如何做到呢？我们知道，每个字节是8位，Wireshark用两个十六进制数字表示一个字节，我们在每个字节之间加上破折号就可以了。比如下面这样也可以搜到带“abc”的报文：</p><pre><code class="language-bash">tcp.payload contains 61-62-63\n</code></pre><p>当我们要根据TCP报文的标志位进行过滤的时候，就可以用下面这种过滤器：</p><pre><code class="language-bash">tcp.flags.reset eq 1      #找到RST报文\ntcp.flags.syn eq 1        #找到握手的SYN报文\ntcp.flags.fin eq 1        #找到挥手的FIN报文\n</code></pre><p>我们还可以根据时间匹配来过滤报文，比如：</p><pre><code class="language-bash">frame.time &gt;="Mar 28, 2022 00:00:00"\n</code></pre><h4>tcp.analysis类过滤器</h4><p>此外，Wireshark的过滤器中还有一个非常重要的部分，是TCP分析器tcp.analysis，当你在Wiresahrk过滤器栏输入tcp.analysis后，就自动有很多的过滤器提示出来。这些过滤器经常能起到很大的帮助，因为它们大多不是直接的报文内容，而是从报文中推导出来的信息，已经包含了一定的分析价值。</p><p>比如下面这个过滤器，可以找到超过200ms才发回的确认报文：</p><pre><code class="language-bash">tcp.analysis.ack_rtt &gt;0.2 and tcp.len == 0\n</code></pre><p>当我们要找到所有的TCP Window Full的报文的时候，就可以输入：</p><pre><code class="language-bash">tcp.analysis.window_full\n</code></pre><p>更多的tcp.analysis过滤器你可以自己去Wireshark里摸索一下，相信你会有很大的收获。</p><h2>排查技巧</h2><p>其实，网络排查很多人都在做，也都会抓包，那为什么不是每个人都可以从抓包文件中分析出有效的信息来呢？可能关键就在于我课程里提到过的<strong>两大鸿沟</strong>。</p><ul>\n<li>应用现象跟网络现象之间的鸿沟：你可能看得懂应用层的日志，但是不知道网络上具体发生了什么。</li>\n<li>工具提示跟协议理解之间的鸿沟：你看得懂Wireshark、tcpdump这类工具的输出信息的含义，但就是无法真正地把它们跟你对协议的理解对应起来。</li>\n</ul><p>为了突破第一个鸿沟，我们需要熟悉抓包技术，知道什么时候抓包、抓哪些包，然后也有能力把抓包文件中的信息，跟应用层日志中的信息对应起来。</p><p>为了突破第二个鸿沟，我们需要深刻理解网络协议，知道TCP、IP、HTTP这些协议的行为和脾性，然后结合Wireshark的各种技巧，把网络协议跟实际现象结合起来，从而推进排查工作。</p><p>当然，填平这两个鸿沟只是我们的目标，而手段就是这里要讨论的排查技巧了。这里的内容也比较多，我选了几个典型场景和思路，希望对你有所启发。</p><h2>逐段测试法</h2><p>我们经常会遇到一个请求从开始到结束会经过很多环节的情况，比如：</p><blockquote>\n<p>客户端-&gt; 第四层LB -&gt; 第七层LB -&gt; 服务端</p>\n</blockquote><p>在这种时候，我们可以做逐段测试，比如从客户端直接访问服务端，这样可以很快地确定中间这两个环节是否跟问题有直接关系了。比如像下面这样：</p><p><img src="https://static001.geekbang.org/resource/image/5d/0e/5d65cc33e0dcac560610779f92c2070e.jpg?wh=2000x435" alt=""></p><h3>抓包分析思路</h3><p>在进入抓包分析阶段后，从经验性的排查思路这个角度来说，一般的抓包分析可以用我们在<a href="https://time.geekbang.org/column/article/481042">第5讲“定位防火墙（一）”</a>中提到过的步骤，也就是：</p><ul>\n<li>查看 Expert Information；</li>\n<li>重点关注可疑报文（比如Warining级别），追踪其整个TCP流；</li>\n<li>深入分析这个可疑TCP流的第二到四层的报文，再结合应用层表象，综合分析其根因；</li>\n<li>结合两侧的抓包文件，找到属于同一个TCP流的数据包，对比分析，得出结论。</li>\n</ul><p>下图就概括了这个过程，你可以来参考一下：</p><p><img src="https://static001.geekbang.org/resource/image/dd/ac/dd1958be82b3ea019ffef41ffbc120ac.jpg?wh=2000x360" alt=""></p><h3>整数值</h3><p>在有整数值出现的时候，我们需要提起精神，务必重点调查这个整数值背后的原因。因为一般来说，这往往意味着<strong>有人为的设置</strong>在里面，比如某种客户端超时设置。这些设置经常会跟问题有直接或间接的关系。</p><p>比如在<a href="https://time.geekbang.org/column/article/488979">第15讲</a>，我们就发现客户端有5秒超时的机制，并在Wireshark里体现了这一点。</p><p><img src="https://static001.geekbang.org/resource/image/ea/cb/eab1ef276008a85d53f6a222034b89cb.jpg?wh=827x221" alt="图片"></p><h3>偶发性问题</h3><p>对于偶发性问题，我们可以采用这样的策略：</p><blockquote>\n<p>预估-&gt;开始抓包和观察-&gt;问题重现-&gt;停止抓包-&gt;分析</p>\n</blockquote><p>下图概括了这个过程，供你参考：</p><p><img src="https://static001.geekbang.org/resource/image/2a/dd/2aa3b64b995d21037cb9323ccc57d9dd.jpg?wh=2000x1125" alt=""></p><h3>对比分析</h3><p>还有一个很关键的技术是“对比分析”。我们做抓包分析，经常要在多处抓包后做对比分析。比如以下两种典型场景：</p><ul>\n<li>同一个TCP流在客户端和服务端的抓包文件；</li>\n<li>LB前后不同TCP连接的抓包文件。</li>\n</ul><p>对于第一种场景，我们可以<strong>用报文本身的字段</strong>作为匹配条件，因为属于同一个TCP流，它们的这些属性字段值一定不会变化。比如用裸序列号（原始序列号），像下面的过滤器：</p><pre><code class="language-bash">tcp.seq_raw eq 123456\n</code></pre><p>找到同一个TCP流之后，就可以把两个Wireshark窗口都打开，放在一起进行比较。你可以回顾下第5讲的案例，参考我们是如何做对比分析的。</p><p><img src="https://static001.geekbang.org/resource/image/e8/94/e8c1561009a05c1c9523dbef2fd8bb94.png?wh=1303x303" alt="图片"></p><p>而对于第二种场景，TCP连接就是完全不同的了，所以无法像上面第一种场景那样，把TCP报文里的字段值作为关联映射的条件。我们一般可以<strong>寻找应用层的特征</strong>，比如某个uuid，然后运用Wireshark过滤器，在两侧不同TCP连接的抓包文件中，找到同样包含这个uuid的报文。比如这些过滤器：</p><pre><code class="language-bash">http contains "uuidxxxxx"\ntcp contains "uuidxxxxx"\nip contains "uuidxxxxx"\nframe contains "uuidxxxxx"\n</code></pre><h2>小结</h2><p>这节课，我们是对整个课程中，不少抓包分析的技术细节进行了回顾。我在<a href="https://time.geekbang.org/column/article/484358">春节特别放送（三）</a>里其实提到过，思维导图是一个很好的学习工具，所以我就汇总整理了专栏中的核心内容。因为内容也比较多，我分为了4个部分来分别展示，你可以通过它们快速找到你需要的知识点。</p><ul>\n<li><strong>排查技术</strong></li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/f0/c2/f034075fyybd5eccf3c61494dcf731c2.jpg?wh=4000x3615" alt=""></p><ul>\n<li><strong>协议</strong></li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/22/fe/22b139be4e6a642ee0792152938c22fe.jpg?wh=4000x2482" alt=""></p><ul>\n<li><strong>系统相关</strong></li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/c8/a3/c8d8628bdc9a7b5051b0bffddbb189a3.jpg?wh=4000x2493" alt=""></p><ul>\n<li><strong>抓包分析</strong></li>\n</ul><p><img src="https://static001.geekbang.org/resource/image/18/fb/18b39b2624bb23afd24f7fe52caf23fb.jpg?wh=4000x2022" alt=""></p><h2>思考题</h2><p>给你留两道思考题：</p><ul>\n<li>假设通信的一端发送了TCP Zero Window报文，并且后续没有更多新的报文过来，那么另一端是不是一直不能发送新的数据了呢？</li>\n<li>请写一个tshark命令，用来过滤出抓包文件中的DupAck。</li>\n</ul><p>欢迎你把答案分享到留言区，我们一同进步、成长。</p>',
        article_title: "25 | 抓包分析的回顾、拾遗，和提高",
      },
      {
        title: "结束语 | 珍惜握手，难说再见",
        id: 503613,
        content:
          '<p>你好，我是胜辉。</p><p>从1月12日咱们课程的开篇词上线，我和你也正式“握手”，建立连接了！到今天结束语，历经3个月的专栏就要进入“挥手”阶段说再见了，心中满是不舍与感恩。做完这个专栏对我来说，算是完成了一件不小的事情，投入很大，收获感也很高。跟随专栏一路走来的你也不容易，我相信你也一定有所收获，也有很多话想说。所以在这个特殊的时刻，我们就谈谈心，聊一些技术以外的话题。</p><p>在一开始，我想跟你聊一下“知行合一”。</p><h2>知行合一</h2><p>很多时候，当我们在学习和工作上发现不足和问题，经过自我的总结或者他人的指导，我们都能很快找到原因和方案，也知道要做什么来切实地提高自己。“这个道理我懂了”，这是我们经常说的话。</p><p>不过你有没有想过，对于每一个道理，你真的完全理解其中的含义、重要性和迫切性吗？还是说，仅仅觉得里面的逻辑是圆满自洽的，自己能够接受，但内心并没有真正彻底地认同呢？</p><p>比如“持续学习对我们很重要”这个道理，大部分人都会认同，但是真正能保持学习习惯的人就没有那么多了。然后其中一些人在工作数年之后，慢慢地就跟不上原先同一起点的同学和同事，自身的竞争力下降，容易引发职业危机。</p><p>这是为什么呢？不少人会这样回答：“我也想学习，可是我太忙了啊。”</p><!-- [[[read_end]]] --><p>其实，你有没有发现，如果我们真的认同学习是重要的，就真的会去学习。这就是知行合一的要义：“<strong>知而未行者，只是未知</strong>”。这也提醒我们，如果某件事你认为很重要但一直没做，那就要很小心了，很可能你内心并没有真正理解它的重要性，很可能在不远的将来，你还会领教到没有去做的后果。</p><p>这也可以解释近几年流行的一句话：“尽管我们懂很多道理，但还是无法过好自己的人生”。现在再来读这句话，你是否发现，所谓的“很多道理”，我们是真的懂还是假的懂呢？相信你已经有答案了。</p><h2>时间管理法则</h2><p>懂了道理就要去践行，这时就面临一个很重要的话题：时间管理。在这个话题上，我推荐你去听一下TED上最热门的演讲之一《如何掌控你的自由时间》。演讲者劳拉（Laura Vanderkam）提到了一个例子：一位工作繁忙的女士平时都挤不出时间做锻炼，但有一次她家里的水管坏了，淹没了她家的地下室，于是她挤出了7个小时跟管道工一起修好它。</p><blockquote>\n<p>补充：你可以到<a href="https://www.ximalaya.com/waiyu/31817783/341451523">这里</a>听这个音频。</p>\n</blockquote><p>在这个例子里，为什么这位女士平时挤不出时间，但水管坏了就硬是能挤出来时间来，而且有7小时这么长呢？原因就是：修水管这件事对她很重要，于是她一定可以安排出时间来。那么同样地，<strong>我们如果真的重视一件事，也一定可以安排出时间来</strong>，无论它是多久。这就是时间管理的核心秘密。</p><p>反过来说，你时间花在哪里，就表示你重视哪里。这个道理，你可以用来检视自己日常的行为，看看自己在重要事情的投入是否到位。其实，这个时间管理法则跟上面说的“知行合一”这个东方智慧，有着异曲同工之妙。你不花时间的地方就是行得不够，行得不够就说明知得不够。</p><h2>从小的成功做起</h2><p>那我们知道了道理，也真的花时间去实践了，是否一定能成功呢？其实，这里还有实践中的具体方法要考虑。在这里我推荐你一个方法：<strong>从小的成功做起</strong>。</p><p>这其实是管理领域的一个概念，英文叫small success或者small win。比如当我们培养一个工程师的时候，不要一开始给他一个难度明显超出他当前能力的项目，而应该找的是一个“跳一跳够得着”的合适的项目，从而提高他成功的概率，这就是小的成功。</p><p>这个法则对我们进入一个新的领域时特别有效。在新领域里，我们一方面要面临这个领域本身的业务方面的要求，一方面还要担负“在这方面没有底气”的精神压力。这时候，小的成功就变得更加重要了，它也是后续大的成功的基石。</p><p>比如，你决心今年提高网络排查能力，但是网络知识基础又比较薄弱。那么你先不要一上来就想用抓包分析搞定一个之前搞不定的问题，而是可以先用Wireshark抓一个包，认识一下IP、TCP等协议头部的格式，再对比我们课程里提到的知识点，把这些知识点理解好。这就是一个“小的成功”。</p><p>别看这好像算不上什么，但就是这些小的细节处做得扎实，我们才更容易做到稳定的提升，在这个势头起来之后，你不仅能积累技术，也能积累信心，后续做成的概率就更大了。</p><h2>接纳你自己</h2><p>还有一点也很重要，无论是知行合一、时间管理，还是小的成功，主体都是我们自己。缺少了“我”这个主语，所有的道理都没有意义。所以，无论发生什么，我们都要<strong>接纳自己</strong>，这是我们做事情的起点。</p><p>我暂时不会没关系，我就接纳我自己，从这个低的起点开始学。</p><p>我学得比别人慢一点没关系，我就接纳我自己，我愿意花更多的时间去学。</p><p>我目前的成就比别人低一点没关系，我就接纳我自己，我不攀比，我尽力做最好的自己。</p><p>事实上，我们选择努力，是因为努力正是我们可以选择的东西，而同样有很多我们无法选择的因素也在影响着我们的命运。无论生活和工作给你什么样的反馈，无论是好的还是不好的，我们要学会接纳它，因为这就是我们的生活，因为我们还是依然可以选择努力。</p><p>给你分享我喜欢的罗翔老师的一段话：</p><blockquote>\n<p>虽然一生中95%的事情可能是我们决定不了的，但是我们依然要用5%的努力去撬动那95%我们无法决定的事情。<br>\n&nbsp;<br>\n凡事尽力而为，但同时要接受命运一切的安排。这是我的一种表达，看似有一种悖论。但是人生中不就是在各种悖论中寻找一种整合，在各种似非而是中去寻找一种超越。</p>\n</blockquote><h2>我为什么做这个专栏？</h2><p>最后，我也想跟你聊聊为什么我会做这个专栏。其实，我很早就萌生了要写一本书或者做一个课程的想法，自己也断断续续写了不少文字。不过，从这个念头起来，到今天专栏全部结束，中间也有好几年了。说来惭愧，我虽然觉得这个事情挺重要的，但行动上也没有真的执行，也是前面说的“未知”的状态。</p><p>直到去年，我看到我们部门的总监许健老师的《技术管理案例课》，还有架构师李程远老师的《容器实战高手课》等陆续上线，我就很羡慕。我当时的心理状态就是：“他们不动声色地就做好了一门课程，真是太厉害了！”</p><p>到我真正动手系统性地写这个专栏的时候，我发现难度也超出了我的预估。可以说，做专栏要投入的心力，不亚于养育一个孩子。比如我在12月和1月就把攒了一年的年假给请掉了，要不然课程来不及准备。即便如此，我也时常为了写稿要熬到夜里2点之后，因为思路无法中断，只有不负状态，一鼓作气才行。</p><p>还有很多次，我在电脑前呆坐了大半天，感觉也没写多少，只好安慰自己“我大概是进入了心流状态，所以感觉时间过得很快”。这里要特别感谢我的家人，他们在我需要的时候给了最大的支持，这份小的成绩离开这份支持是根本做不到的。</p><h2>我自己的成长</h2><p>我现在可能算一个比较努力的人，但很早以前我也跟很多同学一样挺迷茫的。转变发生在我家老大出生时，我看着一个新生命，强烈地意识到：我需要为他负责起来。这不仅是给他吃好穿好，还需要给他一个感觉：他的爸爸是有责任心，一直努力的人。这样的家风是积极的，可以惠泽孩子的一生。</p><p>那么，在努力工作的过程中，我发现我对网络的兴趣是比较浓厚的。虽然我学习新知识的速度并不比别人快，甚至可能有点慢，但是通过持续地做这件事，我发现，以前懵懵懂懂的知识点变得清晰了，以前不敢接触的领域居然也“敢”看了，然后在这些技术的理解上有了比较大的提升。这有点像我们调整望远镜的焦距一样，调对了就非常清晰，知识点会变得触手可及一般。</p><p>我有一次在朋友圈转发了这样一段话：</p><blockquote>\n<p>有人说聪明有两种，一种是短时间内学得特别快；一种是学得不快，但是时间长了能研究得很深。</p>\n</blockquote><p>当然也有朋友评论说：感觉第二种更像是一种性格。</p><p>你会怎么看呢？这里我想说的是，聪明与否已经不是我们学习的阻碍了，<strong>坚持</strong>才是。我这样平凡普通的人，通过长年的积累，也能在某一个细分领域有点小小的成绩，相信你可以做得更好。</p><h2>感恩你的支持</h2><p>我在每次课的结尾都说“我们一起进步、成长”，真非虚言。</p><p>在做这个专栏的过程中，极客时间给予了大力的支持。编辑花了大量的精力来辅导我如何写好课程文稿，音频导演多次耐心指导我如何做好录音，还有非常多的同学给了各个方面的帮助。我也感觉自己在多个方面也取得了明显的成长。可以说，这次专栏做完，我在如何做好技术输出，如何做好类似“技术布道者”等方面的底气和自信，也增加了不少。</p><p>当然在技术本身上，做专栏对我的提升也很大，这是我没有充分预料到的。特别是同学们在留言区的很多问题，也戳中了我的很多盲点，于是我去翻文档、查代码，乃至做实验，确认以后再貌似轻松地进行回复。我在这上面投入的时间也很多，也倒逼我不断完善自己的知识体系。古人说“教学相长”，我也在教学输出的过程中得到了提高，这让我很开心。</p><p>所以在这里，我也要对每一位同学说一声：谢谢你们！</p><h2>不是结束的结束</h2><p>我们计划中的专栏内容都已经结束了，而且在计划之外我们还增加了4次的春节加餐和4次的答疑加餐，一下子把原计划的20多讲扩充到了接近40讲。而今天之后，你依然可以继续在留言区提问，我会持续地来这里跟你交流。</p><p>另外，我们的不定期加餐也会保持，有新的心得和有价值的案例的时候，我还会跟你分享。我在这里立一个flag：我会把我们这个专栏做到我能做的最好，成为极客时间里“实践性特别强”的基础技术方面的专栏。</p><p>好了，我们的连接是长连接，虽然课程的事务结束了，但是长连接继续，期待我们后续的互动。让我们：</p><center>知行合一</center><center>时间管理</center><center>小成做起</center><center>接纳自己</center><p>最后的最后，我还给你准备了一份毕业问卷，希望你能花两三分钟填写一下，非常期待能听到你对这门课程的反馈。</p><p><a href="https://jinshuju.net/f/WanCUE"><img src="https://static001.geekbang.org/resource/image/6d/45/6d1cea8623059cf945e9ed9332871545.jpg?wh=1142x801" alt=""></a></p><p>咱们留言区见！</p>',
        article_title: "结束语 | 珍惜握手，难说再见",
      },
      {
        title: "结课测试 | “网络排查案例课”100分试卷等你来挑战！",
        id: 512739,
        content:
          '<p>你好，我是胜辉。</p><p>咱们的这门课程已经完结一段时间了，在完结的这段时间里，我依然收到了很多留言，很感谢你一直以来的认真学习和支持！</p><p>为了帮助你检验自己的学习效果，我特别给你准备了一套结课测试题。这套测试题共有20道题目，满分 100 分，核心考点都出自课程里讲到的知识点，希望可以帮助你进行一场自测。</p><p>好了，话不多说，点击下面的按钮开始测试吧！</p><p><a href="http://time.geekbang.org/quiz/intro?act_id=3539&exam_id=9227"><img src="https://static001.geekbang.org/resource/image/28/a4/28d1be62669b4f3cc01c36466bf811a4.png?wh=1142*201" alt=""></a></p><!-- [[[read_end]]] -->',
        article_title: "结课测试 | “网络排查案例课”100分试卷等你来挑战！",
      },
    ],
  },
];
