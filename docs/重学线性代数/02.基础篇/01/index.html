<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no"
    />
    <link rel="shortcut icon" type="image/x-icon" href="/logo.png" />
    <link rel="stylesheet" href="/blog-base/umi.css" />
    <script>
      window.routerBase = "/blog-base";
    </script>
    <script>
      window.publicPath = window.resourceBaseUrl || "/blog-base/";
    </script>
    <script>
      //! umi version: 3.5.41
    </script>
    <script>
      !(function () {
        var e =
            navigator.cookieEnabled && void 0 !== window.localStorage
              ? localStorage.getItem("dumi:prefers-color")
              : "auto",
          o = window.matchMedia("(prefers-color-scheme: dark)").matches,
          t = ["light", "dark", "auto"];
        document.documentElement.setAttribute(
          "data-prefers-color",
          e === t[2] ? (o ? t[1] : t[0]) : t.indexOf(e) > -1 ? e : t[0]
        );
      })();
    </script>
    <title>01 | 导读：如何在机器学习中运用线性代数工具？ - 大师兄</title>
  </head>
  <body>
    <div id="root"><div class="__dumi-default-layout" data-route="/重学线性代数/02.基础篇/01" data-show-sidemenu="true" data-show-slugs="true" data-site-mode="true" data-gapless="false"><div class="__dumi-default-navbar" data-mode="site"><button class="__dumi-default-navbar-toggle"></button><a class="__dumi-default-navbar-logo" style="background-image:url(&#x27;/logo.png&#x27;)" href="/blog-base/">大师兄</a><nav><div class="__dumi-default-search"><input type="search" class="__dumi-default-search-input" value=""/><ul></ul></div><span>计算机基础<ul><li><a href="/blog-base/编译原理之美">编译原理之美</a></li><li><a href="/blog-base/编译原理实战">编译原理实战</a></li><li><a href="/blog-base/深入浅出计算机组成原理">深入浅出计算机组成原理</a></li><li><a href="/blog-base/详解http">详解http</a></li><li><a href="/blog-base/计算机网络通关29讲">计算机网络通关29讲</a></li><li><a href="/blog-base/网络排查案例课">网络排查案例课</a></li><li><a href="/blog-base/linux操作系统">linux操作系统</a></li><li><a href="/blog-base/linux内核技术实战课">linux内核技术实战课</a></li><li><a href="/blog-base/linux性能优化实战">linux性能优化实战</a></li><li><a href="/blog-base/程序员数学基础">程序员数学基础</a></li><li><a href="/blog-base/趣谈网络协议">趣谈网络协议</a></li><li><a href="/blog-base/操作系统实战">操作系统实战</a></li><li><a href="/blog-base/软件工程之美">软件工程之美</a></li><li><a href="/blog-base/sql必知必会">sql必知必会</a></li><li><a href="/blog-base/操作系统实战45讲">操作系统实战45讲</a></li><li><a href="/blog-base/网络编程实战">网络编程实战</a></li><li><a href="/blog-base/趣谈linux操作系统">趣谈linux操作系统</a></li></ul></span><span>算法<ul><li><a href="/blog-base/常用算法25讲">常用算法25讲</a></li><li><a href="/blog-base/数据结构与算法之美">数据结构与算法之美</a></li><li><a href="/blog-base/业务开发算法50讲">业务开发算法50讲</a></li><li><a href="/blog-base/动态规划面试宝典">动态规划面试宝典</a></li></ul></span><span>前端开发<ul><li><a href="/blog-base/正则表达式入门">正则表达式入门</a></li></ul></span><span>前端工程化</span><span>前端性能优化</span><span>移动端开发</span><span>软件测试</span><span>产品与用户体验</span><span>面试</span><span>杂谈<ul><li><a href="/blog-base/代码之丑">代码之丑</a></li><li><a href="/blog-base/代码精进之路">代码精进之路</a></li><li><a href="/blog-base/数据分析思维课">数据分析思维课</a></li><li><a href="/blog-base/朱涛kotlin编程第一课">朱涛kotlin编程第一课</a></li><li><a aria-current="page" class="active" href="/blog-base/重学线性代数">重学线性代数</a></li></ul></span><div class="__dumi-default-navbar-tool"><div class="__dumi-default-dark"><div class="__dumi-default-dark-switch "></div></div></div></nav></div><div class="__dumi-default-menu" data-mode="site"><div class="__dumi-default-menu-inner"><div class="__dumi-default-menu-header"><a class="__dumi-default-menu-logo" style="background-image:url(&#x27;/logo.png&#x27;)" href="/blog-base/"></a><h1>大师兄</h1><p></p></div><div class="__dumi-default-menu-mobile-area"><ul class="__dumi-default-menu-nav-list"><li>计算机基础<ul><li><a href="/blog-base/编译原理之美">编译原理之美</a></li><li><a href="/blog-base/编译原理实战">编译原理实战</a></li><li><a href="/blog-base/深入浅出计算机组成原理">深入浅出计算机组成原理</a></li><li><a href="/blog-base/详解http">详解http</a></li><li><a href="/blog-base/计算机网络通关29讲">计算机网络通关29讲</a></li><li><a href="/blog-base/网络排查案例课">网络排查案例课</a></li><li><a href="/blog-base/linux操作系统">linux操作系统</a></li><li><a href="/blog-base/linux内核技术实战课">linux内核技术实战课</a></li><li><a href="/blog-base/linux性能优化实战">linux性能优化实战</a></li><li><a href="/blog-base/程序员数学基础">程序员数学基础</a></li><li><a href="/blog-base/趣谈网络协议">趣谈网络协议</a></li><li><a href="/blog-base/操作系统实战">操作系统实战</a></li><li><a href="/blog-base/软件工程之美">软件工程之美</a></li><li><a href="/blog-base/sql必知必会">sql必知必会</a></li><li><a href="/blog-base/操作系统实战45讲">操作系统实战45讲</a></li><li><a href="/blog-base/网络编程实战">网络编程实战</a></li><li><a href="/blog-base/趣谈linux操作系统">趣谈linux操作系统</a></li></ul></li><li>算法<ul><li><a href="/blog-base/常用算法25讲">常用算法25讲</a></li><li><a href="/blog-base/数据结构与算法之美">数据结构与算法之美</a></li><li><a href="/blog-base/业务开发算法50讲">业务开发算法50讲</a></li><li><a href="/blog-base/动态规划面试宝典">动态规划面试宝典</a></li></ul></li><li>前端开发<ul><li><a href="/blog-base/正则表达式入门">正则表达式入门</a></li></ul></li><li>前端工程化</li><li>前端性能优化</li><li>移动端开发</li><li>软件测试</li><li>产品与用户体验</li><li>面试</li><li>杂谈<ul><li><a href="/blog-base/代码之丑">代码之丑</a></li><li><a href="/blog-base/代码精进之路">代码精进之路</a></li><li><a href="/blog-base/数据分析思维课">数据分析思维课</a></li><li><a href="/blog-base/朱涛kotlin编程第一课">朱涛kotlin编程第一课</a></li><li><a aria-current="page" class="active" href="/blog-base/重学线性代数">重学线性代数</a></li></ul></li></ul><div class="__dumi-default-dark"><div class="__dumi-default-dark-switch "><button title="Dark theme" class="__dumi-default-dark-moon "><svg viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="3854" width="22" height="22"><path d="M991.816611 674.909091a69.166545 69.166545 0 0 0-51.665455-23.272727 70.795636 70.795636 0 0 0-27.438545 5.585454A415.674182 415.674182 0 0 1 754.993338 698.181818c-209.594182 0-393.472-184.785455-393.472-395.636363 0-52.363636 38.539636-119.621818 69.515637-173.614546 4.887273-8.610909 9.634909-16.756364 14.103272-24.901818A69.818182 69.818182 0 0 0 384.631156 0a70.842182 70.842182 0 0 0-27.438545 5.585455C161.678429 90.298182 14.362065 307.898182 14.362065 512c0 282.298182 238.824727 512 532.38691 512a522.286545 522.286545 0 0 0 453.957818-268.334545A69.818182 69.818182 0 0 0 991.816611 674.909091zM546.679156 954.181818c-248.785455 0-462.941091-192-462.941091-442.181818 0-186.647273 140.637091-372.829091 300.939637-442.181818-36.817455 65.629091-92.578909 151.970909-92.578909 232.727273 0 250.181818 214.109091 465.454545 462.917818 465.454545a488.331636 488.331636 0 0 0 185.181091-46.545455 453.003636 453.003636 0 0 1-393.565091 232.727273z m103.656728-669.323636l-14.266182 83.781818a34.909091 34.909091 0 0 0 50.362182 36.770909l74.775272-39.563636 74.752 39.563636a36.142545 36.142545 0 0 0 16.174546 3.956364 34.909091 34.909091 0 0 0 34.210909-40.727273l-14.289455-83.781818 60.509091-59.345455a35.025455 35.025455 0 0 0-19.223272-59.578182l-83.61891-12.101818-37.376-76.101818a34.56 34.56 0 0 0-62.254545 0l-37.376 76.101818-83.618909 12.101818a34.909091 34.909091 0 0 0-19.246546 59.578182z m70.423272-64.698182a34.280727 34.280727 0 0 0 26.135273-19.083636l14.312727-29.090909 14.336 29.090909a34.257455 34.257455 0 0 0 26.135273 19.083636l32.046546 4.887273-23.272728 22.574545a35.234909 35.234909 0 0 0-10.007272 30.952727l5.46909 32.116364-28.625454-15.127273a34.490182 34.490182 0 0 0-32.302546 0l-28.695272 15.127273 5.469091-32.116364a35.141818 35.141818 0 0 0-9.984-30.952727l-23.272728-22.574545z" p-id="3855"></path></svg></button><button title="Light theme" class="__dumi-default-dark-sun "><svg viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="4026" width="22" height="22"><path d="M915.2 476.16h-43.968c-24.704 0-44.736 16-44.736 35.84s20.032 35.904 44.736 35.904H915.2c24.768 0 44.8-16.064 44.8-35.904s-20.032-35.84-44.8-35.84zM512 265.6c-136.704 0-246.464 109.824-246.464 246.4 0 136.704 109.76 246.464 246.464 246.464S758.4 648.704 758.4 512c0-136.576-109.696-246.4-246.4-246.4z m0 425.6c-99.008 0-179.2-80.128-179.2-179.2 0-98.944 80.192-179.2 179.2-179.2S691.2 413.056 691.2 512c0 99.072-80.192 179.2-179.2 179.2zM197.44 512c0-19.84-19.136-35.84-43.904-35.84H108.8c-24.768 0-44.8 16-44.8 35.84s20.032 35.904 44.8 35.904h44.736c24.768 0 43.904-16.064 43.904-35.904zM512 198.464c19.776 0 35.84-20.032 35.84-44.8v-44.8C547.84 84.032 531.84 64 512 64s-35.904 20.032-35.904 44.8v44.8c0 24.768 16.128 44.864 35.904 44.864z m0 627.136c-19.776 0-35.904 20.032-35.904 44.8v44.736C476.096 940.032 492.16 960 512 960s35.84-20.032 35.84-44.8v-44.736c0-24.768-16.064-44.864-35.84-44.864z m329.92-592.832c17.472-17.536 20.288-43.072 6.4-57.024-14.016-14.016-39.488-11.2-57.024 6.336-4.736 4.864-26.496 26.496-31.36 31.36-17.472 17.472-20.288 43.008-6.336 57.024 13.952 14.016 39.488 11.2 57.024-6.336 4.8-4.864 26.496-26.56 31.296-31.36zM213.376 759.936c-4.864 4.8-26.56 26.624-31.36 31.36-17.472 17.472-20.288 42.944-6.4 56.96 14.016 13.952 39.552 11.2 57.024-6.336 4.8-4.736 26.56-26.496 31.36-31.36 17.472-17.472 20.288-43.008 6.336-56.96-14.016-13.952-39.552-11.072-56.96 6.336z m19.328-577.92c-17.536-17.536-43.008-20.352-57.024-6.336-14.08 14.016-11.136 39.488 6.336 57.024 4.864 4.864 26.496 26.56 31.36 31.424 17.536 17.408 43.008 20.288 56.96 6.336 14.016-14.016 11.264-39.488-6.336-57.024-4.736-4.864-26.496-26.56-31.296-31.424z m527.168 628.608c4.864 4.864 26.624 26.624 31.36 31.424 17.536 17.408 43.072 20.224 57.088 6.336 13.952-14.016 11.072-39.552-6.4-57.024-4.864-4.8-26.56-26.496-31.36-31.36-17.472-17.408-43.072-20.288-57.024-6.336-13.952 14.016-11.008 39.488 6.336 56.96z" p-id="4027"></path></svg></button><button title="Default to system" class="__dumi-default-dark-auto "><svg viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="11002" width="22" height="22"><path d="M127.658667 492.885333c0-51.882667 10.24-101.717333 30.378666-149.162666s47.786667-88.064 81.92-122.538667 75.093333-61.781333 122.538667-81.92 96.938667-30.378667 149.162667-30.378667 101.717333 10.24 149.162666 30.378667 88.405333 47.786667 122.88 81.92 61.781333 75.093333 81.92 122.538667 30.378667 96.938667 30.378667 149.162666-10.24 101.717333-30.378667 149.162667-47.786667 88.405333-81.92 122.88-75.093333 61.781333-122.88 81.92-97.28 30.378667-149.162666 30.378667-101.717333-10.24-149.162667-30.378667-88.064-47.786667-122.538667-81.92-61.781333-75.093333-81.92-122.88-30.378667-96.938667-30.378666-149.162667z m329.045333 0c0 130.048 13.994667 244.394667 41.984 343.381334h12.970667c46.762667 0 91.136-9.216 133.461333-27.306667s78.848-42.666667 109.568-73.386667 54.954667-67.242667 73.386667-109.568 27.306667-86.698667 27.306666-133.461333c0-46.421333-9.216-90.794667-27.306666-133.12s-42.666667-78.848-73.386667-109.568-67.242667-54.954667-109.568-73.386667-86.698667-27.306667-133.461333-27.306666h-11.605334c-28.672 123.562667-43.349333 237.909333-43.349333 343.722666z" p-id="11003"></path></svg></button></div></div></div><ul class="__dumi-default-menu-list"><li><a href="/blog-base/重学线性代数">重学线性代数</a></li><li><a href="/blog-base/重学线性代数/01.开篇词">01.开篇词</a><ul><li><a href="/blog-base/重学线性代数/01.开篇词/01"><span>开篇词 | 从今天起，学会线性代数</span></a></li></ul></li><li><a aria-current="page" class="active" href="/blog-base/重学线性代数/02.基础篇">02.基础篇</a><ul><li><a aria-current="page" class="active" href="/blog-base/重学线性代数/02.基础篇/01"><span>01 | 导读：如何在机器学习中运用线性代数工具？</span></a></li><li><a href="/blog-base/重学线性代数/02.基础篇/02"><span>02 | 基本概念：线性代数研究的到底是什么问题？</span></a></li><li><a href="/blog-base/重学线性代数/02.基础篇/03"><span>03 | 矩阵：为什么说矩阵是线性方程组的另一种表达？</span></a></li><li><a href="/blog-base/重学线性代数/02.基础篇/04"><span>04 | 解线性方程组：为什么用矩阵求解的效率这么高？</span></a></li><li><a href="/blog-base/重学线性代数/02.基础篇/05"><span>05 | 线性空间：如何通过向量的结构化空间在机器学习中做降维处理？</span></a></li><li><a href="/blog-base/重学线性代数/02.基础篇/06"><span>06 | 线性无关：如何理解向量在N维空间的几何意义？</span></a></li><li><a href="/blog-base/重学线性代数/02.基础篇/07"><span>07 | 基和秩：为什么说它表达了向量空间中“有用”的向量个数？</span></a></li><li><a href="/blog-base/重学线性代数/02.基础篇/08"><span>08 | 线性映射：如何从坐标系角度理解两个向量空间之间的函数？</span></a></li><li><a href="/blog-base/重学线性代数/02.基础篇/09"><span>09 | 仿射空间：如何在图形的平移操作中大显身手？</span></a></li><li><a href="/blog-base/重学线性代数/02.基础篇/10"><span>10 | 解析几何：为什么说它是向量从抽象到具象的表达？</span></a></li><li><a href="/blog-base/重学线性代数/02.基础篇/11"><span>基础通关 | 线性代数5道典型例题及解析</span></a></li></ul></li><li><a href="/blog-base/重学线性代数/03.应用篇">03.应用篇</a><ul><li><a href="/blog-base/重学线性代数/03.应用篇/01"><span>11 | 如何运用线性代数方法解决图论问题？</span></a></li><li><a href="/blog-base/重学线性代数/03.应用篇/02"><span>12 | 如何通过矩阵转换让3D图形显示到二维屏幕上？</span></a></li><li><a href="/blog-base/重学线性代数/03.应用篇/03"><span>13 | 如何通过有限向量空间加持的希尔密码，提高密码被破译的难度？</span></a></li><li><a href="/blog-base/重学线性代数/03.应用篇/04"><span>14 | 如何在深度学习中运用数值代数的迭代法做训练？</span></a></li><li><a href="/blog-base/重学线性代数/03.应用篇/05"><span>15 | 如何从计算机的角度来理解线性代数？</span></a></li><li><a href="/blog-base/重学线性代数/03.应用篇/06"><span>强化通关 | 线性代数水平测试20题</span></a></li></ul></li><li><a href="/blog-base/重学线性代数/04.结束语">04.结束语</a><ul><li><a href="/blog-base/重学线性代数/04.结束语/01"><span>结束语 | 和数学打交道这么多年，我的三点感悟</span></a></li></ul></li><li><a href="/blog-base/重学线性代数/summary">重学线性代数</a></li></ul></div></div><ul role="slug-list" class="__dumi-default-layout-toc"><li title="从机器学习到线性代数" data-depth="2"><a href="/blog-base/重学线性代数/02.基础篇/01#从机器学习到线性代数"><span>从机器学习到线性代数</span></a></li><li title="鸢尾花分类问题中的线性代数" data-depth="2"><a href="/blog-base/重学线性代数/02.基础篇/01#鸢尾花分类问题中的线性代数"><span>鸢尾花分类问题中的线性代数</span></a></li><li title="1.数据集的收集、加载和分析" data-depth="3"><a href="/blog-base/重学线性代数/02.基础篇/01#1数据集的收集加载和分析"><span>1.数据集的收集、加载和分析</span></a></li><li title="2.数据集的准备" data-depth="3"><a href="/blog-base/重学线性代数/02.基础篇/01#2数据集的准备"><span>2.数据集的准备</span></a></li><li title="3.训练模型" data-depth="3"><a href="/blog-base/重学线性代数/02.基础篇/01#3训练模型"><span>3.训练模型</span></a></li><li title="4.模型测试" data-depth="3"><a href="/blog-base/重学线性代数/02.基础篇/01#4模型测试"><span>4.模型测试</span></a></li><li title="本节小结" data-depth="2"><a href="/blog-base/重学线性代数/02.基础篇/01#本节小结"><span>本节小结</span></a></li></ul><div class="__dumi-default-layout-content"><div class="markdown"><h1 id="01--导读如何在机器学习中运用线性代数工具"><a aria-hidden="true" tabindex="-1" href="/blog-base/重学线性代数/02.基础篇/01#01--导读如何在机器学习中运用线性代数工具"><span class="icon icon-link"></span></a>01 | 导读：如何在机器学习中运用线性代数工具？</h1><p>你好，我是朱维刚。欢迎你跟我一起重学线性代数！</p><p>在开篇词中，我和你大致讲过我自己的经历，从2006年开始到现在14年的时间里，我都专注于机器学习领域。对于<strong>线性代数</strong>在机器学习中的应用，我非常了解。而这也是线性代数最主要的应用场景之一。因此，今天第一节课，我想先和你聊一聊，如何在机器学习中运用线性代数工具，在我们开始自下而上的学习之前，先从上层来看一看。</p><p>我们都知道，“数据”是机器学习的前提，机器学习的第一步就是要进行<strong>数据</strong>的收集、预处理和特征提取；而<strong>模型</strong>就是通过数据来学习的算法；<strong>学习</strong>则是一个循环过程，一个自动在数据中寻找模式，并不停调优模型参数的过程。那我们就从机器学习的三个核心概念：数据、模型和学习说起。</p><p><img src="/images/httpsstatic001geekbangorgresourceimage3a323a2a7433d5d13b676abe05041a1bcd32.png" alt=""/></p><p>你看，不论是模型，还是学习，都涉及数据，而数据加上模型和学习，就是数学的一般过程了，也就是：观察、实验、推理和抽象。所以，我认为学好数学，不仅有利于理解复杂的机器学习系统，还能调优算法参数，甚至能帮助你创建新的机器学习解决方案。</p><h2 id="从机器学习到线性代数"><a aria-hidden="true" tabindex="-1" href="/blog-base/重学线性代数/02.基础篇/01#从机器学习到线性代数"><span class="icon icon-link"></span></a>从机器学习到线性代数</h2><p>那机器学习和线性代数之间到底有着怎样的关系呢？我想，用一个实际的机器学习算法的例子来解释，你可能更容易搞清楚。接下来，我使用KNN（K-Nearest Neighbor，K最近邻分类算法）来让你简单了解一下机器学习，以及它和线性代数之间的关系。</p><p>之所以选KNN分类算法，因为它是一个理论上比较成熟的方法，也是最简单的机器学习算法之一。这个方法的思路是：如果一个样本在特征空间中的K个最相似（即特征空间中最邻近）的样本中的大多数属于某一个类别，则该样本也属于这个类别。</p><p>这里有个前提，KNN算法中，所选择的“邻居”都是已经正确分类的对象。KNN分类算法在分类决策上只依据最邻近的一个或者几个样本的类别，来决定待分样本所属的类别。我们通过图来理解的话或许更容易一些。</p><p><img src="/images/httpsstatic001geekbangorgresourceimage43aa439cefee464eb01ed110e70515f94eaa.png" alt=""/></p><p>假设图片中那个绿色圆就要是我们要决策的对象，那么根据KNN算法它属于哪一类？是红色三角形还是蓝色四方形？</p><p>如果K=3（实线圆），也就是包含离绿色圆最近的3个，由于红色三角形所占比例为2/3，绿色圆就属于红色三角形那个类。但如果K=5（虚线圆），就是包含离绿色圆最近的5个，由于蓝色四方形比例为3/5，绿色圆就属于蓝色四方形那个类。</p><h2 id="鸢尾花分类问题中的线性代数"><a aria-hidden="true" tabindex="-1" href="/blog-base/重学线性代数/02.基础篇/01#鸢尾花分类问题中的线性代数"><span class="icon icon-link"></span></a>鸢尾花分类问题中的线性代数</h2><p>通过前面这个小例子，你应该已经理解了KNN算法的概念。那么接下来，我们就试着使用KNN在给定鸢尾花特征值的情况下，给鸢尾花做花种分类，带你来实际看一下线性代数在这里起到的作用。</p><p>特别说明一下，<strong>鸢尾花分类问题</strong>是一个国际上通用的案例，一般都被作为机器学习入门来使用，所以它的数据集也是公开的。</p><h3 id="1数据集的收集加载和分析"><a aria-hidden="true" tabindex="-1" href="/blog-base/重学线性代数/02.基础篇/01#1数据集的收集加载和分析"><span class="icon icon-link"></span></a>1.数据集的收集、加载和分析</h3><p>首先，我们要做的是数据集的收集、加载和分析，你也可以点击<a target="_blank" rel="noopener noreferrer" href="https://www.kaggle.com/notlir/iriscsv">这里<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="__dumi-default-external-link-icon"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>下载原始数据集，来看看原始数据长什么样，下面是获取和加载数据的代码，sklearn数据集已经包含了样本数据，你可以直接用。</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">import pandas as pd</span></div><div class="token-line"><span class="token plain">    </span></div><div class="token-line"><span class="token plain">    from sklearn import datasets</span></div><div class="token-line"><span class="token plain">    iris = datasets.load_iris()</span></div><div class="token-line"><span class="token plain">    </span></div><div class="token-line"><span class="token plain">    species = [iris.target_names[x] for x in iris.target]</span></div><div class="token-line"><span class="token plain">    </span></div><div class="token-line"><span class="token plain">    iris = pd.DataFrame(iris[&#x27;data&#x27;], columns = [&#x27;Sepal_Length&#x27;, &#x27;Sepal_Width&#x27;, &#x27;Petal_Length&#x27;, &#x27;Petal_Width&#x27;])</span></div><div class="token-line"><span class="token plain">    </span></div><div class="token-line"><span class="token plain">    iris[&#x27;Species&#x27;] = species</span></div></pre></div><p>从显示的结果，我们能够看出鸢尾花有四个特征：花萼的长、宽和花瓣的长、宽。我们来看下这四个特征的数据类型：</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">iris.dtypes</span></div><div class="token-line"><span class="token plain">    Sepal_Length    float64</span></div><div class="token-line"><span class="token plain">    Sepal_Width     float64</span></div><div class="token-line"><span class="token plain">    Petal_Length    float64</span></div><div class="token-line"><span class="token plain">    Petal_Width     float64</span></div><div class="token-line"><span class="token plain">    Species          object</span></div><div class="token-line"><span class="token plain">    dtype: object</span></div></pre></div><p>这些特征都是数值型，而且标签Species表示的是花种，是一个字符串类型的变量。我们继续看一下鸢尾花的分类统计：</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">iris[&#x27;count&#x27;] = 1</span></div><div class="token-line"><span class="token plain">    iris[[&#x27;Species&#x27;, &#x27;count&#x27;]].groupby(&#x27;Species&#x27;).count()</span></div></pre></div><p><img src="/images/httpsstatic001geekbangorgresourceimagea7cea7ff740c15de327cfd8c1c9a4b681cce.png" alt=""/></p><p>这里我们直接能够看到，鸢尾花有三个花种，每个种类有50个实例，或者说50条数据，我们再用图来更直观地显示这三种鸢尾花。</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">%matplotlib inline</span></div><div class="token-line"><span class="token plain">    </span></div><div class="token-line"><span class="token plain">    def plot_iris(iris, col1, col2):</span></div><div class="token-line"><span class="token plain">        import seaborn as sns</span></div><div class="token-line"><span class="token plain">        import matplotlib.pyplot as plt</span></div><div class="token-line"><span class="token plain">    </span></div><div class="token-line"><span class="token plain">        sns.lmplot(x = col1, y = col2,</span></div><div class="token-line"><span class="token plain">                   data = iris,</span></div><div class="token-line"><span class="token plain">                   hue = &quot;Species&quot;,</span></div><div class="token-line"><span class="token plain">                   fit_reg = False)</span></div><div class="token-line"><span class="token plain">    </span></div><div class="token-line"><span class="token plain">        plt.xlabel(col1)</span></div><div class="token-line"><span class="token plain">    </span></div><div class="token-line"><span class="token plain">        plt.ylabel(col2)</span></div><div class="token-line"><span class="token plain">    </span></div><div class="token-line"><span class="token plain">        plt.title(&#x27;Iris species shown by color&#x27;)</span></div><div class="token-line"><span class="token plain">    </span></div><div class="token-line"><span class="token plain">        plt.show()</span></div><div class="token-line"><span class="token plain">    </span></div><div class="token-line"><span class="token plain">    plot_iris(iris, &#x27;Petal_Width&#x27;, &#x27;Sepal_Length&#x27;)</span></div><div class="token-line"><span class="token plain">    </span></div><div class="token-line"><span class="token plain">    plot_iris(iris, &#x27;Sepal_Width&#x27;, &#x27;Sepal_Length&#x27;)</span></div></pre></div><p><img src="/images/httpsstatic001geekbangorgresourceimagec293c216f676f59e00cae4b52481fdf88293.png" alt=""/><img src="/images/httpsstatic001geekbangorgresourceimagea80aa8337b9d13c23ef18e3bd8a4dbb91b0a.png" alt=""/></p><p>蓝、黄、绿，这三种颜色分别代表了三种鸢尾花，显示还是很清楚的。</p><h3 id="2数据集的准备"><a aria-hidden="true" tabindex="-1" href="/blog-base/重学线性代数/02.基础篇/01#2数据集的准备"><span class="icon icon-link"></span></a>2.数据集的准备</h3><p>接下来的第二步就是数据集的准备了。在训练任何机器学习模型前，数据准备都相当重要，这里也要涉及两步准备。</p><p>第一步，特征数值标准化。如果我们不做标准化，后果就是大数值特征会主宰模型训练，这会导致更有意义的小数值特征被忽略。这里我们用Z Score标准化，使每一类特征平均值为0，方差为1.0，我们可以通过代码实现来看下效果。</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">from sklearn.preprocessing import scale</span></div><div class="token-line"><span class="token plain">    </span></div><div class="token-line"><span class="token plain">    import pandas as pd</span></div><div class="token-line"><span class="token plain">    </span></div><div class="token-line"><span class="token plain">    num_cols = [&#x27;Sepal_Length&#x27;, &#x27;Sepal_Width&#x27;, &#x27;Petal_Length&#x27;, &#x27;Petal_Width&#x27;]</span></div><div class="token-line"><span class="token plain">    </span></div><div class="token-line"><span class="token plain">    iris_scaled = scale(iris[num_cols])</span></div><div class="token-line"><span class="token plain">    </span></div><div class="token-line"><span class="token plain">    iris_scaled = pd.DataFrame(iris_scaled, columns = num_cols)</span></div><div class="token-line"><span class="token plain">    </span></div><div class="token-line"><span class="token plain">    print(iris_scaled.describe().round(3))</span></div></pre></div><p><img src="/images/httpsstatic001geekbangorgresourceimage1fda1f7bbea1c93dcdbbcd9c1ba4e32178da.png" alt=""/></p><p>你可以看到，每一列平均值为0，标准差大约是1.0。为了分类需要，我们用字典把花种从字符串类型转换成数字表示。</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">levels = {&#x27;setosa&#x27;:0, &#x27;versicolor&#x27;:1, &#x27;virginica&#x27;:2}</span></div><div class="token-line"><span class="token plain">    </span></div><div class="token-line"><span class="token plain">    iris_scaled[&#x27;Species&#x27;] = [levels[x] for x in iris[&#x27;Species&#x27;]]</span></div><div class="token-line"><span class="token plain">    </span></div><div class="token-line"><span class="token plain">    iris_scaled.head()</span></div></pre></div><p><img src="/images/httpsstatic001geekbangorgresourceimagebc5ebc14b245ab9076d3a8911dyy2da8895e.png" alt=""/></p><p>第二步，把数据集随机分割成样本训练集和评估数据集，训练集用来训练KNN模型，评估集用来测试和评估KNN的分类结果。</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">from sklearn.model_selection import train_test_split</span></div><div class="token-line"><span class="token plain">    </span></div><div class="token-line"><span class="token plain">    import numpy as np</span></div><div class="token-line"><span class="token plain">    </span></div><div class="token-line"><span class="token plain">    np.random.seed(3456)</span></div><div class="token-line"><span class="token plain">    </span></div><div class="token-line"><span class="token plain">    iris_split = train_test_split(np.asmatrix(iris_scaled), test_size = 75)</span></div><div class="token-line"><span class="token plain">    </span></div><div class="token-line"><span class="token plain">    iris_train_features = iris_split[0][:, :4]</span></div><div class="token-line"><span class="token plain">    </span></div><div class="token-line"><span class="token plain">    iris_train_labels = np.ravel(iris_split[0][:, 4])</span></div><div class="token-line"><span class="token plain">    </span></div><div class="token-line"><span class="token plain">    iris_test_features = iris_split[1][:, :4]</span></div><div class="token-line"><span class="token plain">    </span></div><div class="token-line"><span class="token plain">    iris_test_labels = np.ravel(iris_split[1][:, 4])</span></div><div class="token-line"><span class="token plain">    </span></div><div class="token-line"><span class="token plain">    print(iris_train_features.shape)</span></div><div class="token-line"><span class="token plain">    </span></div><div class="token-line"><span class="token plain">    print(iris_train_labels.shape)</span></div><div class="token-line"><span class="token plain">    </span></div><div class="token-line"><span class="token plain">    print(iris_test_features.shape)</span></div><div class="token-line"><span class="token plain">    </span></div><div class="token-line"><span class="token plain">    print(iris_test_labels.shape)</span></div></pre></div><p>通过代码，我们得到了下面这样的结果。</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">(75, 4)</span></div><div class="token-line"><span class="token plain">    (75,)</span></div><div class="token-line"><span class="token plain">    (75, 4)</span></div><div class="token-line"><span class="token plain">    (75,)</span></div></pre></div><h3 id="3训练模型"><a aria-hidden="true" tabindex="-1" href="/blog-base/重学线性代数/02.基础篇/01#3训练模型"><span class="icon icon-link"></span></a>3.训练模型</h3><p>数据准备好后，就是第三步训练模型了。这里我们使用K=3来训练KNN模型，当然你也可以调整这个参数来进行观察和调优。</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">from sklearn.neighbors import KNeighborsClassifier</span></div><div class="token-line"><span class="token plain">    </span></div><div class="token-line"><span class="token plain">    KNN_mod = KNeighborsClassifier(n_neighbors = 3)</span></div><div class="token-line"><span class="token plain">    </span></div><div class="token-line"><span class="token plain">    KNN_mod.fit(iris_train_features, iris_train_labels)</span></div></pre></div><h3 id="4模型测试"><a aria-hidden="true" tabindex="-1" href="/blog-base/重学线性代数/02.基础篇/01#4模型测试"><span class="icon icon-link"></span></a>4.模型测试</h3><p>执行KNN训练后，我们来到了最后一步，模型测试，这里我们使用测试集来测试模型。</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">iris_test = pd.DataFrame(iris_test_features, columns = num_cols)</span></div><div class="token-line"><span class="token plain">    </span></div><div class="token-line"><span class="token plain">    iris_test[&#x27;predicted&#x27;] = KNN_mod.predict(iris_test_features)</span></div><div class="token-line"><span class="token plain">    </span></div><div class="token-line"><span class="token plain">    iris_test[&#x27;correct&#x27;] = [1 if x == z else 0 for x, z in zip(iris_test[&#x27;predicted&#x27;], iris_test_labels)]</span></div><div class="token-line"><span class="token plain">    </span></div><div class="token-line"><span class="token plain">    accuracy = 100.0 * float(sum(iris_test[&#x27;correct&#x27;])) / float(iris_test.shape[0])</span></div><div class="token-line"><span class="token plain">    </span></div><div class="token-line"><span class="token plain">    print(accuracy)</span></div></pre></div><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">96.0</span></div></pre></div><p>最终，我们得到的准确率是96.0，说明了KNN的训练模型不错，适用这类场景。我们通过代码把其中的两个分类setosa和versicolor打印出来看看。</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">levels = {0:&#x27;setosa&#x27;, 1:&#x27;versicolor&#x27;, 2:&#x27;virginica&#x27;}</span></div><div class="token-line"><span class="token plain">    </span></div><div class="token-line"><span class="token plain">    iris_test[&#x27;Species&#x27;] = [levels[x] for x in iris_test[&#x27;predicted&#x27;]]</span></div><div class="token-line"><span class="token plain">    </span></div><div class="token-line"><span class="token plain">    markers = {1:&#x27;^&#x27;, 0:&#x27;o&#x27;}</span></div><div class="token-line"><span class="token plain">    </span></div><div class="token-line"><span class="token plain">    colors = {&#x27;setosa&#x27;:&#x27;blue&#x27;, &#x27;versicolor&#x27;:&#x27;green&#x27;,}</span></div><div class="token-line"><span class="token plain">    </span></div><div class="token-line"><span class="token plain">    def plot_shapes(df, col1,col2,  markers, colors):</span></div><div class="token-line"><span class="token plain">        import matplotlib.pyplot as plt</span></div><div class="token-line"><span class="token plain">        import seaborn as sns</span></div><div class="token-line"><span class="token plain">    </span></div><div class="token-line"><span class="token plain">        ax = plt.figure(figsize=(6, 6)).gca() # define plot axis</span></div><div class="token-line"><span class="token plain">    </span></div><div class="token-line"><span class="token plain">        for m in markers: # iterate over marker dictioary keys</span></div><div class="token-line"><span class="token plain">            for c in colors: # iterate over color dictionary keys</span></div><div class="token-line"><span class="token plain">                df_temp = df[(df[&#x27;correct&#x27;] == m)  &amp; (df[&#x27;Species&#x27;] == c)]</span></div><div class="token-line"><span class="token plain">                sns.regplot(x = col1, y = col2,</span></div><div class="token-line"><span class="token plain">                            data = df_temp, </span></div><div class="token-line"><span class="token plain">                            fit_reg = False,</span></div><div class="token-line"><span class="token plain">                            scatter_kws={&#x27;color&#x27;: colors[c]},</span></div><div class="token-line"><span class="token plain">                            marker = markers[m],</span></div><div class="token-line"><span class="token plain">                            ax = ax)</span></div><div class="token-line"><span class="token plain">        plt.xlabel(col1)</span></div><div class="token-line"><span class="token plain">        plt.ylabel(col2)</span></div><div class="token-line"><span class="token plain">        plt.title(&#x27;Iris species by color&#x27;)</span></div><div class="token-line"><span class="token plain">        return &#x27;Done&#x27;</span></div><div class="token-line"><span class="token plain">    </span></div><div class="token-line"><span class="token plain">    plot_shapes(iris_test, &#x27;Petal_Width&#x27;, &#x27;Sepal_Length&#x27;, markers, colors)</span></div><div class="token-line"><span class="token plain">    plot_shapes(iris_test, &#x27;Sepal_Width&#x27;, &#x27;Sepal_Length&#x27;, markers, colors)</span></div></pre></div><p><img src="/images/httpsstatic001geekbangorgresourceimage9e7f9e2c398552558a970ff1644905f6347f.png" alt=""/><img src="/images/httpsstatic001geekbangorgresourceimage10471057ba92123f1b3faa7d98b3162a4c47.png" alt=""/></p><p>从显示的效果来说，分类还是挺明显的，熟悉了最基础的机器学习过程后，你可能会问，讲了半天，线性代数到底在哪里呢？关键就在KNeighborsClassifier模块上，这个模型算法的实现背后，其实用到了线性代数的核心原理。</p><p>首先，因为每种鸢尾花都有四个特征：花萼的长、宽和花瓣的长、宽，所以每条数据都是四维向量。</p><p>接着，量化样本之间的相似度，也就是计算向量之间的距离。而向量之间距离的运算有很多方式，比如：曼哈顿距离、欧式距离、切比雪夫距离、闵可夫斯基距离等等。其中，欧式距离你应该很熟悉了，因为我们初中都学过，在二维平面上计算两点之间的距离公式：</p><p>$$d=\sqrt<!-- -->{<!-- -->\left(x_<!-- -->{<!-- -->1<!-- -->}<!-- -->-x_<!-- -->{<!-- -->2<!-- -->}<!-- -->\right)^<!-- -->{<!-- -->2<!-- -->}<!-- -->+\left(y_<!-- -->{<!-- -->1<!-- -->}<!-- -->-y_<!-- -->{<!-- -->2<!-- -->}<!-- -->\right)^<!-- -->{<!-- -->2<!-- -->}<!-- -->}<!-- -->$$</p><p>扩展到我们实例中的四维向量，也是同样的算法。</p><p>你看，这就是线性代数在机器学习中的一种应用场景。KNN是一种监督学习算法，因为在样本集中有分类信息，通过计算距离来衡量样本之间相似度，算法简单，易于理解和实现。还有另一种机器学习算法是无监督学习，底层的数学原理其实也是差不多的，总的思想就是“物以类聚”。</p><p>现在，你是不是有一种豁然开朗的感觉？终于看到了线性代数原来那么有意义，而且再简单的公式也是美的。</p><h2 id="本节小结"><a aria-hidden="true" tabindex="-1" href="/blog-base/重学线性代数/02.基础篇/01#本节小结"><span class="icon icon-link"></span></a>本节小结</h2><p>好了，到这里导读这一讲就结束了，最后我再总结一下前面讲解的内容。</p><p>这一讲我使用机器学习的监督学习算法KNN，在给定鸢尾花特征值的情况下，给鸢尾花做花种分类，让你了解机器学习最基本的过程外，能够真正了解其背后的线性代数真相，为你进入后面课程的学习提供一个感性的认知。</p><p>机器学习中用到的线性代数知识点比比皆是，而且往往软件架构上看上去复杂的事情，在数学上反而很简单，希望你在学习了这门课程后，能够多从数学角度出发去构思解决问题的方案。</p><p>同时，欢迎你在留言区说说自己对机器学习的理解，也可以分享一下自己的线性代数学习经历，如果你有所收获，也欢迎你把这篇文章分享给你的朋友。</p></div><div class="__dumi-default-layout-footer-meta"><a target="_blank" rel="noopener noreferrer" href="https://github.com/GGwujun/blog/edit/master/ssrc/重学线性代数/02.基础篇/01.md">在 GitHub 上编辑此页<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="__dumi-default-external-link-icon"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a><span data-updated-text="最后更新时间：">2023/9/23 18:44:27</span></div></div></div></div>
	<script>
  window.g_useSSR = true;
  window.g_initialProps = {};
	</script>

    <script>
      (function () {
        if (!location.port) {
          (function (i, s, o, g, r, a, m) {
            i["GoogleAnalyticsObject"] = r;
            (i[r] =
              i[r] ||
              function () {
                (i[r].q = i[r].q || []).push(arguments);
              }),
              (i[r].l = 1 * new Date());
            (a = s.createElement(o)), (m = s.getElementsByTagName(o)[0]);
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a, m);
          })(
            window,
            document,
            "script",
            "//www.google-analytics.com/analytics.js",
            "ga"
          );
          ga("create", "UA-149864185-1", "auto");
          ga("send", "pageview");
        }
      })();
    </script>
    <script src="/blog-base/umi.js"></script>
  </body>
</html>
