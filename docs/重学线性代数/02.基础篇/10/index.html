<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no"
    />
    <link rel="shortcut icon" type="image/x-icon" href="/logo.png" />
    <link rel="stylesheet" href="/blog-base/umi.css" />
    <script>
      window.routerBase = "/blog-base";
    </script>
    <script>
      //! umi version: 3.5.41
    </script>
    <script>
      !(function () {
        var e =
            navigator.cookieEnabled && void 0 !== window.localStorage
              ? localStorage.getItem("dumi:prefers-color")
              : "auto",
          o = window.matchMedia("(prefers-color-scheme: dark)").matches,
          t = ["light", "dark", "auto"];
        document.documentElement.setAttribute(
          "data-prefers-color",
          e === t[2] ? (o ? t[1] : t[0]) : t.indexOf(e) > -1 ? e : t[0]
        );
      })();
    </script>
    <title>10 | 解析几何：为什么说它是向量从抽象到具象的表达？ - 大师兄</title>
  </head>
  <body>
    <div id="root"><div class="__dumi-default-layout" data-route="/重学线性代数/02.基础篇/10" data-show-sidemenu="true" data-show-slugs="true" data-site-mode="true" data-gapless="false"><div class="__dumi-default-navbar" data-mode="site"><button class="__dumi-default-navbar-toggle"></button><a class="__dumi-default-navbar-logo" style="background-image:url(&#x27;/logo.png&#x27;)" href="/blog-base/">大师兄</a><nav><div class="__dumi-default-search"><input type="search" class="__dumi-default-search-input" value=""/><ul></ul></div><span>计算机基础<ul><li><a href="/blog-base/编译原理之美">编译原理之美</a></li><li><a href="/blog-base/编译原理实战">编译原理实战</a></li><li><a href="/blog-base/深入浅出计算机组成原理">深入浅出计算机组成原理</a></li><li><a href="/blog-base/详解http">详解http</a></li><li><a href="/blog-base/计算机网络通关29讲">计算机网络通关29讲</a></li><li><a href="/blog-base/网络排查案例课">网络排查案例课</a></li><li><a href="/blog-base/linux操作系统">linux操作系统</a></li><li><a href="/blog-base/linux内核技术实战课">linux内核技术实战课</a></li><li><a href="/blog-base/linux性能优化实战">linux性能优化实战</a></li><li><a href="/blog-base/程序员数学基础">程序员数学基础</a></li><li><a href="/blog-base/趣谈网络协议">趣谈网络协议</a></li><li><a href="/blog-base/操作系统实战">操作系统实战</a></li><li><a href="/blog-base/软件工程之美">软件工程之美</a></li><li><a href="/blog-base/sql必知必会">sql必知必会</a></li><li><a href="/blog-base/操作系统实战45讲">操作系统实战45讲</a></li><li><a href="/blog-base/网络编程实战">网络编程实战</a></li><li><a href="/blog-base/趣谈linux操作系统">趣谈linux操作系统</a></li></ul></span><span>算法<ul><li><a href="/blog-base/常用算法25讲">常用算法25讲</a></li><li><a href="/blog-base/数据结构与算法之美">数据结构与算法之美</a></li><li><a href="/blog-base/业务开发算法50讲">业务开发算法50讲</a></li><li><a href="/blog-base/动态规划面试宝典">动态规划面试宝典</a></li></ul></span><span>前端开发<ul><li><a href="/blog-base/正则表达式入门">正则表达式入门</a></li></ul></span><span>前端工程化</span><span>前端性能优化</span><span>移动端开发</span><span>软件测试</span><span>产品与用户体验</span><span>面试</span><span>杂谈<ul><li><a href="/blog-base/代码之丑">代码之丑</a></li><li><a href="/blog-base/代码精进之路">代码精进之路</a></li><li><a href="/blog-base/数据分析思维课">数据分析思维课</a></li><li><a href="/blog-base/朱涛kotlin编程第一课">朱涛kotlin编程第一课</a></li><li><a aria-current="page" class="active" href="/blog-base/重学线性代数">重学线性代数</a></li></ul></span><div class="__dumi-default-navbar-tool"><div class="__dumi-default-dark"><div class="__dumi-default-dark-switch "></div></div></div></nav></div><div class="__dumi-default-menu" data-mode="site"><div class="__dumi-default-menu-inner"><div class="__dumi-default-menu-header"><a class="__dumi-default-menu-logo" style="background-image:url(&#x27;/logo.png&#x27;)" href="/blog-base/"></a><h1>大师兄</h1><p></p></div><div class="__dumi-default-menu-mobile-area"><ul class="__dumi-default-menu-nav-list"><li>计算机基础<ul><li><a href="/blog-base/编译原理之美">编译原理之美</a></li><li><a href="/blog-base/编译原理实战">编译原理实战</a></li><li><a href="/blog-base/深入浅出计算机组成原理">深入浅出计算机组成原理</a></li><li><a href="/blog-base/详解http">详解http</a></li><li><a href="/blog-base/计算机网络通关29讲">计算机网络通关29讲</a></li><li><a href="/blog-base/网络排查案例课">网络排查案例课</a></li><li><a href="/blog-base/linux操作系统">linux操作系统</a></li><li><a href="/blog-base/linux内核技术实战课">linux内核技术实战课</a></li><li><a href="/blog-base/linux性能优化实战">linux性能优化实战</a></li><li><a href="/blog-base/程序员数学基础">程序员数学基础</a></li><li><a href="/blog-base/趣谈网络协议">趣谈网络协议</a></li><li><a href="/blog-base/操作系统实战">操作系统实战</a></li><li><a href="/blog-base/软件工程之美">软件工程之美</a></li><li><a href="/blog-base/sql必知必会">sql必知必会</a></li><li><a href="/blog-base/操作系统实战45讲">操作系统实战45讲</a></li><li><a href="/blog-base/网络编程实战">网络编程实战</a></li><li><a href="/blog-base/趣谈linux操作系统">趣谈linux操作系统</a></li></ul></li><li>算法<ul><li><a href="/blog-base/常用算法25讲">常用算法25讲</a></li><li><a href="/blog-base/数据结构与算法之美">数据结构与算法之美</a></li><li><a href="/blog-base/业务开发算法50讲">业务开发算法50讲</a></li><li><a href="/blog-base/动态规划面试宝典">动态规划面试宝典</a></li></ul></li><li>前端开发<ul><li><a href="/blog-base/正则表达式入门">正则表达式入门</a></li></ul></li><li>前端工程化</li><li>前端性能优化</li><li>移动端开发</li><li>软件测试</li><li>产品与用户体验</li><li>面试</li><li>杂谈<ul><li><a href="/blog-base/代码之丑">代码之丑</a></li><li><a href="/blog-base/代码精进之路">代码精进之路</a></li><li><a href="/blog-base/数据分析思维课">数据分析思维课</a></li><li><a href="/blog-base/朱涛kotlin编程第一课">朱涛kotlin编程第一课</a></li><li><a aria-current="page" class="active" href="/blog-base/重学线性代数">重学线性代数</a></li></ul></li></ul><div class="__dumi-default-dark"><div class="__dumi-default-dark-switch "><button title="Dark theme" class="__dumi-default-dark-moon "><svg viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="3854" width="22" height="22"><path d="M991.816611 674.909091a69.166545 69.166545 0 0 0-51.665455-23.272727 70.795636 70.795636 0 0 0-27.438545 5.585454A415.674182 415.674182 0 0 1 754.993338 698.181818c-209.594182 0-393.472-184.785455-393.472-395.636363 0-52.363636 38.539636-119.621818 69.515637-173.614546 4.887273-8.610909 9.634909-16.756364 14.103272-24.901818A69.818182 69.818182 0 0 0 384.631156 0a70.842182 70.842182 0 0 0-27.438545 5.585455C161.678429 90.298182 14.362065 307.898182 14.362065 512c0 282.298182 238.824727 512 532.38691 512a522.286545 522.286545 0 0 0 453.957818-268.334545A69.818182 69.818182 0 0 0 991.816611 674.909091zM546.679156 954.181818c-248.785455 0-462.941091-192-462.941091-442.181818 0-186.647273 140.637091-372.829091 300.939637-442.181818-36.817455 65.629091-92.578909 151.970909-92.578909 232.727273 0 250.181818 214.109091 465.454545 462.917818 465.454545a488.331636 488.331636 0 0 0 185.181091-46.545455 453.003636 453.003636 0 0 1-393.565091 232.727273z m103.656728-669.323636l-14.266182 83.781818a34.909091 34.909091 0 0 0 50.362182 36.770909l74.775272-39.563636 74.752 39.563636a36.142545 36.142545 0 0 0 16.174546 3.956364 34.909091 34.909091 0 0 0 34.210909-40.727273l-14.289455-83.781818 60.509091-59.345455a35.025455 35.025455 0 0 0-19.223272-59.578182l-83.61891-12.101818-37.376-76.101818a34.56 34.56 0 0 0-62.254545 0l-37.376 76.101818-83.618909 12.101818a34.909091 34.909091 0 0 0-19.246546 59.578182z m70.423272-64.698182a34.280727 34.280727 0 0 0 26.135273-19.083636l14.312727-29.090909 14.336 29.090909a34.257455 34.257455 0 0 0 26.135273 19.083636l32.046546 4.887273-23.272728 22.574545a35.234909 35.234909 0 0 0-10.007272 30.952727l5.46909 32.116364-28.625454-15.127273a34.490182 34.490182 0 0 0-32.302546 0l-28.695272 15.127273 5.469091-32.116364a35.141818 35.141818 0 0 0-9.984-30.952727l-23.272728-22.574545z" p-id="3855"></path></svg></button><button title="Light theme" class="__dumi-default-dark-sun "><svg viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="4026" width="22" height="22"><path d="M915.2 476.16h-43.968c-24.704 0-44.736 16-44.736 35.84s20.032 35.904 44.736 35.904H915.2c24.768 0 44.8-16.064 44.8-35.904s-20.032-35.84-44.8-35.84zM512 265.6c-136.704 0-246.464 109.824-246.464 246.4 0 136.704 109.76 246.464 246.464 246.464S758.4 648.704 758.4 512c0-136.576-109.696-246.4-246.4-246.4z m0 425.6c-99.008 0-179.2-80.128-179.2-179.2 0-98.944 80.192-179.2 179.2-179.2S691.2 413.056 691.2 512c0 99.072-80.192 179.2-179.2 179.2zM197.44 512c0-19.84-19.136-35.84-43.904-35.84H108.8c-24.768 0-44.8 16-44.8 35.84s20.032 35.904 44.8 35.904h44.736c24.768 0 43.904-16.064 43.904-35.904zM512 198.464c19.776 0 35.84-20.032 35.84-44.8v-44.8C547.84 84.032 531.84 64 512 64s-35.904 20.032-35.904 44.8v44.8c0 24.768 16.128 44.864 35.904 44.864z m0 627.136c-19.776 0-35.904 20.032-35.904 44.8v44.736C476.096 940.032 492.16 960 512 960s35.84-20.032 35.84-44.8v-44.736c0-24.768-16.064-44.864-35.84-44.864z m329.92-592.832c17.472-17.536 20.288-43.072 6.4-57.024-14.016-14.016-39.488-11.2-57.024 6.336-4.736 4.864-26.496 26.496-31.36 31.36-17.472 17.472-20.288 43.008-6.336 57.024 13.952 14.016 39.488 11.2 57.024-6.336 4.8-4.864 26.496-26.56 31.296-31.36zM213.376 759.936c-4.864 4.8-26.56 26.624-31.36 31.36-17.472 17.472-20.288 42.944-6.4 56.96 14.016 13.952 39.552 11.2 57.024-6.336 4.8-4.736 26.56-26.496 31.36-31.36 17.472-17.472 20.288-43.008 6.336-56.96-14.016-13.952-39.552-11.072-56.96 6.336z m19.328-577.92c-17.536-17.536-43.008-20.352-57.024-6.336-14.08 14.016-11.136 39.488 6.336 57.024 4.864 4.864 26.496 26.56 31.36 31.424 17.536 17.408 43.008 20.288 56.96 6.336 14.016-14.016 11.264-39.488-6.336-57.024-4.736-4.864-26.496-26.56-31.296-31.424z m527.168 628.608c4.864 4.864 26.624 26.624 31.36 31.424 17.536 17.408 43.072 20.224 57.088 6.336 13.952-14.016 11.072-39.552-6.4-57.024-4.864-4.8-26.56-26.496-31.36-31.36-17.472-17.408-43.072-20.288-57.024-6.336-13.952 14.016-11.008 39.488 6.336 56.96z" p-id="4027"></path></svg></button><button title="Default to system" class="__dumi-default-dark-auto "><svg viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="11002" width="22" height="22"><path d="M127.658667 492.885333c0-51.882667 10.24-101.717333 30.378666-149.162666s47.786667-88.064 81.92-122.538667 75.093333-61.781333 122.538667-81.92 96.938667-30.378667 149.162667-30.378667 101.717333 10.24 149.162666 30.378667 88.405333 47.786667 122.88 81.92 61.781333 75.093333 81.92 122.538667 30.378667 96.938667 30.378667 149.162666-10.24 101.717333-30.378667 149.162667-47.786667 88.405333-81.92 122.88-75.093333 61.781333-122.88 81.92-97.28 30.378667-149.162666 30.378667-101.717333-10.24-149.162667-30.378667-88.064-47.786667-122.538667-81.92-61.781333-75.093333-81.92-122.88-30.378667-96.938667-30.378666-149.162667z m329.045333 0c0 130.048 13.994667 244.394667 41.984 343.381334h12.970667c46.762667 0 91.136-9.216 133.461333-27.306667s78.848-42.666667 109.568-73.386667 54.954667-67.242667 73.386667-109.568 27.306667-86.698667 27.306666-133.461333c0-46.421333-9.216-90.794667-27.306666-133.12s-42.666667-78.848-73.386667-109.568-67.242667-54.954667-109.568-73.386667-86.698667-27.306667-133.461333-27.306666h-11.605334c-28.672 123.562667-43.349333 237.909333-43.349333 343.722666z" p-id="11003"></path></svg></button></div></div></div><ul class="__dumi-default-menu-list"><li><a href="/blog-base/重学线性代数">重学线性代数</a></li><li><a href="/blog-base/重学线性代数/01.开篇词">01.开篇词</a><ul><li><a href="/blog-base/重学线性代数/01.开篇词/01"><span>开篇词 | 从今天起，学会线性代数</span></a></li></ul></li><li><a aria-current="page" class="active" href="/blog-base/重学线性代数/02.基础篇">02.基础篇</a><ul><li><a href="/blog-base/重学线性代数/02.基础篇/01"><span>01 | 导读：如何在机器学习中运用线性代数工具？</span></a></li><li><a href="/blog-base/重学线性代数/02.基础篇/02"><span>02 | 基本概念：线性代数研究的到底是什么问题？</span></a></li><li><a href="/blog-base/重学线性代数/02.基础篇/03"><span>03 | 矩阵：为什么说矩阵是线性方程组的另一种表达？</span></a></li><li><a href="/blog-base/重学线性代数/02.基础篇/04"><span>04 | 解线性方程组：为什么用矩阵求解的效率这么高？</span></a></li><li><a href="/blog-base/重学线性代数/02.基础篇/05"><span>05 | 线性空间：如何通过向量的结构化空间在机器学习中做降维处理？</span></a></li><li><a href="/blog-base/重学线性代数/02.基础篇/06"><span>06 | 线性无关：如何理解向量在N维空间的几何意义？</span></a></li><li><a href="/blog-base/重学线性代数/02.基础篇/07"><span>07 | 基和秩：为什么说它表达了向量空间中“有用”的向量个数？</span></a></li><li><a href="/blog-base/重学线性代数/02.基础篇/08"><span>08 | 线性映射：如何从坐标系角度理解两个向量空间之间的函数？</span></a></li><li><a href="/blog-base/重学线性代数/02.基础篇/09"><span>09 | 仿射空间：如何在图形的平移操作中大显身手？</span></a></li><li><a aria-current="page" class="active" href="/blog-base/重学线性代数/02.基础篇/10"><span>10 | 解析几何：为什么说它是向量从抽象到具象的表达？</span></a></li><li><a href="/blog-base/重学线性代数/02.基础篇/11"><span>基础通关 | 线性代数5道典型例题及解析</span></a></li></ul></li><li><a href="/blog-base/重学线性代数/03.应用篇">03.应用篇</a><ul><li><a href="/blog-base/重学线性代数/03.应用篇/01"><span>11 | 如何运用线性代数方法解决图论问题？</span></a></li><li><a href="/blog-base/重学线性代数/03.应用篇/02"><span>12 | 如何通过矩阵转换让3D图形显示到二维屏幕上？</span></a></li><li><a href="/blog-base/重学线性代数/03.应用篇/03"><span>13 | 如何通过有限向量空间加持的希尔密码，提高密码被破译的难度？</span></a></li><li><a href="/blog-base/重学线性代数/03.应用篇/04"><span>14 | 如何在深度学习中运用数值代数的迭代法做训练？</span></a></li><li><a href="/blog-base/重学线性代数/03.应用篇/05"><span>15 | 如何从计算机的角度来理解线性代数？</span></a></li><li><a href="/blog-base/重学线性代数/03.应用篇/06"><span>强化通关 | 线性代数水平测试20题</span></a></li></ul></li><li><a href="/blog-base/重学线性代数/04.结束语">04.结束语</a><ul><li><a href="/blog-base/重学线性代数/04.结束语/01"><span>结束语 | 和数学打交道这么多年，我的三点感悟</span></a></li></ul></li><li><a href="/blog-base/重学线性代数/summary">重学线性代数</a></li></ul></div></div><ul role="slug-list" class="__dumi-default-layout-toc"><li title="范数" data-depth="2"><a href="/blog-base/重学线性代数/02.基础篇/10#范数"><span>范数</span></a></li><li title="内积" data-depth="2"><a href="/blog-base/重学线性代数/02.基础篇/10#内积"><span>内积</span></a></li><li title="点积" data-depth="3"><a href="/blog-base/重学线性代数/02.基础篇/10#点积"><span>点积</span></a></li><li title="其他内积" data-depth="3"><a href="/blog-base/重学线性代数/02.基础篇/10#其他内积"><span>其他内积</span></a></li><li title="内积空间" data-depth="3"><a href="/blog-base/重学线性代数/02.基础篇/10#内积空间"><span>内积空间</span></a></li><li title="对称正定矩阵" data-depth="2"><a href="/blog-base/重学线性代数/02.基础篇/10#对称正定矩阵"><span>对称正定矩阵</span></a></li><li title="长度、距离和角度" data-depth="2"><a href="/blog-base/重学线性代数/02.基础篇/10#长度距离和角度"><span>长度、距离和角度</span></a></li><li title="正交投影" data-depth="2"><a href="/blog-base/重学线性代数/02.基础篇/10#正交投影"><span>正交投影</span></a></li><li title="投影到一维子空间上（线）" data-depth="3"><a href="/blog-base/重学线性代数/02.基础篇/10#投影到一维子空间上线"><span>投影到一维子空间上（线）</span></a></li><li title="本节小结" data-depth="2"><a href="/blog-base/重学线性代数/02.基础篇/10#本节小结"><span>本节小结</span></a></li><li title="线性代数练习场" data-depth="2"><a href="/blog-base/重学线性代数/02.基础篇/10#线性代数练习场"><span>线性代数练习场</span></a></li></ul><div class="__dumi-default-layout-content"><div class="markdown"><h1 id="10--解析几何为什么说它是向量从抽象到具象的表达"><a aria-hidden="true" tabindex="-1" href="/blog-base/重学线性代数/02.基础篇/10#10--解析几何为什么说它是向量从抽象到具象的表达"><span class="icon icon-link"></span></a>10 | 解析几何：为什么说它是向量从抽象到具象的表达？</h1><p>你好，我是朱维刚。欢迎你继续跟我学习线性代数，今天我们要讲的内容是“解析几何”。</p><p>前面所有章节我们都是围绕向量、矩阵，以及向量空间来展开的。但这一节课有点不一样，我要讲的是解析几何，它使得向量从抽象走向了具象，让向量具有了几何的含义。比如，计算向量的长度、向量之间的距离和角度，这在机器学习的主成分分析PCA中是非常有用的。</p><h2 id="范数"><a aria-hidden="true" tabindex="-1" href="/blog-base/重学线性代数/02.基础篇/10#范数"><span class="icon icon-link"></span></a>范数</h2><p>讲解析几何我们得从“范数”开始讲起。</p><p>因为很多人看到几何向量的第一反应就是，它是从原点开始的有向线段，并且向量的长度是这个有向线段的终端和起始端之间的距离。而范数，就是被用来度量某个向量空间或矩阵中的每个向量的长度或大小的。</p><p>现在，我们先来看一下范数的数学定义：一个向量空间$V$上的一个范数就是一个函数，它计算$V$中的每一个向量$x$的长度，用符号来表示的话就是：$\|x\| \in R$，它满足三种性质：</p><ol><li>正齐次性： 如果输入参数扩大正$λ$倍，其对应的函数也扩正大倍。设$λ \in R$，$x \in V$，$\|\lambda x\|=|\lambda|\|x\|$；</li><li>次可加性：类似三角不等式，两边之和大于第三边。设$x,y \in V$，$\|x+y\| \leq\|x\|+\|y\|$；</li><li>正定性：向量$x$的长度一定大于等于零。$\|x\| \geq 0$。</li></ol><p>看到这里，你也许会问，范数似乎和以前老师教的<strong>向量的模</strong>一样啊。先别急，它们还真有那么一点关系，你听我慢慢道来。由于范数是度量某个向量空间或矩阵中的每个向量的长度或大小的，所以它和向量空间维度是有关系的，于是，我们可以把范数写成这样的模式来区分不同维度的大小计算：$L_<!-- -->{<!-- -->1<!-- -->}<!-- -->, L_<!-- -->{<!-- -->2<!-- -->}<!-- -->, \ldots, L_<!-- -->{<!-- -->\infty<!-- -->}<!-- -->$。</p><ul><li>$L_<!-- -->{<!-- -->1<!-- -->}<!-- -->$范数：曼哈顿范数，也叫曼哈顿距离，设$x \in R^<!-- -->{<!-- -->n<!-- -->}<!-- -->$，得到下面这个表达式。</li></ul><p>$$<br/>\|x\|_<!-- -->{<!-- -->1<!-- -->}<!-- -->=\sum_<!-- -->{<!-- -->i=1<!-- -->}<!-- -->^<!-- -->{<!-- -->n<!-- -->}<!-- -->\left|x_<!-- -->{<!-- -->i<!-- -->}<!-- -->\right|<br/>$$</p><ul><li>$L_<!-- -->{<!-- -->2<!-- -->}<!-- -->$范数：欧式范数，也叫欧式距离，设$x \in R^<!-- -->{<!-- -->n<!-- -->}<!-- -->$，得到下面这个表达式。</li></ul><p>$$<br/>\|x\|_<!-- -->{<!-- -->2<!-- -->}<!-- -->=\sqrt<!-- -->{<!-- -->\sum_<!-- -->{<!-- -->i=1<!-- -->}<!-- -->^<!-- -->{<!-- -->n<!-- -->}<!-- --> x_<!-- -->{<!-- -->i<!-- -->}<!-- -->^<!-- -->{<!-- -->2<!-- -->}<!-- -->}<br/>$$</p><ul><li>$L_<!-- -->{<!-- -->\infty<!-- -->}<!-- -->$范数：切比雪夫范数，也叫切比雪夫距离，设$x \in R^<!-- -->{<!-- -->n<!-- -->}<!-- -->$，得到下面这个表达式。</li></ul><p>$$<br/>\|x\|_<!-- -->{<!-- -->\infty<!-- -->}<!-- -->=\max \left(\left|x_<!-- -->{<!-- -->1<!-- -->}<!-- -->\right|,\left|x_<!-- -->{<!-- -->2<!-- -->}<!-- -->\right|, \ldots,\left|x_<!-- -->{<!-- -->n<!-- -->}<!-- -->\right|\right)<br/>$$</p><p>我们发现，向量的模和$L_<!-- -->{<!-- -->2<!-- -->}<!-- -->$范数的计算方式都是一样的，都表示的是欧氏距离，所以，我们可以简单地认为向量的模等于$L_<!-- -->{<!-- -->2<!-- -->}<!-- -->$范数。而其他的范数模式和向量的模则没有任何关系。</p><h2 id="内积"><a aria-hidden="true" tabindex="-1" href="/blog-base/重学线性代数/02.基础篇/10#内积"><span class="icon icon-link"></span></a>内积</h2><p>学习解析几何时，我们必须掌握的第二个概念就是内积。</p><p>如果说范数是模式，是用来描述向量长度或大小的概念性表达，那么内积可以让我们很直观地了解一个向量的长度、两个向量之间的距离和角度，它的一个主要目的就是判断向量之间是否是正交的，正交这个概念我们会在后面讲解。</p><h3 id="点积"><a aria-hidden="true" tabindex="-1" href="/blog-base/重学线性代数/02.基础篇/10#点积"><span class="icon icon-link"></span></a>点积</h3><p>我们从特殊到一般，先来看点积，它和第三篇矩阵中说的“普通矩阵乘”形式一样，点积是特殊的内积，为什么说它特殊呢？那是因为在表示两个向量之间的距离时，它就是大家熟悉的欧式距离，点积可以表示成这样的形式：</p><p>$$<br/>x^<!-- -->{<!-- -->T<!-- -->}<!-- --> y=\sum_<!-- -->{<!-- -->i=1<!-- -->}<!-- -->^<!-- -->{<!-- -->n<!-- -->}<!-- --> x_<!-- -->{<!-- -->i<!-- -->}<!-- --> y_<!-- -->{<!-- -->i<!-- -->}<br/>$$</p><h3 id="其他内积"><a aria-hidden="true" tabindex="-1" href="/blog-base/重学线性代数/02.基础篇/10#其他内积"><span class="icon icon-link"></span></a>其他内积</h3><p>除了点积外，我们再来看另一个不同的内积：设内积空间$V$是$R^<!-- -->{<!-- -->2<!-- -->}<!-- -->$，定义内积$\langle x, y\rangle=x_<!-- -->{<!-- -->1<!-- -->}<!-- --> y_<!-- -->{<!-- -->1<!-- -->}<!-- -->-(x_<!-- -->{<!-- -->1<!-- -->}<!-- --> y_<!-- -->{<!-- -->2<!-- -->}<!-- -->+x_<!-- -->{<!-- -->2<!-- -->}<!-- --> y_<!-- -->{<!-- -->1<!-- -->}<!-- -->)+2 x_<!-- -->{<!-- -->2<!-- -->}<!-- --> y_<!-- -->{<!-- -->2<!-- -->}<!-- -->$，一看便知这个和点积完全不同。</p><h3 id="内积空间"><a aria-hidden="true" tabindex="-1" href="/blog-base/重学线性代数/02.基础篇/10#内积空间"><span class="icon icon-link"></span></a>内积空间</h3><p>最后，我们再来看一般内积和内积空间。因为解析几何关注的是向量的长度、两个向量之间的距离和角度，所以，我们要在原来向量空间上加一个额外的结构，这个额外结构就是内积，而加了内积的向量空间，我们就叫做内积空间。</p><p>为了表达方便，我们可以把内积写成$\langle\ ·,· \rangle$这样的形式，那么内积空间$V$可以被表示成这样：$(V,\langle\ ·,· \rangle)$。这时，如果一般内积由点积来表达，那这个向量空间就变成了更具体的欧式向量空间。</p><p>接下来看下内积空间有什么性质？我们定义一个内积空间V和它的元素$x$、$y$、$z$，以及一个$c \in R$：</p><ul><li><p>满足对称性：$x$和$y$的内积等于$y$和$x$的内积，$\langle x, y\rangle=\langle y, x\rangle$；</p></li><li><p>满足线性性：$x$和$y+cz$的内积等于，$x$和$y$的内积，与$x$和$z$的内积乘以$c$后的和，<br/>$\langle x, y+c z\rangle=\langle x, y\rangle+c\langle x, z\rangle$；</p></li><li><p>满足正定性：$x$和$y$的内积大于等于零，$\langle x, y\rangle \geq 0$。</p></li></ul><h2 id="对称正定矩阵"><a aria-hidden="true" tabindex="-1" href="/blog-base/重学线性代数/02.基础篇/10#对称正定矩阵"><span class="icon icon-link"></span></a>对称正定矩阵</h2><p>内积还定义了一类矩阵，这类矩阵在机器学习中很重要，因为它可以被用来判定多元函数极值，而在深度学习中，它更是被用来获取最小化损失函数，我们把这类矩阵叫做对称正定矩阵。</p><p>对称正定矩阵的定义是：如果一个对称矩阵$A$属于方阵$R^<!-- -->{<!-- -->n×n<!-- -->}<!-- -->$，对任意非零向量$x$，都有$x^<!-- -->{<!-- -->T<!-- -->}<!-- --> A x&gt;0$，那么$A$就是对称正定矩阵。</p><p>我们来看两个例子，判断它们是不是对称正定矩阵。</p><p>第一个例子，请你回答下面这个矩阵是对称正定矩阵吗？</p><p>$$<br/>A=\left[\begin<!-- -->{<!-- -->array<!-- -->}<!-- -->{<!-- -->ll<!-- -->}<br/>9 &amp; 6 \\\<br/>6 &amp; 5<br/>\end<!-- -->{<!-- -->array<!-- -->}<!-- -->\right]<br/>$$</p><p>答案：是的，它是对称正定矩阵。因为$x^<!-- -->{<!-- -->T<!-- -->}<!-- --> A x&gt;0$。</p><p>$$<br/>x^<!-- -->{<!-- -->T<!-- -->}<!-- --> A x=\left[\begin<!-- -->{<!-- -->array<!-- -->}<!-- -->{<!-- -->ll<!-- -->}<br/>x_<!-- -->{<!-- -->1<!-- -->}<!-- --> &amp; x_<!-- -->{<!-- -->2<!-- -->}<br/>\end<!-- -->{<!-- -->array<!-- -->}<!-- -->\right]\left[\begin<!-- -->{<!-- -->array<!-- -->}<!-- -->{<!-- -->ll<!-- -->}<br/>9 &amp; 6 \\\<br/>6 &amp; 5<br/>\end<!-- -->{<!-- -->array<!-- -->}<!-- -->\right]\left[\begin<!-- -->{<!-- -->array<!-- -->}<!-- -->{<!-- -->l<!-- -->}<br/>x_<!-- -->{<!-- -->1<!-- -->}<!-- --> \\\<br/>x_<!-- -->{<!-- -->2<!-- -->}<br/>\end<!-- -->{<!-- -->array<!-- -->}<!-- -->\right]=(3 x_<!-- -->{<!-- -->1<!-- -->}<!-- -->+2 x_<!-- -->{<!-- -->2<!-- -->}<!-- -->)^<!-- -->{<!-- -->2<!-- -->}<!-- -->+x_<!-- -->{<!-- -->2<!-- -->}<!-- -->^<!-- -->{<!-- -->2<!-- -->}<!-- -->&gt;0<br/>$$</p><p>第二个例子，请你看下面这个矩阵是对称正定矩阵吗？</p><p>$$<br/>A=\left[\begin<!-- -->{<!-- -->array<!-- -->}<!-- -->{<!-- -->ll<!-- -->}<br/>9 &amp; 6 \\\<br/>6 &amp; 3<br/>\end<!-- -->{<!-- -->array<!-- -->}<!-- -->\right]<br/>$$</p><p>答案：不是的，它只是对称矩阵。因为$x^<!-- -->{<!-- -->T<!-- -->}<!-- --> A x$可能小于0。</p><p>$$<br/>x^<!-- -->{<!-- -->T<!-- -->}<!-- --> A x=\left[\begin<!-- -->{<!-- -->array<!-- -->}<!-- -->{<!-- -->ll<!-- -->}<br/>x_<!-- -->{<!-- -->1<!-- -->}<!-- --> &amp; x_<!-- -->{<!-- -->2<!-- -->}<br/>\end<!-- -->{<!-- -->array<!-- -->}<!-- -->\right]\left[\begin<!-- -->{<!-- -->array<!-- -->}<!-- -->{<!-- -->ll<!-- -->}<br/>9 &amp; 6 \\\<br/>6 &amp; 3<br/>\end<!-- -->{<!-- -->array<!-- -->}<!-- -->\right]\left[\begin<!-- -->{<!-- -->array<!-- -->}<!-- -->{<!-- -->l<!-- -->}<br/>x_<!-- -->{<!-- -->1<!-- -->}<!-- --> \\\<br/>x_<!-- -->{<!-- -->2<!-- -->}<br/>\end<!-- -->{<!-- -->array<!-- -->}<!-- -->\right]=(3 x_<!-- -->{<!-- -->1<!-- -->}<!-- -->+2 x_<!-- -->{<!-- -->2<!-- -->}<!-- -->)^<!-- -->{<!-- -->2<!-- -->}<!-- -->-x_<!-- -->{<!-- -->2<!-- -->}<!-- -->^<!-- -->{<!-- -->2<!-- -->}<br/>$$</p><h2 id="长度距离和角度"><a aria-hidden="true" tabindex="-1" href="/blog-base/重学线性代数/02.基础篇/10#长度距离和角度"><span class="icon icon-link"></span></a>长度、距离和角度</h2><p>前面我们通过范数讲了向量的长度，但从内积的角度来看，我们发现，内积和范数之间有着千丝万缕的关系。我们来看看下面这个等式。</p><p>$$<br/>\|x\|=\sqrt<!-- -->{<!-- -->\langle x, x\rangle<!-- -->}<br/>$$</p><p>从这个等式我们发现，内积可以用来产生范数，确实是这样。不过，不是每一个范数都能被一个内积产生的，比如：曼哈顿范数。接下来，我们还是来关注能由内积产生的范数上，从不同的角度来看看几何上的长度、距离和角度的概念。</p><p>我们先用内积来计算一个<strong>向量的长度</strong>，比如：向量$x=[\begin<!-- -->{<!-- -->array<!-- -->}<!-- -->{<!-- -->ll<!-- -->}<!-- -->1 &amp; 1\end<!-- -->{<!-- -->array<!-- -->}<!-- -->]^<!-- -->{<!-- -->T<!-- -->}<!-- -->$，我们可以使用点积来计算，计算后得出$x$的范数是$\sqrt<!-- -->{<!-- -->2<!-- -->}<!-- -->$，具体计算过程是这样的：$\|x\|=\sqrt<!-- -->{<!-- -->x^<!-- -->{<!-- -->T<!-- -->}<!-- --> x<!-- -->}<!-- -->=\sqrt<!-- -->{<!-- -->1^<!-- -->{<!-- -->2<!-- -->}<!-- -->+1^<!-- -->{<!-- -->2<!-- -->}<!-- -->}<!-- -->=\sqrt<!-- -->{<!-- -->2<!-- -->}<!-- -->$。</p><p>接着，我们再来看一下<strong>向量之间的距离</strong>，一个内积空间$V$，$(V,\langle\ ·,· \rangle)$，$x$和$y$是它的两个向量，那么$x$和$y$之间的距离就可以表示成：$d(x, y)=\|x-y\|=\sqrt<!-- -->{<!-- -->\langle x-y, x-y\rangle<!-- -->}<!-- -->$。</p><p>如果用点积来计算$x$和$y$之间的距离，那这个距离就叫做欧式距离。</p><p>再接着，来看看两个<strong>向量之间的角度</strong>。我们使用柯西-施瓦茨不等式（Cauchy-Schwarz Inequality）来表示内积空间中两个向量$x$和$y$之间的角度：$a$。</p><p>$$<br/>-1 \leq \frac<!-- -->{<!-- -->\langle x, y\rangle<!-- -->}<!-- -->{<!-- -->\|x\|\|y\|<!-- -->}<!-- --> \leq 1<br/>$$</p><p>取值是从$-1$到$1$之间，那么角度就是从$0$到$π$之间，我们用$cos$来表示就是：</p><p>$$<br/>\cos (a)=\frac<!-- -->{<!-- -->\langle x, y\rangle<!-- -->}<!-- -->{<!-- -->\|x\|\|y\|<!-- -->}<br/>$$</p><p>其中$a$就是角度，$a$的角度取值是$0$到$π$之间。我们很容易就能发现，其实两个向量之间的角度，就是告诉了我们两个向量之间方向的相似性。例如：$x$和$y=4x$，使用点积来计算它们之间的角度是$0$，也就是说它们的方向是一样的，$y$只是对$x$扩大了$4$倍而已。</p><p>现在，我们通过一个例子，再来更清楚地看下两个向量之间角度的计算，设$x=[\begin<!-- -->{<!-- -->array<!-- -->}<!-- -->{<!-- -->ll<!-- -->}<!-- -->1 &amp; 1\end<!-- -->{<!-- -->array<!-- -->}<!-- -->]^<!-- -->{<!-- -->T<!-- -->}<!-- -->$，$y=[\begin<!-- -->{<!-- -->array<!-- -->}<!-- -->{<!-- -->ll<!-- -->}<!-- -->1 &amp; 2\end<!-- -->{<!-- -->array<!-- -->}<!-- -->]^<!-- -->{<!-- -->T<!-- -->}<!-- -->$，使用点积来计算，我们得出：</p><p>$$<br/>\cos (a)=\frac<!-- -->{<!-- -->\langle x, y\rangle<!-- -->}<!-- -->{<!-- -->\sqrt<!-- -->{<!-- -->\langle x, x\rangle\langle y, y\rangle<!-- -->}<!-- -->}<!-- -->=\frac<!-- -->{<!-- -->x^<!-- -->{<!-- -->T<!-- -->}<!-- --> y<!-- -->}<!-- -->{<!-- -->\sqrt<!-- -->{<!-- -->x^<!-- -->{<!-- -->T<!-- -->}<!-- --> x y^<!-- -->{<!-- -->T<!-- -->}<!-- --> y<!-- -->}<!-- -->}<!-- -->=\frac<!-- -->{<!-- -->3<!-- -->}<!-- -->{<!-- -->\sqrt<!-- -->{<!-- -->10<!-- -->}<!-- -->}<br/>$$</p><p>那么，这两个向量之间的角度如下。</p><p>$$<br/>\arccos \left(\frac<!-- -->{<!-- -->3<!-- -->}<!-- -->{<!-- -->\sqrt<!-- -->{<!-- -->10<!-- -->}<!-- -->}<!-- -->\right) \approx 0.32<br/>$$</p><p>我们可以用图来更直观地表达一下。</p><p><img src="/images/httpsstatic001geekbangorgresourceimageaae0aabb363c1ae08fdcdf568555b62f87e0.png" alt=""/></p><p>于是，我们最后可以引出一个概念，也就是我们在一开始提到的<strong>正交性</strong>。如果两个向量$x$和$y$内积等于$0$，$\langle x, y\rangle=0$，那么$x$和$y$是正交的，这可以写成：$x \perp y$。再如果，$x$和$y$的范数都等于$1$，$\|x\|=\|y\|=1$，也就是说，如果它们都是单位向量，那么$x$和$y$就是标准正交的。</p><p>我们用图来更直观地表达一下。</p><p><img src="/images/httpsstatic001geekbangorgresourceimage99109982b991fb207180970caf85d867a210.png" alt=""/></p><h2 id="正交投影"><a aria-hidden="true" tabindex="-1" href="/blog-base/重学线性代数/02.基础篇/10#正交投影"><span class="icon icon-link"></span></a>正交投影</h2><p>在理论讲解之后，我们要来了解一下解析几何在实践中经常运用的概念——正交投影，它是一种重要的线性变换，在图形图像、编码理论、统计和机器学习中扮演了重要角色。</p><p>在机器学习中，数据一般都是高维的。众所周知，高维数据是很难来分析和可视化的。而且，不是所有的高维数据都是有用的，可能只有一些数据包含了大部分的重要信息。</p><p>正交投影就是高维到低维的数据投影，在<a target="_blank" rel="noopener noreferrer" href="https://time.geekbang.org/column/article/270329">第5节课线性空间<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="__dumi-default-external-link-icon"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>中，我简单介绍了高维数据投影到低维后，我们就能在低维空间更多地了解数据集、提炼相关模式。而在机器学习中，最普遍的降维算法——PCA主成分分析，就是利用了降维的观点。</p><p>接下来，我开始讲解正交投影，在给出定义之前，先从一张图来了解会更直观。</p><p><img src="/images/httpsstatic001geekbangorgresourceimagebe68be7421186446670dcd791a94da190468.png" alt=""/></p><p>图中的蓝点是原二维数据，黄点是它们的正交投影。所以，实际降维后，在一维空间中就形成了这条黑线表示，它近似地表达了原来二维数据表示的信息。</p><p>现在我们可以来看一下投影的定义：$V$是一个向量空间，$U$是$V$的一个向量子空间，一个从$V$到$U$的线性映射$\Phi$是一个投影，如果它满足：$\Phi^<!-- -->{<!-- -->2<!-- -->}<!-- -->=\Phi \circ \Phi=\Phi$。因为线性映射能够被变换矩阵表示，所以，这个定义同样适用于一个特殊类型变换矩阵：投影矩阵$P_<!-- -->{<!-- -->\Phi<!-- -->}<!-- -->$，它也满足：$P_<!-- -->{<!-- -->\Phi<!-- -->}<!-- -->^<!-- -->{<!-- -->2<!-- -->}<!-- -->=P_<!-- -->{<!-- -->\Phi<!-- -->}<!-- -->$。</p><h3 id="投影到一维子空间上线"><a aria-hidden="true" tabindex="-1" href="/blog-base/重学线性代数/02.基础篇/10#投影到一维子空间上线"><span class="icon icon-link"></span></a>投影到一维子空间上（线）</h3><p>接下来，我们来看看如何投影到一维子空间，也就是把内积空间的向量正交投影到子空间，这里我们使用点积作为内积。</p><p>假设有一条通过原点的线，这条线是由基向量$b$产生的一维子空间$U$，当我们把一个向量$x$投影到$U$时，需要寻找另一个最靠近$x$的向量$\Phi_<!-- -->{<!-- -->U<!-- -->}<!-- -->(x)$。还是老样子，我们通过图来看一下。</p><p><img src="/images/httpsstatic001geekbangorgresourceimagee190e1e9yy6ae45b6b36975bf20acf5c8a90.png" alt=""/></p><p>首先，投影$\Phi_<!-- -->{<!-- -->U<!-- -->}<!-- -->(x)$靠近$x$，也就是要找出$x$和$\Phi_<!-- -->{<!-- -->U<!-- -->}<!-- -->(x)$之间的$\left\|x-\Phi_<!-- -->{<!-- -->U<!-- -->}<!-- -->(x)\right\|$最小距离，从几何角度来说，就是线段$\Phi_<!-- -->{<!-- -->U<!-- -->}<!-- -->(x)-x$和$b$正交，满足等式：$\left\langle\Phi_<!-- -->{<!-- -->U<!-- -->}<!-- -->(x)-x, b\right\rangle=0$。其次，投影$\Phi_<!-- -->{<!-- -->U<!-- -->}<!-- -->(x)$必须是$U$的一个元素，也就是，基向量$b$的一个乘来产生$U$，$\Phi_<!-- -->{<!-- -->U<!-- -->}<!-- -->(x)=λb$。</p><p>于是，我们可以通过三个步骤来分别得到$λ$、投影$\Phi_<!-- -->{<!-- -->U<!-- -->}<!-- -->(x)$和投影矩阵$P_<!-- -->{<!-- -->\Phi<!-- -->}<!-- -->$，来把任意$x$映射到子空间$U$上。</p><p>第一步，计算$λ$，通过正交条件产生这样的等式：<br/>$\left\langle x-\Phi_<!-- -->{<!-- -->U<!-- -->}<!-- -->(x), b\right\rangle=0$。因为$\Phi_<!-- -->{<!-- -->U<!-- -->}<!-- -->(x)=λb$，所以它可以转变成：$\langle x-\lambda b, b\rangle=0$。</p><p>利用内积的双线性：$\langle x, b\rangle-\lambda\langle b, b\rangle=0$，我们得到：</p><p>$$<br/>\lambda=\frac<!-- -->{<!-- -->\langle x, b\rangle<!-- -->}<!-- -->{<!-- -->\langle b, b\rangle<!-- -->}<!-- -->=\frac<!-- -->{<!-- -->\langle b, x\rangle<!-- -->}<!-- -->{<!-- -->\|b\|^<!-- -->{<!-- -->2<!-- -->}<!-- -->}<br/>$$</p><p>然后，我们通过点积得到：</p><p>$$<br/>\lambda=\frac<!-- -->{<!-- -->b^<!-- -->{<!-- -->T<!-- -->}<!-- --> x<!-- -->}<!-- -->{<!-- -->\|b\|^<!-- -->{<!-- -->2<!-- -->}<!-- -->}<br/>$$</p><p>如果$\|b\|=1$，那$λ$就等于$b^<!-- -->{<!-- -->T<!-- -->}<!-- -->x$。</p><p>接着第二步，是计算投影点$\Phi_<!-- -->{<!-- -->U<!-- -->}<!-- -->(x)$。从$\Phi_<!-- -->{<!-- -->U<!-- -->}<!-- -->(x)=λb$，我们得到：</p><p>$$<br/>\Phi_<!-- -->{<!-- -->U<!-- -->}<!-- -->(x)=\lambda b=\frac<!-- -->{<!-- -->\langle x, b\rangle<!-- -->}<!-- -->{<!-- -->\|b\|^<!-- -->{<!-- -->2<!-- -->}<!-- -->}<!-- --> b=\frac<!-- -->{<!-- -->b^<!-- -->{<!-- -->T<!-- -->}<!-- --> x<!-- -->}<!-- -->{<!-- -->\|b\|^<!-- -->{<!-- -->2<!-- -->}<!-- -->}<!-- --> b<br/>$$</p><p>通过点积来计算，我们就得到了$\Phi_<!-- -->{<!-- -->U<!-- -->}<!-- -->(x)$的长度：</p><p>$$<br/>\left\|\Phi_<!-- -->{<!-- -->U<!-- -->}<!-- -->(x)\right\|=\frac<!-- -->{<!-- -->\left|b^<!-- -->{<!-- -->T<!-- -->}<!-- --> x\right|<!-- -->}<!-- -->{<!-- -->\|b\|^<!-- -->{<!-- -->2<!-- -->}<!-- -->}<!-- -->\|b\|=|\cos (a)|\|x\|\|b\| \frac<!-- -->{<!-- -->\|b\|<!-- -->}<!-- -->{<!-- -->\|b\|^<!-- -->{<!-- -->2<!-- -->}<!-- -->}<!-- -->=\mid \cos (a)\|x\|<br/>$$</p><p>这里的$a$，是$x$和$b$之间的夹角。</p><p>最后第三步，是计算投影矩阵$P_<!-- -->{<!-- -->\Phi<!-- -->}<!-- -->$，投影矩阵是一个线性映射。所以，我们可以得到：$\Phi_<!-- -->{<!-- -->U<!-- -->}<!-- -->(x)=P_<!-- -->{<!-- -->\Phi<!-- -->}<!-- -->x$，通过$\Phi_<!-- -->{<!-- -->U<!-- -->}<!-- -->(x)=λb$，我们可以得到：</p><p>$$<br/>\Phi_<!-- -->{<!-- -->U<!-- -->}<!-- -->(x)=\lambda b=b \lambda=b \frac<!-- -->{<!-- -->b^<!-- -->{<!-- -->T<!-- -->}<!-- --> x<!-- -->}<!-- -->{<!-- -->\|b\|^<!-- -->{<!-- -->2<!-- -->}<!-- -->}<!-- -->=\frac<!-- -->{<!-- -->b b^<!-- -->{<!-- -->T<!-- -->}<!-- -->}<!-- -->{<!-- -->\|b\|^<!-- -->{<!-- -->2<!-- -->}<!-- -->}<!-- --> x<br/>$$</p><p>这里，我们立即可以得到投影矩阵$P_<!-- -->{<!-- -->\Phi<!-- -->}<!-- -->$的计算等式：</p><p>$$<br/>P_<!-- -->{<!-- -->\Phi<!-- -->}<!-- -->=\frac<!-- -->{<!-- -->b b^<!-- -->{<!-- -->T<!-- -->}<!-- -->}<!-- -->{<!-- -->\|b\|^<!-- -->{<!-- -->2<!-- -->}<!-- -->}<br/>$$</p><h2 id="本节小结"><a aria-hidden="true" tabindex="-1" href="/blog-base/重学线性代数/02.基础篇/10#本节小结"><span class="icon icon-link"></span></a>本节小结</h2><p>这一节课覆盖的知识点有点多，因为要把解析几何的知识点，浓缩到核心的几个点来讲解是一项艰巨的任务。不过不要怕，前面的几个知识点都是为这一节的重点“正交投影”来铺垫的。范数，被用来度量某个向量空间或矩阵中的每个向量的长度或大小，而内积让我们很直观地了解一个向量的长度、两个向量之间的距离和角度，以及判断向量之间是否是正交的。</p><p>所以，希望你能掌握范数和内积的理论知识，并把它和正交投影结合，运用在一些实践应用场景中，比如：3D图形图像的坐标变换、数据压缩，以及机器学习的降维。</p><h2 id="线性代数练习场"><a aria-hidden="true" tabindex="-1" href="/blog-base/重学线性代数/02.基础篇/10#线性代数练习场"><span class="icon icon-link"></span></a>线性代数练习场</h2><p>请用之前学到的正交投影的投影矩阵算法，来计算一条线上的投影矩阵$P_<!-- -->{<!-- -->\Phi<!-- -->}<!-- -->$。</p><p>这条线通过原点，由基$b=\left[\begin<!-- -->{<!-- -->array<!-- -->}<!-- -->{<!-- -->lll<!-- -->}<!-- -->1 &amp; 2 &amp; 2\end<!-- -->{<!-- -->array<!-- -->}<!-- -->\right]^<!-- -->{<!-- -->T<!-- -->}<!-- -->$产生，$P_<!-- -->{<!-- -->\Phi<!-- -->}<!-- -->$计算后，再通过一个$x$来验证一下它是否在$b$产生的子空间中，我们取$x=\left[\begin<!-- -->{<!-- -->array<!-- -->}<!-- -->{<!-- -->lll<!-- -->}<!-- -->1 &amp; 1 &amp; 1\end<!-- -->{<!-- -->array<!-- -->}<!-- -->\right]^<!-- -->{<!-- -->T<!-- -->}<!-- -->$。</p><p>欢迎在留言区晒出你的运算结果，我会及时回复。同时，也欢迎你把这篇文章分享给你的朋友，一起讨论、学习。</p></div><div class="__dumi-default-layout-footer-meta"><a target="_blank" rel="noopener noreferrer" href="https://github.com/GGwujun/blog/edit/master/ssrc/重学线性代数/02.基础篇/10.md">在 GitHub 上编辑此页<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="__dumi-default-external-link-icon"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a><span data-updated-text="最后更新时间：">2023/9/23 17:33:39</span></div></div></div></div>
	<script>
  window.g_useSSR = true;
  window.g_initialProps = {};
	</script>

    <script>
      (function () {
        if (!location.port) {
          (function (i, s, o, g, r, a, m) {
            i["GoogleAnalyticsObject"] = r;
            (i[r] =
              i[r] ||
              function () {
                (i[r].q = i[r].q || []).push(arguments);
              }),
              (i[r].l = 1 * new Date());
            (a = s.createElement(o)), (m = s.getElementsByTagName(o)[0]);
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a, m);
          })(
            window,
            document,
            "script",
            "//www.google-analytics.com/analytics.js",
            "ga"
          );
          ga("create", "UA-149864185-1", "auto");
          ga("send", "pageview");
        }
      })();
    </script>
    <script src="/blog-base/umi.js"></script>
  </body>
</html>
